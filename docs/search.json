[
  {
    "objectID": "1-TidyverseRecap.html",
    "href": "1-TidyverseRecap.html",
    "title": "Tidyverse Recap",
    "section": "",
    "text": "The tidyverse is a collection of R packages designed for data science.\nAll packages share an underlying design philosophy, grammar, and data structures.\nIt emphasizes tidy data in data frames, performs operations one step at a time, connects with pipes and makes code human readable."
  },
  {
    "objectID": "1-TidyverseRecap.html#tidyr-pivot",
    "href": "1-TidyverseRecap.html#tidyr-pivot",
    "title": "Tidyverse Recap",
    "section": "tidyr: pivot",
    "text": "tidyr: pivot\n\n# Load the tidyr package\nlibrary(tidyr)\n\n\n# Assume we have a dataset 'data' with 'ID1', 'ID2', 'x', and 'y' columns\ndata_ex1 &lt;- tibble(ID1 = rep(LETTERS[1:4],times = 3), \n                   ID2 = rep(letters[1:3], each = 4), \n                   x = 1:12, \n                   y = 21:32)\n\nprint(data_ex1)\n\n# A tibble: 12 × 4\n   ID1   ID2       x     y\n   &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt;\n 1 A     a         1    21\n 2 B     a         2    22\n 3 C     a         3    23\n 4 D     a         4    24\n 5 A     b         5    25\n 6 B     b         6    26\n 7 C     b         7    27\n 8 D     b         8    28\n 9 A     c         9    29\n10 B     c        10    30\n11 C     c        11    31\n12 D     c        12    32\n\n# Use pivot_longer() to convert wide data to long format\ndata_long &lt;- data_ex1 %&gt;% pivot_longer(cols = c(\"x\", \"y\"), \n                                       names_to = \"Variable\", \n                                       values_to = \"Value\")\n\n# Print the long format data\nprint(data_long)\n\n# A tibble: 24 × 4\n   ID1   ID2   Variable Value\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;    &lt;int&gt;\n 1 A     a     x            1\n 2 A     a     y           21\n 3 B     a     x            2\n 4 B     a     y           22\n 5 C     a     x            3\n 6 C     a     y           23\n 7 D     a     x            4\n 8 D     a     y           24\n 9 A     b     x            5\n10 A     b     y           25\n# ℹ 14 more rows\n\n# Use pivot_wider() to convert long data back to wide format\ndata_wide &lt;- data_long %&gt;% pivot_wider(names_from = Variable,\n                                       values_from = Value)\n\n# Print the wide format data\nprint(data_wide)\n\n# A tibble: 12 × 4\n   ID1   ID2       x     y\n   &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt;\n 1 A     a         1    21\n 2 B     a         2    22\n 3 C     a         3    23\n 4 D     a         4    24\n 5 A     b         5    25\n 6 B     b         6    26\n 7 C     b         7    27\n 8 D     b         8    28\n 9 A     c         9    29\n10 B     c        10    30\n11 C     c        11    31\n12 D     c        12    32\n\n\nIn this example, pivot_longer is used to convert the wide format data to long format, where each row is a single observation associated with the variables ID1, ID2, Variable (containing the original column names ‘x’ and ‘y’), and Value (containing the values from ‘x’ and ‘y’ columns). We can then also convert back to wide format using pivot_wider."
  },
  {
    "objectID": "1-TidyverseRecap.html#tidyr-separate",
    "href": "1-TidyverseRecap.html#tidyr-separate",
    "title": "Tidyverse Recap",
    "section": "tidyr: separate",
    "text": "tidyr: separate\n\n# Load the tidyr package\nlibrary(tidyr)\n\n# Assume we have a dataset 'dataNew' with a 'datetime' column\ndata_ex2 &lt;- tibble(datetime = \n                    c(\"2016-01-01 07:30:29\", \"2016-01-02 09:43:36\", \"2016-01-03 13:59:00\"), \n                   event = c(\"u\", \"a\", \"l\"))\n\n# Use the separate() function from tidyr to separate the 'datetime' column into \n# 'date' and 'time'\n# Then separate 'time' into 'hour', 'min', 'second'\ndata_sep &lt;- data_ex2 %&gt;% \n              separate(datetime, c('date', 'time'), sep = ' ') %&gt;% \n              separate(time, c('hour', 'min', 'second'), sep = ':')\n\n# Print the new dataset\nprint(data_sep)\n\n# A tibble: 3 × 5\n  date       hour  min   second event\n  &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;\n1 2016-01-01 07    30    29     u    \n2 2016-01-02 09    43    36     a    \n3 2016-01-03 13    59    00     l    \n\n# change hour, min, second to numeric values\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ndata_sep %&gt;% mutate_at(vars(hour, min, second), as.numeric)\n\n# A tibble: 3 × 5\n  date        hour   min second event\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;\n1 2016-01-01     7    30     29 u    \n2 2016-01-02     9    43     36 a    \n3 2016-01-03    13    59      0 l"
  },
  {
    "objectID": "1-TidyverseRecap.html#example-dplyr",
    "href": "1-TidyverseRecap.html#example-dplyr",
    "title": "Tidyverse Recap",
    "section": "Example: dplyr",
    "text": "Example: dplyr\n\n# Load the dplyr package\nlibrary(dplyr)\n\n# Assume we have a dataset 'data' with 'ID', 'Age', 'Gender', and 'Income' columns\ndata_ex3 &lt;- tibble(ID = 1:4, \n                   Age = c(21, 35, 58, 40), \n                   Gender = c(\"Male\", \"Female\", \"Male\", \"Female\"), \n                   Income = c(50000, 80000, 120000, 75000))\n\n# Use select() to choose the 'ID' and 'Age' columns\nselected_data &lt;- data_ex3 %&gt;% select(ID, Age)\nselected_data\n\n# A tibble: 4 × 2\n     ID   Age\n  &lt;int&gt; &lt;dbl&gt;\n1     1    21\n2     2    35\n3     3    58\n4     4    40\n\n# Use filter() to get rows where 'Age' is greater than 30\nfiltered_data &lt;- data_ex3 %&gt;% filter(Age &gt; 30)\nfiltered_data\n\n# A tibble: 3 × 4\n     ID   Age Gender Income\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1     2    35 Female  80000\n2     3    58 Male   120000\n3     4    40 Female  75000\n\n# Use mutate() to create a new column 'IncomeInThousands'\nmutated_data &lt;- data_ex3 %&gt;% mutate(IncomeInThousands = Income / 1000)\nmutated_data\n\n# A tibble: 4 × 5\n     ID   Age Gender Income IncomeInThousands\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;             &lt;dbl&gt;\n1     1    21 Male    50000                50\n2     2    35 Female  80000                80\n3     3    58 Male   120000               120\n4     4    40 Female  75000                75\n\n# Use arrange() to sort data by 'Income'\narranged_data &lt;- data_ex3 %&gt;% arrange(Income)\narranged_data\n\n# A tibble: 4 × 4\n     ID   Age Gender Income\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1     1    21 Male    50000\n2     4    40 Female  75000\n3     2    35 Female  80000\n4     3    58 Male   120000\n\n# Use summarise() to get the mean 'Income'\nsummary_data &lt;- data_ex3 %&gt;% summarise(MeanIncome = mean(Income))\nsummary_data\n\n# A tibble: 1 × 1\n  MeanIncome\n       &lt;dbl&gt;\n1      81250\n\n# Use group_by() and summarise() to get the mean 'Income' for each 'Gender'\ngrouped_data &lt;- data_ex3 %&gt;% \n                  group_by(Gender) %&gt;% \n                  summarise(MeanIncome = mean(Income))\ngrouped_data\n\n# A tibble: 2 × 2\n  Gender MeanIncome\n  &lt;chr&gt;       &lt;dbl&gt;\n1 Female      77500\n2 Male        85000\n\n\nIn these examples, select is used to choose specific columns, filter is used to select rows based on a condition, mutate is used to create a new column, arrange is used to sort data, summarise is used to calculate summary statistics, and group_by is used to perform operations on groups of data."
  },
  {
    "objectID": "1-TidyverseRecap.html#example-ggplot",
    "href": "1-TidyverseRecap.html#example-ggplot",
    "title": "Tidyverse Recap",
    "section": "Example: ggplot",
    "text": "Example: ggplot"
  },
  {
    "objectID": "1-TidyverseRecap.html#question",
    "href": "1-TidyverseRecap.html#question",
    "title": "Tidyverse Recap",
    "section": "Question",
    "text": "Question\nSuppose we have a dataset called penguins and suppose we would like to study how the ratio of penguin body mass to flipper size differs across the species in the dataset. Rearrange the following steps in the pipeline into an order that accomplishes this goal.\n\n# a\narrange(avg_mass_flipper_ratio)\n\n# b\ngroup_by(species)\n\n# c\npenguins \n  \n# d\nsummarise(\n  avg_mass_flipper_ratioo = median(mass_flipper_ratio)\n)\n  \n# e\nmutate(\n  mass_flipper_ratio = body_mass_g/flipper_length_mm\n)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Introduction to Data Science (2)!",
    "section": "",
    "text": "Welcome to the course website for DS152 Introduction to Data Science (2).\nModule information\nLecture material (slides, notes, videos) are licensed under CC-BY-NC 4.0.\nContact: Niamh Cahill (niamh.cahill@mu.ie)"
  },
  {
    "objectID": "index.html#lecture-slides",
    "href": "index.html#lecture-slides",
    "title": "Welcome to Bayesian Data Analysis!",
    "section": "Lecture Slides",
    "text": "Lecture Slides\n\nWeek 1\n1a: Bayes Rule\n1b: Inferring a Binomial Probability\n\n\nWeek 2\n2a: Beta Binomial\n2b: Bayesian Inference\n\n\nWeek 3\n3: Single Parameter Normal\n\n\nWeek 4\n4: MCMC Sampling\n\n\nWeek 5\n5a: MCMC Diagnostics\n5b: Just Another Gibbs Sampler\n\n\nWeek 6\n6: Bayesian Linear Regression\n\n\nWeek 7\n7: Model Checking\n\n\nWeek 8\n8: Introducing Bayesian Hierarchical Modelling\n\n\nWeek 9\n9: Bayesian Hierarchical Regression Modelling\n\n\nWeek 10\n10: Bayesian Generalised Linear Models (GLMs)\n\n\nWeek 11\n11: Bayesian Hierarchical Modelling - GLM"
  },
  {
    "objectID": "index.html#tutorials",
    "href": "index.html#tutorials",
    "title": "Welcome to Bayesian Data Analysis!",
    "section": "Tutorials",
    "text": "Tutorials\nTutorial Sheet 1: Bayesian inference using binomial and Poisson models\n\nTutorial Sheet 2: Bayesian Model for Multiple Proportions - Email Campaign Click-Through Rates\nTutorial Sheet 3: Bayesian Regression Model - Fisherys Data\nTutorial Sheet 4: Bayesian Hierarchical Regression Modelling - Simulation"
  },
  {
    "objectID": "index.html#assignments",
    "href": "index.html#assignments",
    "title": "Welcome to Introduction to Data Science (2)!",
    "section": "Assignments",
    "text": "Assignments\nAssignment 1: Tidyverse Recap"
  },
  {
    "objectID": "index.html#lecture-notes",
    "href": "index.html#lecture-notes",
    "title": "Welcome to Introduction to Data Science (2)!",
    "section": "Lecture Notes",
    "text": "Lecture Notes\n1: Tidyverse Recap\n2: Correlation (and Causation)\n3: Multiple Regression Models\n4: Sampling Principles and Strategies\n5: Observational Studies"
  },
  {
    "objectID": "0-Information.html#course-organisation",
    "href": "0-Information.html#course-organisation",
    "title": "Module Information",
    "section": "",
    "text": "Lecture and Lab Timetable\n\nMonday 11am (Lecture, CB8), Wednesday 2pm (Lecture, CH)\nTuesday 3pm (Lab, TSI239), Friday 9am (Lab, TSI239)\n\nLabs will start in week 2\nPlease confirm your choice on Moodle.\n\n\nTutorials\n\nTuesdays @ 9am, 10am, 2pm, 3pm, 5pm, Wednesdays @ 12pm, 1pm\nTutorials – starting week 4 (24/02/2025)\n\nOffice hours\n\nBy appointment"
  },
  {
    "objectID": "0-Information.html#course-organisation-1",
    "href": "0-Information.html#course-organisation-1",
    "title": "Module Information",
    "section": "Course organisation",
    "text": "Course organisation\nMarks\n\n10% – compulsory assignments\n10% – quizzes\n15% – mid-term exam (March 28th, MCQ)\n65% – final exam (date TBC)\n\nNotes\n\nNotes will be a combination of handouts posted on Moodle and notes taken down in class\n\nAssignments\n\nWill be uploaded PDFs\n\nRecommended reading\n\nR for Data Science (https://r4ds.had.co.nz)"
  },
  {
    "objectID": "0-Information.html#diversity-inclusion",
    "href": "0-Information.html#diversity-inclusion",
    "title": "Module Information",
    "section": "Diversity & inclusion",
    "text": "Diversity & inclusion\nIt is my intent to present materials and activities that are respectful of diversity: gender identity, sexuality, disability, age, socioeconomic status, ethnicity, race, nationality, religion, and culture. I may not always get this right so please let me know ways to improve the effectiveness of the course for you personally, or for other students or student groups.\nTo help with this:\n\nIf you have a name that differs from those that appear in your official University records, please let me know!\nPlease let me know your preferred pronouns if you wish to do so.\nI (like many people) am still in the process of learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please talk to me about it."
  },
  {
    "objectID": "0-Information.html",
    "href": "0-Information.html",
    "title": "Module Information",
    "section": "",
    "text": "Lecture and Lab Timetable\n\nMonday 11am (Lecture, CB8), Wednesday 2pm (Lecture, CH)\nTuesday 3pm (Lab, TSI239), Friday 9am (Lab, TSI239)\n\nLabs will start in week 2\nPlease confirm your choice on Moodle.\n\n\nTutorials\n\nTuesdays @ 9am, 10am, 2pm, 3pm, 5pm, Wednesdays @ 12pm, 1pm\nTutorials – starting week 4 (24/02/2025)\n\nOffice hours\n\nBy appointment"
  },
  {
    "objectID": "1-TidyverseRecap.html#what-is-tidyverse",
    "href": "1-TidyverseRecap.html#what-is-tidyverse",
    "title": "Tidyverse Recap",
    "section": "",
    "text": "The tidyverse is a collection of R packages designed for data science.\nAll packages share an underlying design philosophy, grammar, and data structures.\nIt emphasizes tidy data in data frames, performs operations one step at a time, connects with pipes and makes code human readable."
  },
  {
    "objectID": "1-TidyverseRecap.html#key-packages-in-tidyverse",
    "href": "1-TidyverseRecap.html#key-packages-in-tidyverse",
    "title": "Tidyverse Recap",
    "section": "Key Packages in tidyverse",
    "text": "Key Packages in tidyverse\n\nreadr: Used for importing data.\ntidyr: Used for tidying and reshaping data.\ndplyr: Used for data transformation.\nggplot2: Used for data visualization.\nmagrittr: Provides the pipe operator (%&gt;%) or (|&gt;) which is used to chain together sequences of operations."
  },
  {
    "objectID": "1-TidyverseRecap.html#importing-data-with-readr",
    "href": "1-TidyverseRecap.html#importing-data-with-readr",
    "title": "Tidyverse Recap",
    "section": "Importing Data with readr",
    "text": "Importing Data with readr\n\nreadr provides faster and consistent replacements for data import functions in base R.\nIt fits into the tidyverse naturally and extends neatly into other data types.\nExample: read_csv(file, show_col_types = FALSE)."
  },
  {
    "objectID": "1-TidyverseRecap.html#tidying-data-with-tidyr",
    "href": "1-TidyverseRecap.html#tidying-data-with-tidyr",
    "title": "Tidyverse Recap",
    "section": "Tidying Data with tidyr",
    "text": "Tidying Data with tidyr\n\ntidyr provides a set of functions that help to tidy data.\nTidy data is data where every column is a variable, every row is an observation, and every cell is a single value.\n\n\ntidyr: pivot\n\n# Load the tidyr package\nlibrary(tidyr)\n\n\n# Assume we have a dataset 'data' with 'ID1', 'ID2', 'x', and 'y' columns\ndata_ex1 &lt;- tibble(ID1 = rep(LETTERS[1:4],times = 3), \n                   ID2 = rep(letters[1:3], each = 4), \n                   x = 1:12, \n                   y = 21:32)\n\nprint(data_ex1)\n\n# A tibble: 12 × 4\n   ID1   ID2       x     y\n   &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt;\n 1 A     a         1    21\n 2 B     a         2    22\n 3 C     a         3    23\n 4 D     a         4    24\n 5 A     b         5    25\n 6 B     b         6    26\n 7 C     b         7    27\n 8 D     b         8    28\n 9 A     c         9    29\n10 B     c        10    30\n11 C     c        11    31\n12 D     c        12    32\n\n# Use pivot_longer() to convert wide data to long format\ndata_long &lt;- data_ex1 %&gt;% pivot_longer(cols = c(\"x\", \"y\"), \n                                       names_to = \"Variable\", \n                                       values_to = \"Value\")\n\n# Print the long format data\nprint(data_long)\n\n# A tibble: 24 × 4\n   ID1   ID2   Variable Value\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;    &lt;int&gt;\n 1 A     a     x            1\n 2 A     a     y           21\n 3 B     a     x            2\n 4 B     a     y           22\n 5 C     a     x            3\n 6 C     a     y           23\n 7 D     a     x            4\n 8 D     a     y           24\n 9 A     b     x            5\n10 A     b     y           25\n# ℹ 14 more rows\n\n# Use pivot_wider() to convert long data back to wide format\ndata_wide &lt;- data_long %&gt;% pivot_wider(names_from = Variable,\n                                       values_from = Value)\n\n# Print the wide format data\nprint(data_wide)\n\n# A tibble: 12 × 4\n   ID1   ID2       x     y\n   &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt;\n 1 A     a         1    21\n 2 B     a         2    22\n 3 C     a         3    23\n 4 D     a         4    24\n 5 A     b         5    25\n 6 B     b         6    26\n 7 C     b         7    27\n 8 D     b         8    28\n 9 A     c         9    29\n10 B     c        10    30\n11 C     c        11    31\n12 D     c        12    32\n\n\nIn this example, pivot_longer is used to convert the wide format data to long format, where each row is a single observation associated with the variables ID1, ID2, Variable (containing the original column names ‘x’ and ‘y’), and Value (containing the values from ‘x’ and ‘y’ columns). We can then also convert back to wide format using pivot_wider.\n\n\ntidyr: separate\n\n# Load the tidyr package\nlibrary(tidyr)\n\n# Assume we have a dataset 'dataNew' with a 'datetime' column\ndata_ex2 &lt;- tibble(datetime = \n                    c(\"2016-01-01 07:30:29\", \"2016-01-02 09:43:36\", \"2016-01-03 13:59:00\"), \n                   event = c(\"u\", \"a\", \"l\"))\n\n# Use the separate() function from tidyr to separate the 'datetime' column into \n# 'date' and 'time'\n# Then separate 'time' into 'hour', 'min', 'second'\ndata_sep &lt;- data_ex2 %&gt;% \n              separate(datetime, c('date', 'time'), sep = ' ') %&gt;% \n              separate(time, c('hour', 'min', 'second'), sep = ':')\n\n# Print the new dataset\nprint(data_sep)\n\n# A tibble: 3 × 5\n  date       hour  min   second event\n  &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;\n1 2016-01-01 07    30    29     u    \n2 2016-01-02 09    43    36     a    \n3 2016-01-03 13    59    00     l    \n\n# change hour, min, second to numeric values\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ndata_sep %&gt;% mutate_at(vars(hour, min, second), as.numeric)\n\n# A tibble: 3 × 5\n  date        hour   min second event\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;\n1 2016-01-01     7    30     29 u    \n2 2016-01-02     9    43     36 a    \n3 2016-01-03    13    59      0 l"
  },
  {
    "objectID": "1-TidyverseRecap.html#transforming-data-with-dplyr",
    "href": "1-TidyverseRecap.html#transforming-data-with-dplyr",
    "title": "Tidyverse Recap",
    "section": "Transforming Data with dplyr",
    "text": "Transforming Data with dplyr\n\ndplyr is a grammar of data manipulation, providing a consistent set of verbs that solve the most common data manipulation challenges.\nExample: filter(data, condition).\n\n\nExample: dplyr\n\n# Load the dplyr package\nlibrary(dplyr)\n\n# Assume we have a dataset 'data' with 'ID', 'Age', 'Gender', and 'Income' columns\ndata_ex3 &lt;- tibble(ID = 1:4, \n                   Age = c(21, 35, 58, 40), \n                   Gender = c(\"Male\", \"Female\", \"Male\", \"Female\"), \n                   Income = c(50000, 80000, 120000, 75000))\n\n# Use select() to choose the 'ID' and 'Age' columns\nselected_data &lt;- data_ex3 %&gt;% select(ID, Age)\nselected_data\n\n# A tibble: 4 × 2\n     ID   Age\n  &lt;int&gt; &lt;dbl&gt;\n1     1    21\n2     2    35\n3     3    58\n4     4    40\n\n# Use filter() to get rows where 'Age' is greater than 30\nfiltered_data &lt;- data_ex3 %&gt;% filter(Age &gt; 30)\nfiltered_data\n\n# A tibble: 3 × 4\n     ID   Age Gender Income\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1     2    35 Female  80000\n2     3    58 Male   120000\n3     4    40 Female  75000\n\n# Use mutate() to create a new column 'IncomeInThousands'\nmutated_data &lt;- data_ex3 %&gt;% mutate(IncomeInThousands = Income / 1000)\nmutated_data\n\n# A tibble: 4 × 5\n     ID   Age Gender Income IncomeInThousands\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;             &lt;dbl&gt;\n1     1    21 Male    50000                50\n2     2    35 Female  80000                80\n3     3    58 Male   120000               120\n4     4    40 Female  75000                75\n\n# Use arrange() to sort data by 'Income'\narranged_data &lt;- data_ex3 %&gt;% arrange(Income)\narranged_data\n\n# A tibble: 4 × 4\n     ID   Age Gender Income\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1     1    21 Male    50000\n2     4    40 Female  75000\n3     2    35 Female  80000\n4     3    58 Male   120000\n\n# Use summarise() to get the mean 'Income'\nsummary_data &lt;- data_ex3 %&gt;% summarise(MeanIncome = mean(Income))\nsummary_data\n\n# A tibble: 1 × 1\n  MeanIncome\n       &lt;dbl&gt;\n1      81250\n\n# Use group_by() and summarise() to get the mean 'Income' for each 'Gender'\ngrouped_data &lt;- data_ex3 %&gt;% \n                  group_by(Gender) %&gt;% \n                  summarise(MeanIncome = mean(Income))\ngrouped_data\n\n# A tibble: 2 × 2\n  Gender MeanIncome\n  &lt;chr&gt;       &lt;dbl&gt;\n1 Female      77500\n2 Male        85000\n\n\nIn these examples, select is used to choose specific columns, filter is used to select rows based on a condition, mutate is used to create a new column, arrange is used to sort data, summarise is used to calculate summary statistics, and group_by is used to perform operations on groups of data."
  },
  {
    "objectID": "1-TidyverseRecap.html#visualizing-data-with-ggplot2",
    "href": "1-TidyverseRecap.html#visualizing-data-with-ggplot2",
    "title": "Tidyverse Recap",
    "section": "Visualizing Data with ggplot2",
    "text": "Visualizing Data with ggplot2\n\nggplot2 is a system for declaratively creating graphics, based on “The Grammar of Graphics”.\nYou provide the data, tell ggplot2 how to map variables to aesthetics, what graphic to use, and it takes care of the details.\n\n\n\n\nExample: ggplot\nBasic scatter plot with a regression line\n\n# Load the ggplot2 package\nlibrary(ggplot2)\n\np1 &lt;- ggplot(mtcars, aes(x = mpg, y = hp)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\")\np1\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nHistogram\n\n# Load the ggplot2 package\nlibrary(ggplot2)\n\np2 &lt;- ggplot(mtcars, aes(x = mpg)) +\n  geom_histogram(binwidth = 2, fill = \"blue\", color = \"white\")\np2\n\n\n\n\n\n\n\n\nBoxplot\n\n# Load the ggplot2 package\nlibrary(ggplot2)\n\np3 &lt;- ggplot(mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_boxplot(fill = \"orange\", color = \"darkred\")\np3\n\n\n\n\n\n\n\n\nBar chart\n\n# Load the ggplot2 package\nlibrary(ggplot2)\n\np4 &lt;- ggplot(mtcars, aes(x = factor(cyl))) +\n  geom_bar(fill = \"steelblue\") +\n  labs(x = \"Number of cylinders\", y = \"Frequency\")\np4\n\n\n\n\n\n\n\n\nIn these examples, geom_point is used to create a scatter plot, geom_smooth with method = \"lm\" is used to add a linear regression line, geom_histogram is used to create a histogram, geom_boxplot is used to create a boxplot, and geom_bar is used to create a bar chart."
  },
  {
    "objectID": "1-TidyverseRecap.html#class-exercise",
    "href": "1-TidyverseRecap.html#class-exercise",
    "title": "Tidyverse Recap",
    "section": "Class Exercise",
    "text": "Class Exercise\nSuppose we have a dataset called penguins and suppose we would like to study how the ratio of penguin body mass to flipper size differs across the species in the dataset. Rearrange the following steps in the pipeline into an order that accomplishes this goal.\n\n# a\narrange(avg_mass_flipper_ratio)\n\n# b\ngroup_by(species) %&gt;% \n\n# c\npenguins %&gt;% \n  \n# d\nsummarise(\n  avg_mass_flipper_ratio = median(mass_flipper_ratio, na.rm = TRUE)\n) %&gt;% \n  \n# e\nmutate(\n  mass_flipper_ratio = body_mass_g/flipper_length_mm\n) %&gt;%"
  },
  {
    "objectID": "2-Correlation.html",
    "href": "2-Correlation.html",
    "title": "Correlation (and Causation)",
    "section": "",
    "text": "When we see a pattern, we don’t just say “how extraordinary!” and move on; instead, we try and attribute a cause!\nhttps://www.tylervigen.com/spurious-correlations"
  },
  {
    "objectID": "2-Correlation.html#correlation-vs.-causation",
    "href": "2-Correlation.html#correlation-vs.-causation",
    "title": "Causation and Correlation",
    "section": "",
    "text": "When we see a pattern, we don’t just say “how extraordinary!” and move on; instead, we try and attribute a cause!\n\nWe all draw conclusions on the basis of what we see\nBut it is important for us to remember that just because there is a correlation between two facts, there isn’t necessarily a cause/effect relationship between them.\n\nlistening to loud music and acne\nice cream consumption and shark attacks\nhand size and reading ability in children\n…\n\nThese variables are correlated, but one does not cause the other!\n\nhttps://www.tylervigen.com/spurious-correlations"
  },
  {
    "objectID": "2-Correlation.html#correlation",
    "href": "2-Correlation.html#correlation",
    "title": "Correlation (and Causation)",
    "section": "Correlation",
    "text": "Correlation\nCorrelation (r) quantifies the linear association between two quantitative variables.\n\nThe value of \\(r\\) is between -1 and 1.\n\\(r &gt;\\) 0 when \\(x\\) and \\(y\\) have a positive association.\n\\(r &lt;\\) 0 when \\(x\\) and \\(y\\) have a negative association.\n\\(r\\) = 1 means a perfect positive linear association.\n\\(r\\) = -1 means a perfect negative linear association.\n\\(r\\) = 0 indicates no linear association between \\(x\\) and \\(y\\).\nThe value of \\(r\\) is a measure of the extent to which \\(x\\) and \\(y\\) are linearly related."
  },
  {
    "objectID": "2-Correlation.html#what-are-the-correlation-values-here",
    "href": "2-Correlation.html#what-are-the-correlation-values-here",
    "title": "Correlation (and Causation)",
    "section": "What are the correlation values here?",
    "text": "What are the correlation values here?\nTask: Match the plot panel number to the letter with the correct correlation value.\n\n\n\n\n\n\n\n\n\n          A           B           C           D           E           F \n-0.02770462  0.96643642 -0.69813746  0.04969697 -0.96735724  0.11948059 \n          G           H \n-0.39421900  0.67692544"
  },
  {
    "objectID": "2-Correlation.html#what-are-the-correlation-values-here-1",
    "href": "2-Correlation.html#what-are-the-correlation-values-here-1",
    "title": "Causation and Correlation",
    "section": "What are the correlation values here?",
    "text": "What are the correlation values here?"
  },
  {
    "objectID": "2-Correlation.html#what-are-the-correlation-values-here-2",
    "href": "2-Correlation.html#what-are-the-correlation-values-here-2",
    "title": "Causation and Correlation",
    "section": "What are the correlation values here?",
    "text": "What are the correlation values here?"
  },
  {
    "objectID": "2-Correlation.html#what-are-the-correlation-values-here-3",
    "href": "2-Correlation.html#what-are-the-correlation-values-here-3",
    "title": "Causation and Correlation",
    "section": "What are the correlation values here?",
    "text": "What are the correlation values here?"
  },
  {
    "objectID": "2-Correlation.html#what-are-the-correlation-values-here-4",
    "href": "2-Correlation.html#what-are-the-correlation-values-here-4",
    "title": "Causation and Correlation",
    "section": "What are the correlation values here?",
    "text": "What are the correlation values here?"
  },
  {
    "objectID": "2-Correlation.html#what-are-the-correlation-values-here-5",
    "href": "2-Correlation.html#what-are-the-correlation-values-here-5",
    "title": "Causation and Correlation",
    "section": "What are the correlation values here?",
    "text": "What are the correlation values here?"
  },
  {
    "objectID": "2-Correlation.html#what-are-the-correlation-values-here-6",
    "href": "2-Correlation.html#what-are-the-correlation-values-here-6",
    "title": "Causation and Correlation",
    "section": "What are the correlation values here?",
    "text": "What are the correlation values here?"
  },
  {
    "objectID": "2-Correlation.html#what-are-the-correlation-values-here-7",
    "href": "2-Correlation.html#what-are-the-correlation-values-here-7",
    "title": "Causation and Correlation",
    "section": "What are the correlation values here?",
    "text": "What are the correlation values here?"
  },
  {
    "objectID": "2-Correlation.html#calculating-the-correlation-coefficient",
    "href": "2-Correlation.html#calculating-the-correlation-coefficient",
    "title": "Correlation (and Causation)",
    "section": "Calculating the correlation coefficient",
    "text": "Calculating the correlation coefficient\nWe denote \\[\\begin{eqnarray*}\nS_{xx} &=& \\sum_{i=1}^{n}(x_i - \\bar{x})^2 = \\sum_{i=1}^{n}x_i^2 - n\\bar{x}^2 \\\\\nS_{yy} &=& \\sum_{i=1}^{n}(y_i - \\bar{y})^2 = \\sum_{i=1}^{n}y_i^2 - n\\bar{y}^2 \\\\\nS_{xy} &=& \\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y}) = \\sum_{i=1}^{n}x_iy_i - n\\bar{x}\\bar{y}\n\\end{eqnarray*}\\]\nThen \\[\\begin{eqnarray*}\nr &=& \\frac{Sxy}{\\sqrt{SxxSyy}}     \n\\end{eqnarray*}\\]\n\nExample: Calculate the correlation between \\(x\\) and \\(y\\)\n\n\n\n\n\nx\n2\n4\n1\n6\n7\n\n\ny\n3\n4\n0\n8\n8\n\n\n\n\n\n\n\\[\\begin{align*}\n&\\sum_{i=1}^{n}x_i^2=106, \\sum_{i=1}^{n}y_i^2=153, \\sum_{i=1}^{n}x_iy_i=126, \\bar{x}=4,\\bar{y}=4.6\\\\\n&S_{xx} = \\sum_{i=1}^{n}x_i^2 - n\\bar{x}^2 = 26, S_{yy} = \\sum_{i=1}^{n}y_i^2 - n\\bar{y}^2 = 47.2\\\\\n&S_{xy} = \\sum_{i=1}^{n}x_iy_i - n\\bar{x}\\bar{y} = 34\\\\\n\\\\\n&r = \\frac{Sxy}{\\sqrt{SxxSyy}} = \\frac{34}{\\sqrt{26 \\times 47.2}} = 0.97        \n\\end{align*}\\]\n\n\nExample: Change of scale and the correlation coefficient\nThe distance of the race and the time it took to complete was recorded for five races in kilometres and seconds respectively. The correlation was calculated between the two variables. The data set was also converted into miles (\\(\\times\\) 0.621371192) and minutes (/60) and the correlation was re-calculated.\n\n\n\n\n\nKilometres\nSeconds\nMiles\nMinutes\n\n\n\n\n0.1\n10\n0.0621371\n0.1666667\n\n\n0.4\n120\n0.2485485\n2.0000000\n\n\n0.8\n300\n0.4970970\n5.0000000\n\n\n1.6\n535\n0.9941939\n8.9166667\n\n\n3.0\n950\n1.8641136\n15.8333333\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nIf \\(x\\) and \\(y\\) measurement units are changed, correlation does not change.\nIf \\(x\\) and \\(y\\) are reversed, i.e. correlation of \\(y\\) and \\(x\\), the correlation does not change.\nCorrelation is a measure of linear association. It does not establish causation.\nTwo variables, x and y, could be highly correlated because there is another variable, z, having an impact on both x and y."
  },
  {
    "objectID": "2-Correlation.html#example-calculate-the-correlation-between-x-and-y",
    "href": "2-Correlation.html#example-calculate-the-correlation-between-x-and-y",
    "title": "Causation and Correlation",
    "section": "Example: Calculate the correlation between \\(x\\) and \\(y\\)",
    "text": "Example: Calculate the correlation between \\(x\\) and \\(y\\)\n\n\n\n\n\nx\n2\n4\n1\n6\n7\n\n\ny\n3\n4\n0\n8\n8\n\n\n\n\n\n\n\\[\\begin{align*}\n&\\sum_{i=1}^{n}x_i^2=106, \\sum_{i=1}^{n}y_i^2=153, \\sum_{i=1}^{n}x_iy_i=126, \\bar{x}=4,\\bar{y}=4.6\\\\\n&S_{xx} = \\sum_{i=1}^{n}x_i^2 - n\\bar{x}^2 = 26, S_{yy} = \\sum_{i=1}^{n}y_i^2 - n\\bar{y}^2 = 47.2\\\\\n&S_{xy} = \\sum_{i=1}^{n}x_iy_i - n\\bar{x}\\bar{y} = 34\\\\\n\\\\\n&r = \\frac{Sxy}{\\sqrt{SxxSyy}} = \\frac{34}{\\sqrt{26 \\times 47.2}} = 0.97        \n\\end{align*}\\]"
  },
  {
    "objectID": "2-Correlation.html#notes-on-correlation",
    "href": "2-Correlation.html#notes-on-correlation",
    "title": "Causation and Correlation",
    "section": "Notes on correlation",
    "text": "Notes on correlation\n\nIf \\(x\\) and \\(y\\) measurement units are changed, correlation does not change.\nIf \\(x\\) and \\(y\\) are reversed, i.e. correlation of \\(y\\) and \\(x\\), the correlation does not change.\nCorrelation is a measure of linear association. It does not establish causation.\nTwo variables, x and y, could be highly correlated because there is another variable, z, having an impact on both x and y.\n\nExample, we have not established that extra height causes bigger feet. In fact genetic factors cause both."
  },
  {
    "objectID": "2-Correlation.html#example-change-of-scale-and-the-correlation-coefficient",
    "href": "2-Correlation.html#example-change-of-scale-and-the-correlation-coefficient",
    "title": "Causation and Correlation",
    "section": "Example: Change of scale and the correlation coefficient",
    "text": "Example: Change of scale and the correlation coefficient\nThe distance of the race and the time it took to complete was recorded for five races in kilometres and seconds respectively. The correlation was calculated between the two variables. The data set was also converted into miles (\\(\\times\\) 0.621371192) and minutes (/60) and the correlation was re-calculated.\n\n\n\n\n\nKilometres\nSeconds\nMiles\nMinutes\n\n\n\n\n0.1\n10\n0.0621371\n0.1666667\n\n\n0.4\n120\n0.2485485\n2.0000000\n\n\n0.8\n300\n0.4970970\n5.0000000\n\n\n1.6\n535\n0.9941939\n8.9166667\n\n\n3.0\n950\n1.8641136\n15.8333333"
  },
  {
    "objectID": "2-Correlation.html#explore-spurious-correlations",
    "href": "2-Correlation.html#explore-spurious-correlations",
    "title": "Causation and Correlation",
    "section": "Explore Spurious Correlations",
    "text": "Explore Spurious Correlations\nFind two correlated variables from: https://www.tylervigen.com/spurious-correlations.\nCreate a scatter plot and find the correlation."
  },
  {
    "objectID": "2-Correlation.html#association-between-pga-golfers-accuracy-and-driving-distance",
    "href": "2-Correlation.html#association-between-pga-golfers-accuracy-and-driving-distance",
    "title": "Causation and Correlation",
    "section": "Association between PGA golfer’s accuracy and driving distance",
    "text": "Association between PGA golfer’s accuracy and driving distance\nDescription:\nThe data set `golf’ was taken from PGA Tour Recordsof 195 golf rounds by PGA players in an attempt to explain what golf attributes contribute the most to low scores."
  },
  {
    "objectID": "2-Correlation.html#spurious-correlations",
    "href": "2-Correlation.html#spurious-correlations",
    "title": "Correlation (and Causation)",
    "section": "Spurious Correlations",
    "text": "Spurious Correlations\nA spurious correlation is a statistical relationship between two variables that appears to be meaningful but is actually caused by coincidence or the influence of a third (confounding) variable. This misleading association can arise due to random chance, indirect causation, or omitted variables.\nFor example, there may be a strong correlation between ice cream sales and drowning incidents, but this does not mean one causes the other. Instead, a third factor—hot weather—increases both ice cream sales and swimming activity, which in turn raises the risk of drowning.\nSpurious correlations can often be identified through deeper statistical analysis, such as controlling for confounding variables or using causal inference techniques.\nTask: Find two correlated variables from: https://www.tylervigen.com/spurious-correlations. Create a scatter plot and find the correlation."
  },
  {
    "objectID": "2-Correlation.html#what-is-the-capital-of-france",
    "href": "2-Correlation.html#what-is-the-capital-of-france",
    "title": "Causation and Correlation",
    "section": "What is the capital of France?",
    "text": "What is the capital of France?\n\nLondon\nParis\nBerlin\nMadrid"
  },
  {
    "objectID": "2-Correlation.html#example-data-1-association-between-pga-golfers-accuracy-and-driving-distance",
    "href": "2-Correlation.html#example-data-1-association-between-pga-golfers-accuracy-and-driving-distance",
    "title": "Causation and Correlation",
    "section": "Example Data (1): Association between PGA golfer’s accuracy and driving distance",
    "text": "Example Data (1): Association between PGA golfer’s accuracy and driving distance\nDescription:\nThe data set `golf’ was taken from PGA Tour Recordsof 195 golf rounds by PGA players in an attempt to explain what golf attributes contribute the most to low scores."
  },
  {
    "objectID": "2-Correlation.html#example-data-2-association-between-pga-golfers-accuracy-and-driving-distance",
    "href": "2-Correlation.html#example-data-2-association-between-pga-golfers-accuracy-and-driving-distance",
    "title": "Causation and Correlation",
    "section": "Example Data (2): Association between PGA golfer’s accuracy and driving distance",
    "text": "Example Data (2): Association between PGA golfer’s accuracy and driving distance"
  },
  {
    "objectID": "2-Correlation.html#example-data-1-what-is-the-association-between-pga-golfers-accuracy-and-driving-distance",
    "href": "2-Correlation.html#example-data-1-what-is-the-association-between-pga-golfers-accuracy-and-driving-distance",
    "title": "Correlation (and Causation)",
    "section": "Example Data (1): What is the association between PGA golfer’s accuracy and driving distance?",
    "text": "Example Data (1): What is the association between PGA golfer’s accuracy and driving distance?\nThe data set `golf’ was taken from PGA Tour Recordsof 195 golf rounds by PGA players in an attempt to explain what golf attributes contribute the most to low scores."
  },
  {
    "objectID": "2-Correlation.html#example-data-2-what-is-the-relationship-between-cars-weights-and-their-mileage",
    "href": "2-Correlation.html#example-data-2-what-is-the-relationship-between-cars-weights-and-their-mileage",
    "title": "Correlation (and Causation)",
    "section": "Example Data (2): What is the relationship between cars’ weights and their mileage?",
    "text": "Example Data (2): What is the relationship between cars’ weights and their mileage?\nThe data mtcars was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973–74 models)."
  },
  {
    "objectID": "0-Information.html#assessment-and-materials",
    "href": "0-Information.html#assessment-and-materials",
    "title": "Module Information",
    "section": "Assessment and Materials",
    "text": "Assessment and Materials\nNotes\n\nNotes will be a combination of links posted on Moodle and notes taken down in class\n\nAssignments\n\nWill be uploaded PDFs.\n\nLab quizzes\n\nWill be Moodle Quiz format with multiple attempts allowed until the end of the semester.\n\nMidterm Exam\n\nWill be a Moodle Quiz MCQ.\n\nMarks\n\n10% – compulsory assignments\n10% – quizzes\n15% – mid-term exam (March 28th, MCQ)\n65% – final exam (date TBC)\n\nRecommended reading\n\nR for Data Science (https://r4ds.had.co.nz)"
  },
  {
    "objectID": "0-Information.html#topics-covered",
    "href": "0-Information.html#topics-covered",
    "title": "Module Information",
    "section": "Topics Covered",
    "text": "Topics Covered\n\nTidyverse Recap\nCorrelation (and Causation)\nObservational Studies\nControlled Experiments\nSurveys\nData Privacy and Anonymisation\nSupervised and Unsupervised Learning\nPredictive Analytics\nImage and Text Analysis"
  },
  {
    "objectID": "0-Information.html#materials-and-assessments",
    "href": "0-Information.html#materials-and-assessments",
    "title": "Module Information",
    "section": "Materials and Assessments",
    "text": "Materials and Assessments\nNotes\n\nNotes will be a combination of links posted on Moodle and notes taken down in class\n\nTutorial Questions\n\nPractice tutorial questions will be posted to Moodle.\n\nAssignments\n\nAssignment questions will be posted to Moodle and answers should be uploaded as PDFs.\n\nLab Quizzes\n\nEach lab will have an associated quiz that will be a Moodle Quiz format with multiple attempts allowed until the end of the semester.\n\nMidterm Exam\n\nThe midterm exam will be a Moodle Quiz MCQ.\n\nMarks\n\n10% – compulsory assignments\n10% – quizzes\n15% – mid-term exam (March 28th, MCQ)\n65% – final exam (date TBC)\n\nRecommended reading\n\nR for Data Science (https://r4ds.had.co.nz)"
  },
  {
    "objectID": "3-MultipleRegression.html",
    "href": "3-MultipleRegression.html",
    "title": "Multiple Regression",
    "section": "",
    "text": "We have seen last semester, in DS151, how to study the relationship between two variables using linear regression.\nWe can create a linear regression model that includes a predictor, such that \\[\\hat{\\mbox{y}}=\\beta_0+\\beta_1\\mbox{x}\\]"
  },
  {
    "objectID": "3-MultipleRegression.html#recap-simple-regression",
    "href": "3-MultipleRegression.html#recap-simple-regression",
    "title": "Multiple Regression",
    "section": "",
    "text": "We have seen last semester, in DS151, how to study the relationship between two variables using linear regression.\nWe can create a linear regression model that includes a predictor, such that \\[\\hat{\\mbox{y}}=\\beta_0+\\beta_1\\mbox{x}\\]"
  },
  {
    "objectID": "3-MultipleRegression.html#multiple-regression-example-1-the-crab-dataset",
    "href": "3-MultipleRegression.html#multiple-regression-example-1-the-crab-dataset",
    "title": "Multiple Regression",
    "section": "Multiple Regression Example 1: The Crab Dataset",
    "text": "Multiple Regression Example 1: The Crab Dataset\nIn many research and applied fields, many different variables are measured at the same time, for various reasons. In many cases, we may actually have too many variables and may end up selecting a subset of them (only the most important) to proceed with the analysis. (NB: there are many ways to define variable importance, and we will look at some of them in depth throughout the Data Science programme).\nLet’s have a look at an example where we have many predictor variables. The crabs dataset from package MASS has 200 observations on 2 qualitative variables (species colour and sex), and 5 morphological measurements (frontal lobe size, rear width, carapace length and width, and body depth) of crabs of the species Leptograspus variegatus.\n\n\n\ncrab_dat &lt;- MASS::crabs\nglimpse(crab_dat)\n\nRows: 200\nColumns: 8\n$ sp    &lt;fct&gt; B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B…\n$ sex   &lt;fct&gt; M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M…\n$ index &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 1…\n$ FL    &lt;dbl&gt; 8.1, 8.8, 9.2, 9.6, 9.8, 10.8, 11.1, 11.6, 11.8, 11.8, 12.2, 12.…\n$ RW    &lt;dbl&gt; 6.7, 7.7, 7.8, 7.9, 8.0, 9.0, 9.9, 9.1, 9.6, 10.5, 10.8, 11.0, 1…\n$ CL    &lt;dbl&gt; 16.1, 18.1, 19.0, 20.1, 20.3, 23.0, 23.8, 24.5, 24.2, 25.2, 27.3…\n$ CW    &lt;dbl&gt; 19.0, 20.8, 22.4, 23.1, 23.0, 26.5, 27.1, 28.4, 27.8, 29.3, 31.6…\n$ BD    &lt;dbl&gt; 7.0, 7.4, 7.7, 8.2, 8.2, 9.8, 9.8, 10.4, 9.7, 10.3, 10.9, 11.4, …\n\nhead(crab_dat)\n\n  sp sex index   FL  RW   CL   CW  BD\n1  B   M     1  8.1 6.7 16.1 19.0 7.0\n2  B   M     2  8.8 7.7 18.1 20.8 7.4\n3  B   M     3  9.2 7.8 19.0 22.4 7.7\n4  B   M     4  9.6 7.9 20.1 23.1 8.2\n5  B   M     5  9.8 8.0 20.3 23.0 8.2\n6  B   M     6 10.8 9.0 23.0 26.5 9.8\n\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\nlibrary(ggplot2)\nggpairs(crab_dat, columns = 4:8, ggplot2::aes(colour=sex)) +theme_bw()\n\n\n\n\n\n\n\n\nImagine we are interested in predicting the carapace length (CL) of this species of crab. We can create a linear regression model that includes multiple predictors, such as \\[\\hat{\\mbox{CL}}=\\beta_0+\\beta_1\\mbox{FL}+\\beta_2\\mbox{RW}+\\beta_3\\mbox{CW}+\\beta_4\\mbox{BD}\\] This model can be fitted in R by executing:\n\nfit1 &lt;- lm(CL ~ FL + RW + CW + BD, data = crab_dat)\nfit1 %&gt;% coef\n\n(Intercept)          FL          RW          CW          BD \n  0.3163364   0.2648933  -0.1778948   0.6402190   0.4714152 \n\n\nWe have that \\[\\hat{\\mbox{CL}}=0.32+0.26\\mbox{FL}-0.18\\mbox{RW}+0.64\\mbox{CW}+0.47\\mbox{BD}\\].\nWe can plot the actual observations versus the predicted ones to see how well we did:\n\ncrab_dat &lt;- crab_dat %&gt;% \n              mutate(CL_pred = predict(fit1))\n\nggplot(data = crab_dat,\n       mapping = aes(x = CL, y = CL_pred, colour = sex)) +\n  theme_bw() +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  xlab(\"Observed CL\") + \n  ylab(\"Predicted CL\")\n\n\n\n\n\n\n\n\nHow well would we have done if we only used one predictor variable, say RW?\n\nfit2 &lt;- lm(CL ~ RW, data = crab_dat)\n\ncrab_dat &lt;- crab_dat %&gt;% \n              mutate(CL_pred2 = predict(fit2))\n\nggplot(data = crab_dat,\n       mapping = aes(x = CL, y = CL_pred2, colour = sex)) +\n  theme_bw() +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  xlab(\"Observed CL\") + \n  ylab(\"Predicted CL\")\n\n\n\n\n\n\n\n\nNow what about the categorical variables, namely sex and sp (species colour)? We can also include them in our model. Because they are categorical, they will only influence the overall mean response for each category, and hence we don’t estimate a slope for them, i.e. \\[\\hat{\\mbox{CL}}=\\beta_0+\\beta_1\\mbox{FL}+\\beta_2\\mbox{RW}+\\beta_3\\mbox{CW}+\\beta_4\\mbox{BD}+\\beta_5I(\\mbox{sp}=\\mbox{O})+\\beta_6I(\\mbox{sex}=\\mbox{M})\\] (NB: \\(I(\\mbox{sex}=\\mbox{M})\\) is an indicator function, which is equal to 1 if sex is equal to M and zero otherwise. The same type of interpretation can be drawn from \\(I(\\mbox{sp}=\\mbox{O})\\))\n\nfit3 &lt;- lm(CL ~ FL + RW + CW + BD + sp + sex, data = crab_dat)\nfit3 %&gt;% coef\n\n(Intercept)          FL          RW          CW          BD         spO \n-0.19583237  0.21392757 -0.03750446  0.65118521  0.38956671  0.16389052 \n       sexM \n 0.37020979 \n\ncrab_dat &lt;- crab_dat %&gt;% \n              mutate(CL_pred3 = predict(fit3))\n\nggplot(data = crab_dat,\n       mapping = aes(x = CL, y = CL_pred3, colour = sex)) +\n  theme_bw() +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  xlab(\"Observed CL\") + \n  ylab(\"Predicted CL\")\n\n\n\n\n\n\n\n\nHow do we interpret the coefficients related to sp and sex? They are the increase (or decrease, if negative) in the overall mean if a crab is orange (O) or male (M).\nOne way of comparing how well our modelling strategies did in terms of predictions is to sum the squared discrepancies (residuals) between predicted and observed values, and see which one is smaller.\n\ndiscrepancy_fit1 &lt;- sum(residuals(fit1)^2)\ndiscrepancy_fit2 &lt;- sum(residuals(fit2)^2)\ndiscrepancy_fit3 &lt;- sum(residuals(fit3)^2)\n\ndiscrepancy_fit1\n\n[1] 27.35058\n\ndiscrepancy_fit2\n\n[1] 2047.417\n\ndiscrepancy_fit3\n\n[1] 25.06863\n\n\nTypically we would split the data into training and test set, use only the training set to fit the model, and then perform this computation on both sets. We will see more details on how to compare the models in terms of their predictive power in the module Statistical Machine Learning. We will also see more details on this in the modules Linear Models I and II, how to properly test hypotheses, how to assess goodness-of-fit, what the important assumptions are and how to properly check them.\n\nLogistic Regression Example 2: The Iris Dataset\nWe explored this dataset initially last semester in DS151.\nIris is a genus of about 300 species of flowering plants, taking its name from the Greek word for a rainbow (which is also the name for the Greek goddess of rainbows, Iris). The flowers are very showy, and we would like to know if it is possible to identify some of the species based on measurements of different parts of the flowers.\nLet’s look at a famous example dataset that gives the measurements in centimeters of the variables:\nfor 50 flowers from each of three species if Iris: Iris setosa, Iris versicolor, and Iris virginica.\nThis dataset is available in base R as iris. Let’s have a look:\n\nlibrary(tidyverse)\nglimpse(iris)\n\nRows: 150\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n\nLet’s work, initially, only with species versicolor and virginica. We create a plot that shows in the \\(x\\) axis the petal length, and in the \\(y\\) axis only the values of 0 and 1, representing whether the observation belongs to class virginica or not (0 = the observation belongs to class versicolor; 1 = it belongs to class virginica). We call this a binary (or dummy) variable.\n\niris2 &lt;- iris %&gt;%\n  filter(Species != \"setosa\") %&gt;%\n  mutate(Species.binary = as.numeric(Species == \"virginica\"))\n  \nggplot(iris2, aes(x = Petal.Length, y = Species.binary)) +\n  geom_point() +\n  theme_bw()\n\n\n\n\n\n\n\n\nIt appears that as the petal length increases, so does the likelihood of belonging to the virginica class. We may interpret the numbers 0 and 1 here as the proportion \\(p\\) of plants that belong to class virginica and present a specific measurement of petal length.\nRemember. We transform the scale of the response in such a way that it will be bounded between 0 and 1, and hence our estimates will be sensical. The transformation we use is called the logit, and is the natural logarithm of the odds of belonging to class virginica: \\[\\displaystyle\\log\\left(\\frac{p}{1-p}\\right)=\\beta_0+\\beta_1*\\mbox{Petal.Length},\\] where \\(p\\) is the proportion of plants that belong to class virginica. This way, the estimated proportions will always be bounded between 0 and 1, and are now suitable to our problem. Let’s see how it looks like for our example:\n\nggplot(iris2, aes(x = Petal.Length, y = Species.binary)) +\n  geom_point() +\n  geom_smooth(method = glm, method.args = list(family = \"binomial\"), se = FALSE) +\n  theme_bw()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nIncluding more predictors\nIt is frequently of interest to use multiple covariates to improve our predictive power. Let’s fit a logistic regression model including all four covariates as predictors in our model: \\[\\displaystyle\\log\\left(\\frac{p}{1-p}\\right)=\\beta_0+\\beta_1*\\mbox{Sepal.Length}+\\beta_2*\\mbox{Sepal.Width}+\\beta_3*\\mbox{Petal.Length}+\\beta_4*\\mbox{Petal.Width}\\]\n\nfull_logistic_reg &lt;- glm(Species.binary ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width,\n                         family = binomial, data = iris2)\nfull_logistic_reg %&gt;% coef %&gt;% round(digits = 4)\n\n (Intercept) Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n    -42.6378      -2.4652      -6.6809       9.4294      18.2861 \n\n\nHow well did we do?\n\niris_predict &lt;- iris2 %&gt;% \n                  mutate(p_hat = predict(full_logistic_reg, type = \"response\") %&gt;% round(2),\n                         Species_pred = ifelse(p_hat &gt;= 0.5,\"virginica\",\"versicolor\"))\n\nn_correct_full &lt;- sum(iris_predict$Species_pred == iris_predict$Species)\nn_correct_full\n\n[1] 98\n\n\nOur model now correctly predicts the class of 98 out of 100 observations."
  },
  {
    "objectID": "3-MultipleRegression.html#example-3-the-wine-dataset",
    "href": "3-MultipleRegression.html#example-3-the-wine-dataset",
    "title": "Multiple Regression",
    "section": "Example 3: The Wine Dataset",
    "text": "Example 3: The Wine Dataset"
  },
  {
    "objectID": "3-MultipleRegression.html#logistic-regression-the-wine-dataset",
    "href": "3-MultipleRegression.html#logistic-regression-the-wine-dataset",
    "title": "Multiple Regression",
    "section": "Logistic Regression: The Wine dataset",
    "text": "Logistic Regression: The Wine dataset\nThe wine dataset contains the results of a chemical analysis of wines grown in a specific area of Italy. Three types of wine are represented in the 178 samples, with the results of 13 chemical analyses recorded for each sample. The Type variable has been transformed into a categoric variable.\nThe data contains no missing values and consits of only numeric data, with a three class target variable (Type) for classification.\nLet’s have a glimpse at the dataset:\n\nwine &lt;- read_csv(\"https://www.dropbox.com/s/l5x4ur06gfhpg0h/wine.csv?raw=1\") \n\nRows: 178 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): Type\ndbl (13): Alcohol, Malic, Ash, Alcalinity, Magnesium, Phenols, Flavanoids, N...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(wine)\n\nRows: 178\nColumns: 14\n$ Type                 &lt;chr&gt; \"type1\", \"type1\", \"type1\", \"type1\", \"type1\", \"typ…\n$ Alcohol              &lt;dbl&gt; 14.23, 13.20, 13.16, 14.37, 13.24, 14.20, 14.39, …\n$ Malic                &lt;dbl&gt; 1.71, 1.78, 2.36, 1.95, 2.59, 1.76, 1.87, 2.15, 1…\n$ Ash                  &lt;dbl&gt; 2.43, 2.14, 2.67, 2.50, 2.87, 2.45, 2.45, 2.61, 2…\n$ Alcalinity           &lt;dbl&gt; 15.6, 11.2, 18.6, 16.8, 21.0, 15.2, 14.6, 17.6, 1…\n$ Magnesium            &lt;dbl&gt; 127, 100, 101, 113, 118, 112, 96, 121, 97, 98, 10…\n$ Phenols              &lt;dbl&gt; 2.80, 2.65, 2.80, 3.85, 2.80, 3.27, 2.50, 2.60, 2…\n$ Flavanoids           &lt;dbl&gt; 3.06, 2.76, 3.24, 3.49, 2.69, 3.39, 2.52, 2.51, 2…\n$ Nonflavanoid.phenols &lt;dbl&gt; 0.28, 0.26, 0.30, 0.24, 0.39, 0.34, 0.30, 0.31, 0…\n$ Proanth              &lt;dbl&gt; 2.29, 1.28, 2.81, 2.18, 1.82, 1.97, 1.98, 1.25, 1…\n$ Color                &lt;dbl&gt; 5.64, 4.38, 5.68, 7.80, 4.32, 6.75, 5.25, 5.05, 5…\n$ Hue                  &lt;dbl&gt; 1.04, 1.05, 1.03, 0.86, 1.04, 1.05, 1.02, 1.06, 1…\n$ OD                   &lt;dbl&gt; 3.92, 3.40, 3.17, 3.45, 2.93, 2.85, 3.58, 3.58, 2…\n$ Proline              &lt;dbl&gt; 1065, 1050, 1185, 1480, 735, 1450, 1290, 1295, 10…\n\n\nChange the code below and make different plots with other covariate combinations. What sort of patterns begin to emerge?\n\nggplot(wine, aes(x = Ash, y = Malic, colour = Type)) +\n  geom_point() +\n  theme_bw() +\n  labs(colour = \"Wine Type\")\n\n\n\n\n\n\n\n\nWe will work only with types 1 and 2\n\nwine2 &lt;- wine %&gt;%\n  filter(Type != \"type3\") %&gt;%\n  mutate(Type_binary = as.numeric(Type == \"type1\")) %&gt;% \n  as_tibble\nwine2\n\n# A tibble: 130 × 15\n   Type  Alcohol Malic   Ash Alcalinity Magnesium Phenols Flavanoids\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1 type1    14.2  1.71  2.43       15.6       127    2.8        3.06\n 2 type1    13.2  1.78  2.14       11.2       100    2.65       2.76\n 3 type1    13.2  2.36  2.67       18.6       101    2.8        3.24\n 4 type1    14.4  1.95  2.5        16.8       113    3.85       3.49\n 5 type1    13.2  2.59  2.87       21         118    2.8        2.69\n 6 type1    14.2  1.76  2.45       15.2       112    3.27       3.39\n 7 type1    14.4  1.87  2.45       14.6        96    2.5        2.52\n 8 type1    14.1  2.15  2.61       17.6       121    2.6        2.51\n 9 type1    14.8  1.64  2.17       14          97    2.8        2.98\n10 type1    13.9  1.35  2.27       16          98    2.98       3.15\n# ℹ 120 more rows\n# ℹ 7 more variables: Nonflavanoid.phenols &lt;dbl&gt;, Proanth &lt;dbl&gt;, Color &lt;dbl&gt;,\n#   Hue &lt;dbl&gt;, OD &lt;dbl&gt;, Proline &lt;dbl&gt;, Type_binary &lt;dbl&gt;\n\n\nChange the code below and fit a logistic regression model using the four predictors you believe are the best to classify the two types of wine. Also play around with different thresholds for the classification rule. Compare with your peers. What predictors yielded the best predictive performance?\n\n# include the predictors below\nlogistic_reg &lt;- glm(Type_binary ~ Malic + Ash + Alcalinity + Magnesium,\n                           family = binomial, data = wine2)\n\n# set the threshold for the classification rule below\nthreshold &lt;- 0.50\n\n# computes the percentage of correct predictions\nwine_predict &lt;- wine2 %&gt;% \n                  mutate(p_hat = predict(logistic_reg, type = \"response\") %&gt;% round(2),\n                         Type_pred = ifelse(p_hat &gt;= threshold, \"type1\",\"type2\")) \n\n\nn_correct &lt;- sum(wine_predict$Type_pred == wine_predict$Type)\nn_total &lt;- nrow(wine2)\npercentage_correct &lt;- n_correct/n_total * 100\npercentage_correct\n\n[1] 86.92308"
  },
  {
    "objectID": "3-MultipleRegression.html#modeling-cars",
    "href": "3-MultipleRegression.html#modeling-cars",
    "title": "Multiple Regression",
    "section": "Modeling cars",
    "text": "Modeling cars\n\n\n\n\n\n\n\n\n\n\nDescribe: What is the relationship between cars’ weights and their mileage?\n\n\n\n\n\n\n\n\n\n\n\nPredict: What is your best guess for a car’s MPG that weighs 3,500 pounds?\n\n\n\nWarning in geom_segment(aes(x = 3.5, xend = 3.5, y = -Inf, yend = 18.5), : All aesthetics have length 1, but the data has 32 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = -Inf, xend = 3.5, y = 18.5, yend = 18.5), : All aesthetics have length 1, but the data has 32 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row."
  },
  {
    "objectID": "3-MultipleRegression.html#modelling-cars",
    "href": "3-MultipleRegression.html#modelling-cars",
    "title": "Multiple Regression",
    "section": "Modelling cars",
    "text": "Modelling cars\n\nDescribe: What is the relationship between cars’ weights and their mileage?"
  },
  {
    "objectID": "3-MultipleRegression.html#modelling-cars-1",
    "href": "3-MultipleRegression.html#modelling-cars-1",
    "title": "Multiple Regression",
    "section": "Modelling cars",
    "text": "Modelling cars\n\nPredict: What is your best guess for a car’s MPG that weighs 3,500 pounds?\n\n\n\nWarning in geom_segment(aes(x = 3.5, xend = 3.5, y = -Inf, yend = 18.5), : All aesthetics have length 1, but the data has 32 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = -Inf, xend = 3.5, y = 18.5, yend = 18.5), : All aesthetics have length 1, but the data has 32 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row."
  },
  {
    "objectID": "3-MultipleRegression.html#modelling",
    "href": "3-MultipleRegression.html#modelling",
    "title": "Multiple Regression",
    "section": "Modelling",
    "text": "Modelling\n\nUse models to explain the relationship between variables and to make predictions\nFor now we will focus on linear models (but there are many many other types of models too!)"
  },
  {
    "objectID": "3-MultipleRegression.html#modelling-vocabulary",
    "href": "3-MultipleRegression.html#modelling-vocabulary",
    "title": "Multiple Regression",
    "section": "Modelling vocabulary",
    "text": "Modelling vocabulary\n\nPredictor (explanatory variable)\nOutcome (response variable)\nRegression line\n\nSlope\nIntercept\n\n\n\nPredictor (explanatory variable)\n\n\n\n\n\n\n\n\n\n\nmpg\nwt\n\n\n\n\n21\n2.62\n\n\n21\n2.875\n\n\n22.8\n2.32\n\n\n21.4\n3.215\n\n\n18.7\n3.44\n\n\n18.1\n3.46\n\n\n...\n...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutcome (response variable)\n\n\n\n\n\n\n\n\n\n\nmpg\nwt\n\n\n\n\n21\n2.62\n\n\n21\n2.875\n\n\n22.8\n2.32\n\n\n21.4\n3.215\n\n\n18.7\n3.44\n\n\n18.1\n3.46\n\n\n...\n...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression line\n\n\n\n\n\n\n\n\n\n\n\nRegression line: slope\n\n\n\n\n\n\n\n\n\n\n\nRegression line: intercept"
  },
  {
    "objectID": "3-MultipleRegression.html#predictor-explanatory-variable",
    "href": "3-MultipleRegression.html#predictor-explanatory-variable",
    "title": "Multiple Regression",
    "section": "Predictor (explanatory variable)",
    "text": "Predictor (explanatory variable)\n\n\n\n\n\n\n\n\n\n\nmpg\nwt\n\n\n\n\n21\n2.62\n\n\n21\n2.875\n\n\n22.8\n2.32\n\n\n21.4\n3.215\n\n\n18.7\n3.44\n\n\n18.1\n3.46\n\n\n...\n..."
  },
  {
    "objectID": "3-MultipleRegression.html#outcome-response-variable",
    "href": "3-MultipleRegression.html#outcome-response-variable",
    "title": "Multiple Regression",
    "section": "Outcome (response variable)",
    "text": "Outcome (response variable)\n\n\n\n\n\n\n\n\n\n\nmpg\nwt\n\n\n\n\n21\n2.62\n\n\n21\n2.875\n\n\n22.8\n2.32\n\n\n21.4\n3.215\n\n\n18.7\n3.44\n\n\n18.1\n3.46\n\n\n...\n..."
  },
  {
    "objectID": "3-MultipleRegression.html#correlation-1",
    "href": "3-MultipleRegression.html#correlation-1",
    "title": "Multiple Regression",
    "section": "Correlation",
    "text": "Correlation\n\nRanges between -1 and 1.\nSame sign as the slope.\n\nWe have seen last semester, in DS151, how to study the relationship between two variables using linear regression. See, e.g. the mammals dataset:\n\nlibrary(tidyverse)\nmammals &lt;- read_csv(\"https://www.dropbox.com/s/rb3zd0nk4430tbv/mammals.csv?raw=1\")\nmammals\n\n# A tibble: 62 × 3\n   name               body brain\n   &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt;\n 1 Arctic fox        3.38   44.5\n 2 Owl monkey        0.48   15.5\n 3 Mountain beaver   1.35    8.1\n 4 Cow             465     423  \n 5 Grey wolf        36.3   120. \n 6 Goat             27.7   115  \n 7 Roe deer         14.8    98.2\n 8 Guinea pig        1.04    5.5\n 9 Verbet            4.19   58  \n10 Chinchilla        0.425   6.4\n# ℹ 52 more rows\n\n\n\n\nLet’s have a look at an exploratory plot, using brain weight as the response variable and body weight as the predictor variable:\n\nggplot(data = mammals,\n       mapping = aes(y = brain, x = body)) +\n  theme_bw() +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nLet’s label the points:\n\nggplot(data = mammals,\n       mapping = aes(y = brain, x = body, label = name)) +\n  theme_bw() +\n  geom_point() +\n  geom_label()\n\n\n\n\n\n\n\n\nIt is difficult to see the clump of points at the bottom left part of the plot. Maybe it is better to take the logarithm of the two variables to visualise them –\n\nggplot(data = mammals,\n       mapping = aes(y = log(brain), x = log(body), label = name)) +\n  theme_bw() +\n  geom_label()\n\n\n\n\n\n\n\n\nWe can overlay a linear regression line to the plot:\n\nggplot(data = mammals,\n       mapping = aes(y = log(brain), x = log(body))) +\n  theme_bw() +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe equation that predicts brain weight as a function of body weight is\n\nlm(log(brain) ~ log(body), data = mammals) %&gt;%\n  coef\n\n(Intercept)   log(body) \n  2.1347887   0.7516859 \n\n\n\\[\\log(\\mbox{brain weight}) = 2.13+0.75\\times\\log(\\mbox{body weight})\\]\nHowever, it is rare to find a dataset with only two variables. In many research and applied fields, many different variables are measured at the same time, for various reasons. In many cases, we may actually have too many variables and may end up selecting a subset of them (only the most important) to proceed with the analysis. (NB: there are many ways to define variable importance, and we will look at some of them in depth throughout the Data Science programme)."
  },
  {
    "objectID": "3-MultipleRegression.html#regression-line",
    "href": "3-MultipleRegression.html#regression-line",
    "title": "Multiple Regression",
    "section": "Regression line",
    "text": "Regression line"
  },
  {
    "objectID": "3-MultipleRegression.html#regression-line-slope",
    "href": "3-MultipleRegression.html#regression-line-slope",
    "title": "Multiple Regression",
    "section": "Regression line: slope",
    "text": "Regression line: slope"
  },
  {
    "objectID": "3-MultipleRegression.html#regression-line-intercept",
    "href": "3-MultipleRegression.html#regression-line-intercept",
    "title": "Multiple Regression",
    "section": "Regression line: intercept",
    "text": "Regression line: intercept"
  },
  {
    "objectID": "3-MultipleRegression.html#correlation",
    "href": "3-MultipleRegression.html#correlation",
    "title": "Multiple Regression",
    "section": "Correlation",
    "text": "Correlation"
  },
  {
    "objectID": "3-MultipleRegression.html#example-modeling-cars",
    "href": "3-MultipleRegression.html#example-modeling-cars",
    "title": "Multiple Regression",
    "section": "Example: Modeling cars",
    "text": "Example: Modeling cars\n\n\n\n\n\n\n\n\n\n\nDescribe: What is the relationship between cars’ weights and their mileage?\n\n\n\n\n\n\n\n\n\n\n\nPredict: What is your best guess for a car’s MPG that weighs 3,500 pounds?\n\n\n\n\n\n\n\n\n\n\n\nModelling vocabulary\n\nPredictor (explanatory variable)\nOutcome (response variable)\nRegression line\n\nSlope\nIntercept\n\n\n\nPredictor (explanatory variable)\n\n\n\n\n\n\n\n\n\n\nmpg\nwt\n\n\n\n\n21\n2.62\n\n\n21\n2.875\n\n\n22.8\n2.32\n\n\n21.4\n3.215\n\n\n18.7\n3.44\n\n\n18.1\n3.46\n\n\n...\n...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutcome (response variable)\n\n\n\n\n\n\n\n\n\n\nmpg\nwt\n\n\n\n\n21\n2.62\n\n\n21\n2.875\n\n\n22.8\n2.32\n\n\n21.4\n3.215\n\n\n18.7\n3.44\n\n\n18.1\n3.46\n\n\n...\n...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression line\n\n\n\n\n\n\n\n\n\n\n\nRegression line: slope\n\n\n\n\n\n\n\n\n\n\n\nRegression line: intercept\n\n\n\n\n\n\n\n\n\n\n\n\nR Code: Modeling cars\n\nmtcars_mod &lt;- lm(mpg ~ wt, data = mtcars)\nmtcars_mod\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nCoefficients:\n(Intercept)           wt  \n     37.285       -5.344  \n\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(\n    x = \"Weight (1,000 lbs)\",\n    y = \"Miles per gallon (MPG)\",\n    title = \"MPG vs. weights of cars\"\n  )"
  },
  {
    "objectID": "3-MultipleRegression.html#r-code",
    "href": "3-MultipleRegression.html#r-code",
    "title": "Multiple Regression",
    "section": "R Code",
    "text": "R Code\n\nmtcars_mod &lt;- lm(mpg ~ wt, data = mtcars)\nmtcars_mod\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nCoefficients:\n(Intercept)           wt  \n     37.285       -5.344  \n\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(\n    x = \"Weight (1,000 lbs)\",\n    y = \"Miles per gallon (MPG)\",\n    title = \"MPG vs. weights of cars\"\n  ) \n\n\n\n\n\n\n\n\nHowever, it is rare to find a dataset with only two variables. In many research and applied fields, many different variables are measured at the same time, for various reasons. In many cases, we may actually have too many variables and may end up selecting a subset of them (only the most important) to proceed with the analysis. (NB: there are many ways to define variable importance, and we will look at some of them in depth throughout the Data Science programme)."
  },
  {
    "objectID": "3-MultipleRegression.html#multiple-regression",
    "href": "3-MultipleRegression.html#multiple-regression",
    "title": "Multiple Regression",
    "section": "Multiple Regression",
    "text": "Multiple Regression\nMultiple linear regression is used to model the relationship between a continuous outcome variable and two or more predictor variables. It extends simple linear regression by incorporating multiple predictors to explain variations in the outcome variable. The model estimates coefficients for each predictor, allowing researchers to assess their individual contributions while controlling for others, making it useful for predicting outcomes and identifying key influencing factors."
  },
  {
    "objectID": "3-MultipleRegression.html#example-1-the-crab-dataset",
    "href": "3-MultipleRegression.html#example-1-the-crab-dataset",
    "title": "Multiple Regression",
    "section": "Example 1: The Crab Dataset",
    "text": "Example 1: The Crab Dataset\nLet’s have a look at an example where we have many predictor variables. The crabs dataset from package MASS has 200 observations on 2 qualitative variables (species colour and sex), and 5 morphological measurements (frontal lobe size, rear width, carapace length and width, and body depth) of crabs of the species Leptograspus variegatus.\n\n\n\nExploratory Analysis\n\ncrab_dat &lt;- MASS::crabs\nglimpse(crab_dat)\n\nRows: 200\nColumns: 8\n$ sp    &lt;fct&gt; B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B…\n$ sex   &lt;fct&gt; M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M…\n$ index &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 1…\n$ FL    &lt;dbl&gt; 8.1, 8.8, 9.2, 9.6, 9.8, 10.8, 11.1, 11.6, 11.8, 11.8, 12.2, 12.…\n$ RW    &lt;dbl&gt; 6.7, 7.7, 7.8, 7.9, 8.0, 9.0, 9.9, 9.1, 9.6, 10.5, 10.8, 11.0, 1…\n$ CL    &lt;dbl&gt; 16.1, 18.1, 19.0, 20.1, 20.3, 23.0, 23.8, 24.5, 24.2, 25.2, 27.3…\n$ CW    &lt;dbl&gt; 19.0, 20.8, 22.4, 23.1, 23.0, 26.5, 27.1, 28.4, 27.8, 29.3, 31.6…\n$ BD    &lt;dbl&gt; 7.0, 7.4, 7.7, 8.2, 8.2, 9.8, 9.8, 10.4, 9.7, 10.3, 10.9, 11.4, …\n\nlibrary(GGally)\nlibrary(ggplot2)\nggpairs(crab_dat, columns = 4:8, aes(colour=sex)) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nDescribe: What are the relationships between the variables?\n\n\n\nExploratory Modelling\nImagine we are interested in predicting the carapace length (CL) of this species of crab. We can create a linear regression model that includes multiple predictors, such as \\[\\hat{\\mbox{CL}}=\\beta_0+\\beta_1\\mbox{FL}+\\beta_2\\mbox{RW}+\\beta_3\\mbox{CW}+\\beta_4\\mbox{BD}\\]\nThis model can be fit in R by executing:\n\ncrab_mod1 &lt;- lm(CL ~ FL + RW + CW + BD, data = crab_dat)\ncrab_mod1\n\n\nCall:\nlm(formula = CL ~ FL + RW + CW + BD, data = crab_dat)\n\nCoefficients:\n(Intercept)           FL           RW           CW           BD  \n     0.3163       0.2649      -0.1779       0.6402       0.4714  \n\n\nWe have that \\[\\hat{\\mbox{CL}}=0.32+0.26\\mbox{FL}-0.18\\mbox{RW}+0.64\\mbox{CW}+0.47\\mbox{BD}\\].\nInterpretation:\n\nAs FL increases by one unit, holding the other predictors constant, then carapace length will increase by 0.26.\nAs RW increases by one unit, holding the other predictors constant, then carapace length will decrease by 0.18.\nAs CW increases by one unit, holding the other predictors constant, then carapace length will increase by 0.64.\nAs BD increases by one unit, holding the other predictors constant, then carapace length will increase by 0.47.\n\n\nQuestion: How well does the model do at predicting carapace length?\n\n\ncrab_dat &lt;- crab_dat %&gt;% \n              mutate(CL_pred = predict(crab_mod1))\n\nggplot(data = crab_dat,\n       mapping = aes(x = CL, y = CL_pred, colour = sex)) +\n  theme_bw() +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  xlab(\"Observed CL\") + \n  ylab(\"Predicted CL\")\n\n\n\n\n\n\n\n\n\nQuestion: How well would we have done if we only used one predictor variable, say RW?\n\n\ncrab_mod2 &lt;- lm(CL ~ RW, data = crab_dat)\ncrab_mod2\n\n\nCall:\nlm(formula = CL ~ RW, data = crab_dat)\n\nCoefficients:\n(Intercept)           RW  \n      0.645        2.470  \n\ncrab_dat &lt;- crab_dat %&gt;% \n              mutate(CL_pred2 = predict(crab_mod2))\n\nggplot(data = crab_dat,\n       mapping = aes(x = CL, y = CL_pred2, colour = sex)) +\n  theme_bw() +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  xlab(\"Observed CL\") + \n  ylab(\"Predicted CL\")\n\n\n\n\n\n\n\n\n\nQuestion: What about including the categorical variables, namely sex or sp (species colour)?\n\nWe can include categorical variables in our model, such that\n\\[\\hat{\\mbox{CL}}=\\beta_0+\\beta_1\\mbox{RW}+\\beta_2I(\\mbox{sex}=\\mbox{M})\\] (NB: \\(I(\\mbox{sex}=\\mbox{M})\\) is an indicator function, which is equal to 1 if sex is equal to M and zero otherwise. The same type of interpretation can be drawn from \\(I(\\mbox{sp}=\\mbox{O})\\))\n\ncrab_mod3 &lt;- lm(CL ~ RW + sex, data = crab_dat)\ncrab_mod3\n\n\nCall:\nlm(formula = CL ~ RW + sex, data = crab_dat)\n\nCoefficients:\n(Intercept)           RW         sexM  \n     -6.293        2.792        5.670  \n\ncrab_dat &lt;- crab_dat %&gt;% \n              mutate(CL_pred3 = predict(crab_mod3))\n\nggplot(data = crab_dat,\n       mapping = aes(x = CL, y = CL_pred3, colour = sex)) +\n  theme_bw() +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  xlab(\"Observed CL\") + \n  ylab(\"Predicted CL\")\n\n\n\n\n\n\n\n\n\nQuestion: How do we interpret the coefficients related to sex?\n\nFor a male crab the expected carapace length increases by 5.67 compared to a female crab.\n\n\nModel Comparison\nOne way of comparing how well our modelling strategies did in terms of predictions is to sum the squared discrepancies (residuals) between predicted and observed values, and see which one is smaller.\n\ndiscrepancy_mod1 &lt;- sum(residuals(crab_mod1)^2)\ndiscrepancy_mod2 &lt;- sum(residuals(crab_mod2)^2)\ndiscrepancy_mod3 &lt;- sum(residuals(crab_mod3)^2)\n\n\ndiscrepancy_mod1\n\n[1] 27.35058\n\ndiscrepancy_mod2\n\n[1] 2047.417\n\ndiscrepancy_mod3\n\n[1] 576.4921\n\n\nTypically we would split the data into training and test set, use only the training set to fit the model, and then perform this computation on both sets. We will see more details on how to compare the models in terms of their predictive power in the module Statistical Machine Learning. We will also see more details on this in the modules Linear Models I and II, how to properly test hypotheses, how to assess goodness-of-fit, what the important assumptions are and how to properly check them."
  },
  {
    "objectID": "3-MultipleRegression.html#logistic-regression",
    "href": "3-MultipleRegression.html#logistic-regression",
    "title": "Multiple Regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nLogistic regression is a generalized regression model where the outcome is a two-level categorical variable. The outcome, takes the value 1 or 0 with probability. Ultimately, it is the probability of the outcome taking the value 1 (i.e., being a “success”) that we model in relation to the predictor variables. For this to work, we transform the expected outcome in such a way that it will be bounded between 0 and 1, and hence our estimates will be sensical. The transformation we use is called the logit, and is the natural logarithm of the odds of success.\n\nModelling binary outcomes\n\n\\(y\\) takes on values 0 (failure) or 1 (success)\n\\(p\\): probability of success\n\\(1-p\\): probability of failure\nWe can’t model \\(y\\) directly, so instead we model \\(p\\)\n\nLinear model\n\\[\n\\hat{p}_i = \\beta_o + \\beta_1 \\times x\n\\]\n\nBut remember that \\(p\\) must be between 0 and 1\nWe need a link function that transforms the linear model to have an appropriate range\n\nLogit link function\nThe logit function takes values between 0 and 1 (probabilities) and maps them to values in the range negative infinity to positive infinity:\n\\[\nlogit(p) = log \\bigg( \\frac{p}{1 - p} \\bigg)\n\\]\nGeneralized linear model\n\nWe model the logit (log-odds) of \\(p\\) :\n\n\\[\nlogit(\\hat{p}) = log \\bigg( \\frac{\\hat{p}}{1 - \\hat{p}} \\bigg) = \\beta_o + \\beta_1 \\times x\n\\]\n\nThen take the inverse to obtain the predicted \\(p\\):\n\n\\[\n\\hat{p} = \\frac{e^{\\beta_o + \\beta_1 \\times x }}{1 + e^{\\beta_o + \\beta_1 \\times x }}\n\\]\nA logistic model visualized"
  },
  {
    "objectID": "3-MultipleRegression.html#example-2-the-iris-dataset",
    "href": "3-MultipleRegression.html#example-2-the-iris-dataset",
    "title": "Multiple Regression",
    "section": "Example 2: The Iris Dataset",
    "text": "Example 2: The Iris Dataset\nYou will have explored the Iris dataset initially last semester in DS151.\nIris is a genus of about 300 species of flowering plants, taking its name from the Greek word for a rainbow (which is also the name for the Greek goddess of rainbows, Iris). The flowers are very showy, and we would like to know if it is possible to identify some of the species based on measurements of different parts of the flowers.\nThe dataset gives the measurements in centimeters of the variables:\n\nsepal length\nsepal width\npetal length\npetal width\n\nfor 50 flowers from each of three species of Iris: Iris setosa, Iris versicolor, and Iris virginica.\nThis dataset is available in base R as iris. Let’s have a look:\n\nlibrary(tidyverse)\nglimpse(iris)\n\nRows: 150\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n\nLet’s work, initially, only with species versicolor and virginica. We create a plot that shows in the \\(x\\) axis the petal length, and in the \\(y\\) axis only the values of 0 and 1, representing whether the observation belongs to class virginica or not (0 = the observation belongs to class versicolor; 1 = it belongs to class virginica). We call this a binary (or dummy) variable.\n\niris2 &lt;- iris %&gt;%\n  filter(Species != \"setosa\") %&gt;%\n  mutate(Species.binary = as.numeric(Species == \"virginica\"))\n  \nggplot(iris2, aes(x = Petal.Length, y = Species.binary)) +\n  geom_point() +\n  theme_bw()\n\n\n\n\n\n\n\n\nWe may interpret the numbers 0 and 1 here as the probability \\(p\\) of belonging to class virginica. It appears that as the petal length increases, so does the likelihood of belonging to the virginica class.\n\nggplot(iris2, aes(x = Petal.Length, y = Species.binary)) +\n  geom_point() +\n  geom_smooth(method = glm, method.args = list(family = \"binomial\"), se = FALSE) +\n  theme_bw()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nIncluding more predictors\nIt is frequently of interest to use multiple covariates to improve our predictive power. Let’s fit a logistic regression model including all four covariates as predictors in our model: \\[\\displaystyle\\log\\left(\\frac{p}{1-p}\\right)=\\beta_0+\\beta_1*\\mbox{Sepal.Length}+\\beta_2*\\mbox{Sepal.Width}+\\beta_3*\\mbox{Petal.Length}+\\beta_4*\\mbox{Petal.Width}\\]\n\nfull_logistic_reg &lt;- glm(Species.binary ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width,\n                         family = binomial, data = iris2)\nfull_logistic_reg %&gt;% coef %&gt;% round(digits = 4)\n\n (Intercept) Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n    -42.6378      -2.4652      -6.6809       9.4294      18.2861 \n\n\nHow well did we do?\n\niris_predict &lt;- iris2 %&gt;% \n                  mutate(p_hat = predict(full_logistic_reg, type = \"response\") %&gt;% round(2),\n                         Species_pred = ifelse(p_hat &gt;= 0.5,\"virginica\",\"versicolor\"))\n\nn_correct_full &lt;- sum(iris_predict$Species_pred == iris_predict$Species)\nn_correct_full\n\n[1] 98\n\n\nOur model now correctly predicts the class of 98 out of 100 observations."
  },
  {
    "objectID": "3-MultipleRegression.html#example-the-iris-dataset",
    "href": "3-MultipleRegression.html#example-the-iris-dataset",
    "title": "Multiple Regression",
    "section": "Example: The Iris Dataset",
    "text": "Example: The Iris Dataset\nYou will have explored the Iris dataset initially last semester in DS151.\nIris is a genus of about 300 species of flowering plants, taking its name from the Greek word for a rainbow (which is also the name for the Greek goddess of rainbows, Iris). The flowers are very showy, and we would like to know if it is possible to identify some of the species based on measurements of different parts of the flowers.\nThe dataset gives the measurements in centimeters of the variables:\n\nsepal length\nsepal width\npetal length\npetal width\n\nfor 50 flowers from each of three species of Iris: Iris setosa, Iris versicolor, and Iris virginica.\nThis dataset is available in base R as iris. Let’s have a look:\n\nlibrary(tidyverse)\nglimpse(iris)\n\nRows: 150\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n\n\nExploratory Analysis\nLet’s work, initially, only with species versicolor and virginica. We create a plot that shows in the \\(x\\) axis the petal length, and in the \\(y\\) axis only the values of 0 and 1, representing whether the observation belongs to class virginica or not (0 = the observation belongs to class versicolor; 1 = it belongs to class virginica). We call this a binary (or dummy) variable.\n\niris2 &lt;- iris %&gt;%\n  filter(Species != \"setosa\") %&gt;%\n  mutate(Species.binary = as.numeric(Species == \"virginica\"))\n  \nggplot(iris2, aes(x = Petal.Length, y = Species.binary)) +\n  geom_point() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\nExploratory Modelling\nA simple logistic regression model can be fit, such that \\[\\displaystyle\\log\\left(\\frac{p}{1-p}\\right)=\\beta_0+\\beta_1*\\mbox{Petal.Length}\\]\nDo this in R by executing:\n\niris_mod1 &lt;- glm(Species.binary ~ Petal.Length,\n                 family = binomial,\n                 data = iris2)\niris_mod1 %&gt;% coef() %&gt;% round(digits = 4)\n\n (Intercept) Petal.Length \n    -43.7809       9.0020 \n\n\n\nggplot(iris2, aes(x = Petal.Length, y = Species.binary)) +\n  geom_point() +\n  geom_smooth(method = glm, method.args = list(family = \"binomial\"), se = FALSE) +\n  ylab(\"p\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nWe may interpret the y-axis numbers here as the probability \\(p\\) of belonging to class virginica. It appears that as the petal length increases, so does the likelihood of belonging to the virginica class."
  },
  {
    "objectID": "3-MultipleRegression.html#exploratory-analysis-1",
    "href": "3-MultipleRegression.html#exploratory-analysis-1",
    "title": "Multiple Regression",
    "section": "Exploratory Analysis",
    "text": "Exploratory Analysis\nLet’s work, initially, only with species versicolor and virginica. We create a plot that shows in the \\(x\\) axis the petal length, and in the \\(y\\) axis only the values of 0 and 1, representing whether the observation belongs to class virginica or not (0 = the observation belongs to class versicolor; 1 = it belongs to class virginica). We call this a binary (or dummy) variable.\n\niris2 &lt;- iris %&gt;%\n  filter(Species != \"setosa\") %&gt;%\n  mutate(Species.binary = as.numeric(Species == \"virginica\"))\n  \nggplot(iris2, aes(x = Petal.Length, y = Species.binary)) +\n  geom_point() +\n  theme_bw()"
  },
  {
    "objectID": "3-MultipleRegression.html#exploratory-modelling-1",
    "href": "3-MultipleRegression.html#exploratory-modelling-1",
    "title": "Multiple Regression",
    "section": "Exploratory Modelling",
    "text": "Exploratory Modelling\nA simple logistic regression model can be fit, such that \\[\\displaystyle\\log\\left(\\frac{p}{1-p}\\right)=\\beta_0+\\beta_1*\\mbox{Petal.Length}\\]\nDo this in R by executing:\n\niris_mod1 &lt;- glm(Species.binary ~ Petal.Length,\n                 family = binomial,\n                 data = iris2)\niris_mod1 %&gt;% coef() %&gt;% round(digits = 4)\n\n (Intercept) Petal.Length \n    -43.7809       9.0020 \n\n\n\nggplot(iris2, aes(x = Petal.Length, y = Species.binary)) +\n  geom_point() +\n  geom_smooth(method = glm, method.args = list(family = \"binomial\"), se = FALSE) +\n  ylab(\"p\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nWe may interpret the y-axis numbers here as the probability \\(p\\) of belonging to class virginica. It appears that as the petal length increases, so does the likelihood of belonging to the virginica class.\n\nIncluding more predictors\nIt is frequently of interest to use multiple covariates to improve our predictive power. Let’s fit a logistic regression model including all four covariates as predictors in our model: \\[\\displaystyle\\log\\left(\\frac{p}{1-p}\\right)=\\beta_0+\\beta_1*\\mbox{Sepal.Length}+\\beta_2*\\mbox{Sepal.Width}+\\beta_3*\\mbox{Petal.Length}+\\beta_4*\\mbox{Petal.Width}\\]\n\niris_mod2 &lt;- glm(Species.binary ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width,\n                 family = binomial, \n                 data = iris2)\niris_mod2 %&gt;% coef %&gt;% round(digits = 4)\n\n (Intercept) Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n    -42.6378      -2.4652      -6.6809       9.4294      18.2861 \n\n\n\n\nClassification\n\nQuestion: How well did we do at classifying the Iris species?\n\n\niris_predict &lt;- iris2 %&gt;% \n                  mutate(p_hat = predict(iris_mod2, type = \"response\") %&gt;% round(2),\n                         Species_pred = ifelse(p_hat &gt;= 0.5,\"virginica\",\"versicolor\"))\n\nn_correct_full &lt;- sum(iris_predict$Species_pred == iris_predict$Species)\nn_correct_full\n\n[1] 98\n\n\nOur model correctly predicts the species class of 98 out of 100 observations."
  },
  {
    "objectID": "3-MultipleRegression.html#example-the-wine-dataset",
    "href": "3-MultipleRegression.html#example-the-wine-dataset",
    "title": "Multiple Regression",
    "section": "Example: The Wine Dataset",
    "text": "Example: The Wine Dataset\nThe wine dataset contains the results of a chemical analysis of wines grown in a specific area of Italy. Three types of wine are represented in the 178 samples, with the results of 13 chemical analyses recorded for each sample. The Type variable has been transformed into a categoric variable.\nThe data contains no missing values and consits of only numeric data, with a three class target variable (Type) for classification.\nLet’s have a glimpse at the dataset:\n\nwine &lt;- read_csv(\"https://www.dropbox.com/s/l5x4ur06gfhpg0h/wine.csv?raw=1\") \n\nRows: 178 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): Type\ndbl (13): Alcohol, Malic, Ash, Alcalinity, Magnesium, Phenols, Flavanoids, N...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(wine)\n\nRows: 178\nColumns: 14\n$ Type                 &lt;chr&gt; \"type1\", \"type1\", \"type1\", \"type1\", \"type1\", \"typ…\n$ Alcohol              &lt;dbl&gt; 14.23, 13.20, 13.16, 14.37, 13.24, 14.20, 14.39, …\n$ Malic                &lt;dbl&gt; 1.71, 1.78, 2.36, 1.95, 2.59, 1.76, 1.87, 2.15, 1…\n$ Ash                  &lt;dbl&gt; 2.43, 2.14, 2.67, 2.50, 2.87, 2.45, 2.45, 2.61, 2…\n$ Alcalinity           &lt;dbl&gt; 15.6, 11.2, 18.6, 16.8, 21.0, 15.2, 14.6, 17.6, 1…\n$ Magnesium            &lt;dbl&gt; 127, 100, 101, 113, 118, 112, 96, 121, 97, 98, 10…\n$ Phenols              &lt;dbl&gt; 2.80, 2.65, 2.80, 3.85, 2.80, 3.27, 2.50, 2.60, 2…\n$ Flavanoids           &lt;dbl&gt; 3.06, 2.76, 3.24, 3.49, 2.69, 3.39, 2.52, 2.51, 2…\n$ Nonflavanoid.phenols &lt;dbl&gt; 0.28, 0.26, 0.30, 0.24, 0.39, 0.34, 0.30, 0.31, 0…\n$ Proanth              &lt;dbl&gt; 2.29, 1.28, 2.81, 2.18, 1.82, 1.97, 1.98, 1.25, 1…\n$ Color                &lt;dbl&gt; 5.64, 4.38, 5.68, 7.80, 4.32, 6.75, 5.25, 5.05, 5…\n$ Hue                  &lt;dbl&gt; 1.04, 1.05, 1.03, 0.86, 1.04, 1.05, 1.02, 1.06, 1…\n$ OD                   &lt;dbl&gt; 3.92, 3.40, 3.17, 3.45, 2.93, 2.85, 3.58, 3.58, 2…\n$ Proline              &lt;dbl&gt; 1065, 1050, 1185, 1480, 735, 1450, 1290, 1295, 10…\n\n\nChange the code below and make different plots with other covariate combinations. What sort of patterns begin to emerge?\n\nggplot(wine, aes(x = Ash, y = Malic, colour = Type)) +\n  geom_point() +\n  theme_bw() +\n  labs(colour = \"Wine Type\")\n\n\n\n\n\n\n\n\nWe will work only with types 1 and 2\n\nwine2 &lt;- wine %&gt;%\n  filter(Type != \"type3\") %&gt;%\n  mutate(Type_binary = as.numeric(Type == \"type1\")) %&gt;% \n  as_tibble\nwine2\n\n# A tibble: 130 × 15\n   Type  Alcohol Malic   Ash Alcalinity Magnesium Phenols Flavanoids\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1 type1    14.2  1.71  2.43       15.6       127    2.8        3.06\n 2 type1    13.2  1.78  2.14       11.2       100    2.65       2.76\n 3 type1    13.2  2.36  2.67       18.6       101    2.8        3.24\n 4 type1    14.4  1.95  2.5        16.8       113    3.85       3.49\n 5 type1    13.2  2.59  2.87       21         118    2.8        2.69\n 6 type1    14.2  1.76  2.45       15.2       112    3.27       3.39\n 7 type1    14.4  1.87  2.45       14.6        96    2.5        2.52\n 8 type1    14.1  2.15  2.61       17.6       121    2.6        2.51\n 9 type1    14.8  1.64  2.17       14          97    2.8        2.98\n10 type1    13.9  1.35  2.27       16          98    2.98       3.15\n# ℹ 120 more rows\n# ℹ 7 more variables: Nonflavanoid.phenols &lt;dbl&gt;, Proanth &lt;dbl&gt;, Color &lt;dbl&gt;,\n#   Hue &lt;dbl&gt;, OD &lt;dbl&gt;, Proline &lt;dbl&gt;, Type_binary &lt;dbl&gt;\n\n\nChange the code below and fit a logistic regression model using the four predictors you believe are the best to classify the two types of wine. Also play around with different thresholds for the classification rule. Compare with your peers. What predictors yielded the best predictive performance?\n\n# include the predictors below\nlogistic_reg &lt;- glm(Type_binary ~ Malic + Ash + Alcalinity + Magnesium,\n                           family = binomial, data = wine2)\n\n# set the threshold for the classification rule below\nthreshold &lt;- 0.50\n\n# computes the percentage of correct predictions\nwine_predict &lt;- wine2 %&gt;% \n                  mutate(p_hat = predict(logistic_reg, type = \"response\") %&gt;% round(2),\n                         Type_pred = ifelse(p_hat &gt;= threshold, \"type1\",\"type2\")) \n\n\nn_correct &lt;- sum(wine_predict$Type_pred == wine_predict$Type)\nn_total &lt;- nrow(wine2)\npercentage_correct &lt;- n_correct/n_total * 100\npercentage_correct\n\n[1] 86.92308"
  },
  {
    "objectID": "3-MultipleRegression.html#example-the-crab-dataset",
    "href": "3-MultipleRegression.html#example-the-crab-dataset",
    "title": "Multiple Regression",
    "section": "Example: The Crab Dataset",
    "text": "Example: The Crab Dataset\nLet’s have a look at an example where we have many predictor variables. The crabs dataset from package MASS has 200 observations on 2 qualitative variables (species colour and sex), and 5 morphological measurements (frontal lobe size, rear width, carapace length and width, and body depth) of crabs of the species Leptograspus variegatus.\n\n\n\nExploratory Analysis\n\ncrab_dat &lt;- MASS::crabs\nglimpse(crab_dat)\n\nRows: 200\nColumns: 8\n$ sp    &lt;fct&gt; B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B…\n$ sex   &lt;fct&gt; M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M…\n$ index &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 1…\n$ FL    &lt;dbl&gt; 8.1, 8.8, 9.2, 9.6, 9.8, 10.8, 11.1, 11.6, 11.8, 11.8, 12.2, 12.…\n$ RW    &lt;dbl&gt; 6.7, 7.7, 7.8, 7.9, 8.0, 9.0, 9.9, 9.1, 9.6, 10.5, 10.8, 11.0, 1…\n$ CL    &lt;dbl&gt; 16.1, 18.1, 19.0, 20.1, 20.3, 23.0, 23.8, 24.5, 24.2, 25.2, 27.3…\n$ CW    &lt;dbl&gt; 19.0, 20.8, 22.4, 23.1, 23.0, 26.5, 27.1, 28.4, 27.8, 29.3, 31.6…\n$ BD    &lt;dbl&gt; 7.0, 7.4, 7.7, 8.2, 8.2, 9.8, 9.8, 10.4, 9.7, 10.3, 10.9, 11.4, …\n\nlibrary(GGally)\nlibrary(ggplot2)\nggpairs(crab_dat, columns = 4:8, aes(colour=sex)) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nDescribe: What are the relationships between the variables?\n\n\n\nExploratory Modelling\nImagine we are interested in predicting the carapace length (CL) of this species of crab. We can create a linear regression model that includes multiple predictors, such as \\[\\hat{\\mbox{CL}}=\\beta_0+\\beta_1\\mbox{FL}+\\beta_2\\mbox{RW}+\\beta_3\\mbox{CW}+\\beta_4\\mbox{BD}\\]\nThis model can be fit in R by executing:\n\ncrab_mod1 &lt;- lm(CL ~ FL + RW + CW + BD, data = crab_dat)\ncrab_mod1\n\n\nCall:\nlm(formula = CL ~ FL + RW + CW + BD, data = crab_dat)\n\nCoefficients:\n(Intercept)           FL           RW           CW           BD  \n     0.3163       0.2649      -0.1779       0.6402       0.4714  \n\n\nWe have that \\[\\hat{\\mbox{CL}}=0.32+0.26\\mbox{FL}-0.18\\mbox{RW}+0.64\\mbox{CW}+0.47\\mbox{BD}\\].\nInterpretation:\n\nAs FL increases by one unit, holding the other predictors constant, then carapace length will increase by 0.26.\nAs RW increases by one unit, holding the other predictors constant, then carapace length will decrease by 0.18.\nAs CW increases by one unit, holding the other predictors constant, then carapace length will increase by 0.64.\nAs BD increases by one unit, holding the other predictors constant, then carapace length will increase by 0.47.\n\n\nQuestion: How well does the model do at predicting carapace length?\n\n\ncrab_dat &lt;- crab_dat %&gt;% \n              mutate(CL_pred = predict(crab_mod1))\n\nggplot(data = crab_dat,\n       mapping = aes(x = CL, y = CL_pred)) +\n  theme_bw() +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  xlab(\"Observed CL\") + \n  ylab(\"Predicted CL\")\n\n\n\n\n\n\n\n\n\nQuestion: How well would we have done if we only used one predictor variable, say RW?\n\n\ncrab_mod2 &lt;- lm(CL ~ RW, data = crab_dat)\ncrab_mod2\n\n\nCall:\nlm(formula = CL ~ RW, data = crab_dat)\n\nCoefficients:\n(Intercept)           RW  \n      0.645        2.470  \n\ncrab_dat &lt;- crab_dat %&gt;% \n              mutate(CL_pred2 = predict(crab_mod2))\n\n\nggplot(data = crab_dat,\n      aes(x = RW, y = CL)) +\n  theme_bw() +\n  geom_point() +\n  geom_line(aes(x = RW, y = CL_pred2)) \n\n\n\n\n\n\n\n\n\nQuestion: How well does the model do at predicting carapace length?\n\n\nggplot(data = crab_dat,\n       aes(x = CL, y = CL_pred2)) +\n  theme_bw() +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  xlab(\"Observed CL\") + \n  ylab(\"Predicted CL\")\n\n\n\n\n\n\n\n\n\nQuestion: What about including the categorical variables, namely sex or sp (species colour)?\n\nWe can include categorical variables in our model, such that\n\\[\\hat{\\mbox{CL}}=\\beta_0+\\beta_1\\mbox{RW}+\\beta_2I(\\mbox{sex}=\\mbox{M})\\] (NB: \\(I(\\mbox{sex}=\\mbox{M})\\) is an indicator function, which is equal to 1 if sex is equal to M and zero otherwise. The same type of interpretation can be drawn from \\(I(\\mbox{sp}=\\mbox{O})\\))\n\ncrab_mod3 &lt;- lm(CL ~ RW + sex, data = crab_dat)\ncrab_mod3\n\n\nCall:\nlm(formula = CL ~ RW + sex, data = crab_dat)\n\nCoefficients:\n(Intercept)           RW         sexM  \n     -6.293        2.792        5.670  \n\n\n\nQuestion: How do we interpret the coefficients related to sex?\n\nFor a male crab the expected carapace length increases by 5.67 compared to a female crab.\n\ncrab_dat &lt;- crab_dat %&gt;% \n              mutate(CL_pred3 = predict(crab_mod3))\n\n\nggplot(data = crab_dat,\n      aes(x = RW, y = CL, colour = sex)) +\n  theme_bw() +\n  geom_point() +\n  geom_line(aes(x = RW, y = CL_pred3, colour = sex)) \n\n\n\n\n\n\n\n\n\nQuestion: How well does the model do at predicting carapace length?\n\n\nggplot(data = crab_dat,\n       aes(x = CL, y = CL_pred3, colour = sex)) +\n  theme_bw() +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  xlab(\"Observed CL\") + \n  ylab(\"Predicted CL\")\n\n\n\n\n\n\n\n\n\n\nModel Comparison\nOne way of comparing how well our modelling strategies did in terms of predictions is to sum the squared discrepancies (residuals) between predicted and observed values, and see which one is smaller.\n\ndiscrepancy_mod1 &lt;- sum(residuals(crab_mod1)^2)\ndiscrepancy_mod2 &lt;- sum(residuals(crab_mod2)^2)\ndiscrepancy_mod3 &lt;- sum(residuals(crab_mod3)^2)\n\n\ndiscrepancy_mod1\n\n[1] 27.35058\n\ndiscrepancy_mod2\n\n[1] 2047.417\n\ndiscrepancy_mod3\n\n[1] 576.4921\n\n\nTypically we would split the data into training and test set, use only the training set to fit the model, and then perform this computation on both sets. We will see more details on how to compare the models in terms of their predictive power in the module Statistical Machine Learning. We will also see more details on this in the modules Linear Models I and II, how to properly test hypotheses, how to assess goodness-of-fit, what the important assumptions are and how to properly check them."
  },
  {
    "objectID": "ObservationalStudies.html",
    "href": "ObservationalStudies.html",
    "title": "Observational Studies",
    "section": "",
    "text": "Observational Study: A study which observes individuals and measures variables, but does not attempt to influence the responses.\n\nAn observational study on individuals from a random sample allows one to generalize conclusions about the sample to the population.\nAn observational study cannot show cause-and-effect relationships because there is the possibility that the response is affected by some variable(s) other than the ones being measured. That is, confounding variables may be present. “It ain’t what you don’t know that gets you into trouble. It’s what you know for sure that just ain’t so.” - Mark Twain\nIn prospective observational studies, investigators choose a sample and collect new data generated from that sample. That is, the investigators “look forward in time.”\nIn retrospective observational studies, investigators “look backwards in time” and use data that have already been collected. Retrospective studies are often criticized for having more confounding and bias compared to prospective studies."
  },
  {
    "objectID": "ObservationalStudies.html#scientific-studies",
    "href": "ObservationalStudies.html#scientific-studies",
    "title": "Observational Studies",
    "section": "",
    "text": "Observational\n\nCollect data in a way that does not directly interfere with how the data arise, i.e. merely “observe”;\nBased on an observational study, we can only establish an association, in other words correlation, between the explanatory and response variables;\nWith an observational study you are just observing the data and collecting it after or as it occurs.\n\nExperimental\n\nRandomly assign subjects to treatments\nEstablish causal connections"
  },
  {
    "objectID": "ObservationalStudies.html#studies-and-conclusions",
    "href": "ObservationalStudies.html#studies-and-conclusions",
    "title": "Observational Studies",
    "section": "Studies and Conclusions",
    "text": "Studies and Conclusions"
  },
  {
    "objectID": "ObservationalStudies.html#why-use-observational-studies",
    "href": "ObservationalStudies.html#why-use-observational-studies",
    "title": "Observational Studies",
    "section": "Why use observational studies?",
    "text": "Why use observational studies?\nReasons why we must sometimes use an observational study instead of an experiment …\n\nIt is unethical or impossible to assign people to receive a specific treatment.\n\nFor example, if you want to know the impact of smoking on cancer you cannot design a study and assign some people to be smokers and other to be non-smokers. That is not ethical.\n\nCertain exposure variables are inherent traits and cannot be randomly assigned."
  },
  {
    "objectID": "ObservationalStudies.html#observational-studies",
    "href": "ObservationalStudies.html#observational-studies",
    "title": "Observational Studies",
    "section": "Observational studies",
    "text": "Observational studies\n\nObservational studies are very useful when it is not possible to design a study\nObervational studies often have large sample sizes\nThey are cheaper than designing a study (for example, clinical trials in medicine are very expensive)\nYou have to get to know the data a lot better with observational studies. You haven’t set up to study or controlled for influences other than the thing you are studying."
  },
  {
    "objectID": "ObservationalStudies.html#retrospective-vs-prospective-studies",
    "href": "ObservationalStudies.html#retrospective-vs-prospective-studies",
    "title": "Observational Studies",
    "section": "Retrospective Vs Prospective Studies",
    "text": "Retrospective Vs Prospective Studies\nIf an observational study uses data from the past, it is called retrospective study, whereas if data are collected throughout the study, it is called prospective;"
  },
  {
    "objectID": "ObservationalStudies.html#example-does-working-out-increase-energy-levels",
    "href": "ObservationalStudies.html#example-does-working-out-increase-energy-levels",
    "title": "Observational Studies",
    "section": "Example: Does working out increase energy levels?",
    "text": "Example: Does working out increase energy levels?\nWe want to evaluate if regularly working out has any impact on energy levels.\n\nIn an observational study, we sample two types of people from the population, those who choose to work out regularly and those who don’t.\nWe ask the people in each group to rate their energy levels from 1-10.\nThen, we find the average “energy level” for the two groups of people and compare.\n\n\nCan we conclude from this that working out is the cause of increased energy levels?\n\nThere may be other variables that we didn’t control for in this study that contribute to the observed difference.\nFor example, people who have young children might have less time to work out and also have lower energy levels.\nThis is known as confounding.\nThis study allows us to make correlation statements. But, we cannot make a causal statement attributing increased energy levels to working out!"
  },
  {
    "objectID": "ObservationalStudies.html#example-does-working-out-increase-energy-levels-1",
    "href": "ObservationalStudies.html#example-does-working-out-increase-energy-levels-1",
    "title": "Observational Studies",
    "section": "Example: Does working out increase energy levels?",
    "text": "Example: Does working out increase energy levels?\n\nCan we conclude from this that working out is the cause of increased energy levels?"
  },
  {
    "objectID": "ObservationalStudies.html#example-does-working-out-increase-energy-levels-2",
    "href": "ObservationalStudies.html#example-does-working-out-increase-energy-levels-2",
    "title": "Observational Studies",
    "section": "Example: Does working out increase energy levels?",
    "text": "Example: Does working out increase energy levels?\n\nThere may be other variables that we didn’t control for in this study that contribute to the observed difference.\nFor example, people who have young children might have less time to work out and also have lower energy levels.\nThis is known as confounding.\nThis study allows us to make correlation statements. But, we cannot make a causal statement attributing increased energy levels to working out!"
  },
  {
    "objectID": "ObservationalStudies.html#confounding-variables",
    "href": "ObservationalStudies.html#confounding-variables",
    "title": "Observational Studies",
    "section": "Confounding variables",
    "text": "Confounding variables\nConfounding variables: Extraneous variables that affect both the exposure (e.g., working out) and the outcome variables (e.g., increased energy), and that make it seem like there is a relationship between them are called confounding variables."
  },
  {
    "objectID": "ObservationalStudies.html#example",
    "href": "ObservationalStudies.html#example",
    "title": "Observational Studies",
    "section": "Example",
    "text": "Example\nMany years ago, investigators reported an association between coffee drinking and pancreatic cancer in an observational study (MacMahon B, Yen S, Trichopoulos D, Warren K, Nardi G. Coffee and cancer of the pancreas. N Eng J Med 1981; 304: 630-3).\n\nIf we take coffee as our exposure of interest and correlate it with an increased development of pancreatic cancer there is the potential, as was the case with these investigators, to be misled if there is a third causal factor, such as cigarette smoking, that was more common among those who reported drinking coffee.\n\nOnce the confounding variable, smoking is taken into account the correlation between coffee and pancreatic cancer disappears."
  },
  {
    "objectID": "ObservationalStudies.html#example-1",
    "href": "ObservationalStudies.html#example-1",
    "title": "Observational Studies",
    "section": "Example",
    "text": "Example\nIf we take coffee as our exposure of interest and correlate it with an increased development of pancreatic cancer there is the potential, as was the case with these investigators, to be misled if there is a third causal factor, such as cigarette smoking, that was more common among those who reported drinking coffee.\n\nOnce the confounding variable, smoking is taken into account the correlation between coffee and pancreatic cancer disappears."
  },
  {
    "objectID": "ObservationalStudies.html#reducing-confounding-matching",
    "href": "ObservationalStudies.html#reducing-confounding-matching",
    "title": "Observational Studies",
    "section": "Reducing confounding: Matching",
    "text": "Reducing confounding: Matching\nMatching is a technique that involves selecting study participants with similar characteristics outside the outcome or exposure variables.\n\nRather than using random assignment to equalize the experimental groups, the experimenters do it by matching observable characteristics.\nFor every participant in the exposed group, the researchers find a participant with comparable traits to include in the control group.\nMatching subjects facilitates valid comparisons between those groups.\nThe researchers use subject-area knowledge to identify characteristics that are critical to match."
  },
  {
    "objectID": "ObservationalStudies.html#reducing-confounding-multiple-regression",
    "href": "ObservationalStudies.html#reducing-confounding-multiple-regression",
    "title": "Observational Studies",
    "section": "Reducing confounding: Multiple Regression",
    "text": "Reducing confounding: Multiple Regression\nMultiple regression models specify the way in which different characteristics/variables (exposure and confounders) affects the outcome, thereby isolating the effect of each variable.\nchance of cancer = a x (coffee) + b x (smoking) + c x (gender) + d x (age)\n\nthis allows us to make a statement about what would happen if one variable (i.e., the exposure) were to change while all the others (i.e., the confounders) remained the same.\nObtaining isolated exposure effects conditional on the other variables remaining constant is said to adjust for (or control for) the effect of these confounders"
  },
  {
    "objectID": "ObservationalStudies.html#conditional-probability",
    "href": "ObservationalStudies.html#conditional-probability",
    "title": "Observational Studies",
    "section": "Conditional Probability",
    "text": "Conditional Probability\nYou and your friend are trying to find the perfect restaurant for dinner. You can’t decide so you want to instead rely on some observational data (i.e., restaurant reviews).\nYou find two worthy restaurants, Carla’s and Sophia’s each with 400 reviews and an indicator of whether the restaurant is recommended or not recommended.\nYou find that\n\nrecommended for Sophia’s = 250/400\nrecommended for Carla’s = 216/400\n\nSo what we have is a conditional probability:\np(recommended|Sophia’s) = 62.5%\np(recommended|Carla’s) = 54%"
  },
  {
    "objectID": "ObservationalStudies.html#conditional-probability-1",
    "href": "ObservationalStudies.html#conditional-probability-1",
    "title": "Observational Studies",
    "section": "Conditional Probability",
    "text": "Conditional Probability\nWhat if we consider age as a factor here?\n\nrecommended for 18-35 yr old diners at Sophia’s = 50/150\nrecommended for 35+ diners at Sophia’s = 200/250\nrecommended for 18-35 yr old diners at Carla’s = 180/360\nrecommended for 35+ diners at Carla’s = 36/40\n\nSo what we have is:\np(recommended|Sophia’s, younger) = 30%\np(recommended|Sophia’s, older) = 80%\np(recommended|Carla’s, younger) = 50%\np(recommended|Carla’s, older) = 90%"
  },
  {
    "objectID": "ObservationalStudies.html#whats-going-on",
    "href": "ObservationalStudies.html#whats-going-on",
    "title": "Observational Studies",
    "section": "What’s going on?",
    "text": "What’s going on?\nYou have unknowingly entered the world of Simpson’s Paradox, where a restaurant can be both better and worse than its competitor, exercise can lower and increase the risk of disease, and the same dataset can be used to prove two opposing arguments. Instead of going out to dinner, perhaps you and your friend should spend the evening discussing this fascinating statistical phenomenon.\nSimpson’s Paradox occurs when trends that appear when a dataset is separated into groups reverse when the data are aggregated."
  },
  {
    "objectID": "ObservationalStudies.html#simpsons-paradox-correlation-reversal",
    "href": "ObservationalStudies.html#simpsons-paradox-correlation-reversal",
    "title": "Observational Studies",
    "section": "Simpsons’ Paradox: Correlation Reversal",
    "text": "Simpsons’ Paradox: Correlation Reversal\nSay we have data on the number of hours of exercise per week versus the risk of developing a disease for two sets of patients, those below the age of 50 and those over the age of 50. Here are individual plots showing the relationship between exercise and risk of disease."
  },
  {
    "objectID": "ObservationalStudies.html#example-continued",
    "href": "ObservationalStudies.html#example-continued",
    "title": "Observational Studies",
    "section": "Example Continued",
    "text": "Example Continued\nWe clearly saw a negative correlation, indicating that increased levels of exercise per week are correlated with a lower risk of developing the disease for both groups. Now, let’s combine the data together on a single plot:"
  },
  {
    "objectID": "ObservationalStudies.html#resolving-the-paradox",
    "href": "ObservationalStudies.html#resolving-the-paradox",
    "title": "Observational Studies",
    "section": "Resolving the Paradox",
    "text": "Resolving the Paradox\nTo avoid Simpson’s Paradox leading us to two opposite conclusions, we need to choose to segregate the data in groups or aggregate it together. That seems simple enough, but how do we decide which to do?\nThe answer is to think causally: how was the data generated and based on this, what factors influence the results that we are not shown?\nIn the exercise vs disease example, we intuitively know that exercise is not the only factor affecting the risk of developing a disease. In the data, there are two different causes of disease yet by aggregating the data and looking at only risk vs exercise, we ignore the second cause - age - completely.\nIf we go ahead and plot risk vs age, we can see that the age of the patient is strongly positively correlated with disease risk.\n\n\n\n\n\n\n\n\n\nAs the patient increases in age, their risk of the disease increases which means older patients are more likely to develop the disease than younger patients even with the same amount of exercise. Therefore, to assess the effect of just exercise on disease, we would want to hold the age constant and change the amount of weekly exercise."
  },
  {
    "objectID": "ObservationalStudies.html#resolving-the-paradox-1",
    "href": "ObservationalStudies.html#resolving-the-paradox-1",
    "title": "Observational Studies",
    "section": "Resolving the Paradox",
    "text": "Resolving the Paradox\nIf we go ahead and plot risk vs age, we can see that the age of the patient is strongly positively correlated with disease risk.\n\n\n\n\n\n\n\n\n\nAs the patient increases in age, their risk of the disease increases which means older patients are more likely to develop the disease than younger patients even with the same amount of exercise. Therefore, to assess the effect of just exercise on disease, we would want to hold the age constant and change the amount of weekly exercise."
  },
  {
    "objectID": "3-MultipleRegression.html#takeaways",
    "href": "3-MultipleRegression.html#takeaways",
    "title": "Multiple Regression",
    "section": "Takeaways",
    "text": "Takeaways\n\nGeneralized linear models allow us to fit models to predict non-continuous outcomes\nPredicting binary outcomes requires modeling the log-odds of success, where p = probability of success"
  },
  {
    "objectID": "3-MultipleRegression.html#recap-logistic-regression",
    "href": "3-MultipleRegression.html#recap-logistic-regression",
    "title": "Multiple Regression",
    "section": "Recap: Logistic Regression",
    "text": "Recap: Logistic Regression\nLogistic regression is a generalized regression model where the outcome is a two-level categorical variable. The outcome, takes the value 1 or 0 with probability. Ultimately, it is the probability of the outcome taking the value 1 (i.e., being a “success”) that we model in relation to the predictor variables. For this to work, we transform the expected outcome in such a way that it will be bounded between 0 and 1, and hence our estimates will be sensical. The transformation we use is called the logit, and is the natural logarithm of the odds of success.\n\nModelling binary outcomes\n\n\\(y\\) takes on values 0 (failure) or 1 (success)\n\\(p\\): probability of success\n\\(1-p\\): probability of failure\nWe can’t model \\(y\\) directly, so instead we model \\(p\\)\n\nLinear model\n\\[\n\\hat{p}_i = \\beta_o + \\beta_1 \\times x\n\\]\n\nBut remember that \\(p\\) must be between 0 and 1\nWe need a link function that transforms the linear model to have an appropriate range\n\nLogit link function\nThe logit function takes values between 0 and 1 (probabilities) and maps them to values in the range negative infinity to positive infinity:\n\\[\nlogit(p) = log \\bigg( \\frac{p}{1 - p} \\bigg)\n\\]\nGeneralized linear model\n\nWe model the logit (log-odds) of \\(p\\) :\n\n\\[\nlogit(\\hat{p}) = log \\bigg( \\frac{\\hat{p}}{1 - \\hat{p}} \\bigg) = \\beta_o + \\beta_1 \\times x\n\\]\n\nThen take the inverse to obtain the predicted \\(p\\):\n\n\\[\n\\hat{p} = \\frac{e^{\\beta_o + \\beta_1 \\times x }}{1 + e^{\\beta_o + \\beta_1 \\times x }}\n\\]\nA logistic model visualized"
  },
  {
    "objectID": "3-MultipleRegression.html#multiple-logistic-regression",
    "href": "3-MultipleRegression.html#multiple-logistic-regression",
    "title": "Multiple Regression",
    "section": "Multiple Logistic Regression",
    "text": "Multiple Logistic Regression\nMultiple logistic regression is used to model the relationship between a binary outcome variable and two or more predictor variables. It extends simple logistic regression by including multiple independent variables, which can be continuous or categorical, to assess their combined effect on the probability of an event occurring. The model estimates odds ratios for each predictor while controlling for the effects of others, making it useful for understanding complex associations."
  },
  {
    "objectID": "3-MultipleRegression.html#example-irish-continued---including-more-predictors",
    "href": "3-MultipleRegression.html#example-irish-continued---including-more-predictors",
    "title": "Multiple Regression",
    "section": "Example: Irish continued - including more predictors",
    "text": "Example: Irish continued - including more predictors\nIt is frequently of interest to use multiple covariates to improve our predictive power. Let’s fit a logistic regression model including all four covariates as predictors in our model: \\[\\displaystyle\\log\\left(\\frac{p}{1-p}\\right)=\\beta_0+\\beta_1*\\mbox{Sepal.Length}+\\beta_2*\\mbox{Sepal.Width}+\\beta_3*\\mbox{Petal.Length}+\\beta_4*\\mbox{Petal.Width}\\]\n\niris_mod2 &lt;- glm(Species.binary ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width,\n                 family = binomial, \n                 data = iris2)\niris_mod2 %&gt;% coef %&gt;% round(digits = 4)\n\n (Intercept) Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n    -42.6378      -2.4652      -6.6809       9.4294      18.2861 \n\n\n\nClassification\n\nQuestion: How well did we do at classifying the Iris species?\n\n\niris_predict &lt;- iris2 %&gt;% \n                  mutate(p_hat = predict(iris_mod2, type = \"response\") %&gt;% round(2),\n                         Species_pred = ifelse(p_hat &gt;= 0.5,\"virginica\",\"versicolor\"))\n\nn_correct_full &lt;- sum(iris_predict$Species_pred == iris_predict$Species)\nn_correct_full\n\n[1] 98\n\n\nOur model correctly predicts the species class of 98 out of 100 observations."
  },
  {
    "objectID": "sampling.html",
    "href": "sampling.html",
    "title": "Sampling Principles and Strategies",
    "section": "",
    "text": "The first step in conducting research is to identify topics or questions that are to be investigated. A clearly laid out research question is helpful in identifying what subjects or cases should be studied and what variables are important. It is also important to consider how data are collected so that the data are reliable and help achieve the research goals."
  },
  {
    "objectID": "sampling.html#research-questions",
    "href": "sampling.html#research-questions",
    "title": "Untitled",
    "section": "",
    "text": "Most research questions actually break down into 2 parts:\n\nDescriptive Statistics: What relationship can we observe between the variables, in the sample?\nInferential Statistics: Supposing we see a relationship in the sample data, how much evidence is provided for a relationship in the population? Does the data provide lots of evidence for a relationship in the population, or could the relationship we see in the sample be due just to chance variation in the sampling process that gave us the data?\n\nResearch Question: In the Ireland, what is the mean height of adult males (18 years +)?\nPopulation: all Irish adult males.\nQ. Can we survey the entire population?\nA. This is nearly impossible! It would be much quicker and easier to measure only a subset of the population, a sample.\nQ. How can we ensure the sample is an accurate reflection of the population?"
  },
  {
    "objectID": "sampling.html#sampling",
    "href": "sampling.html#sampling",
    "title": "Sampling",
    "section": "Sampling",
    "text": "Sampling\n\nSuppose that we were able to choose an appropriate sample that provides an accurate representation of the Irish population of men.\nThen, we have two different means to consider:\nMean Height of the Sample\n\nStatistic - describes the sample\nCan be known, but it changes depending on the sample\nSymbol - \\(\\bar{x}\\) (pronounced “x bar”)\n\nMean Height of the Population\n\nParameter - describes the population\nUsually unknown - but we wish we knew it!\nSymbol - \\(\\mu\\) (pronounced “mu”)"
  },
  {
    "objectID": "sampling.html#sample-vs-population",
    "href": "sampling.html#sample-vs-population",
    "title": "Sampling",
    "section": "Sample vs Population",
    "text": "Sample vs Population\nA reminder of an important distinction that we want to make before discussing surveys is the distinction between a sample and a population.\nA population is the set of subjects of interest.\nA sample is the subset of the population for which we have data.\n\n\nOur goal is to use the information we’ve gathered from the sample to infer, or predict, something about the population.\nFor our example, we want to predict the population mean, using our knowledge of the sample.\nThe accuracy of our sample mean relies heavily upon how well our sample represents the population at large.\nIf our sample does a poor job at representing the population, then any inferences that we make about the population are also going to be poor.\n\nThus, it is very important to select a good sample!"
  },
  {
    "objectID": "sampling.html#random-sampling",
    "href": "sampling.html#random-sampling",
    "title": "Sampling",
    "section": "Random Sampling",
    "text": "Random Sampling\nThere are four different methods of random sampling that we will introduce:\n\nSimple Random Sampling (SRS)\nSystematic Sampling\nStratified Sampling\nCluster Sampling"
  },
  {
    "objectID": "sampling.html#example-data-fakeschool",
    "href": "sampling.html#example-data-fakeschool",
    "title": "Sampling Principles and Strategies",
    "section": "Example Data: FakeSchool",
    "text": "Example Data: FakeSchool\n\nWe will use the FakeSchool data to compare the sampling methods.\nThe dataset contains information on 28 students from FakeSchool. We will assume that this is the population from which we will sample.\n\nHere is a snippet of the data:\n\n\n# A tibble: 4 × 5\n  Students Sex   class   GPA Honors\n  &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; \n1 Alice    F     Fr     3.8  Yes   \n2 Brad     M     Fr     2.6  Yes   \n3 Caleb    M     Fr     2.25 No    \n4 Daisy    F     Fr     2.1  No    \n\n\n\nLet’s say that we are interested in mean GPA\nWe can compute the true mean GPA\n\n\nFakeSchool %&gt;% \n  pull(GPA) %&gt;% \n  mean\n\n[1] 2.766429\n\n\n\nRemember this value is not typically known!"
  },
  {
    "objectID": "sampling.html#simple-random-sampling",
    "href": "sampling.html#simple-random-sampling",
    "title": "Sampling Principles and Strategies",
    "section": "Simple Random Sampling",
    "text": "Simple Random Sampling\nIn simple random sampling (SRS), for a given sample size, n, each member of the population has an equal chance of being selected.\nLet’s select a simple random sample of 7 elements without replacement. We can accomplish this easily with the dplyr function sample_n() in R. This function requires two pieces of information:\n\nthe size of the sample\nthe dataset from which to draw the sample\n\n\nSimple Random Sampling in R\n\n## create a simple random sample (n = 7)\nsrs &lt;- FakeSchool %&gt;% \n          sample_n(7)\nsrs\n\n# A tibble: 7 × 5\n  Students Sex   class   GPA Honors\n  &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; \n1 Daisy    F     Fr      2.1 No    \n2 Bob      M     Sr      3.8 Yes   \n3 Frank    M     Sr      2   No    \n4 Garth    M     Jr      1.1 No    \n5 Eliott   M     Jr      1.9 No    \n6 Angela   F     Sr      4   Yes   \n7 Chris    M     So      4   Yes   \n\n## calculate the mean of the srs\nsrs %&gt;% \n  pull(GPA) %&gt;% \n  mean\n\n[1] 2.7\n\n\n\n\nSRS Strength vs Weaknesses\nStrengths\n\nThe selection of one element does not affect the selection of others.\nEach possible sample, of a given size, has an equal chance of being selected.\nSimple random samples tend to be good representations of the population.\nRequires little knowledge of the population.\n\nWeaknesses\n\nIf there are small subgroups within the population, a SRS may not give an accurate representation of that subgroup. In fact, it may not include it at all! This is especially true if the sample size is small.\nIf the population is large and widely dispersed, it can be costly (both in time and money) to collect the data."
  },
  {
    "objectID": "sampling.html#systematic-sampling",
    "href": "sampling.html#systematic-sampling",
    "title": "Sampling Principles and Strategies",
    "section": "Systematic Sampling",
    "text": "Systematic Sampling\nIn a systematic sample, the sample members from a larger population are selected according to a random starting point but with a fixed, periodic interval, i.\nThese are the two main steps required to implement systematic sampling:\n\nDivide the size of the target population “N” by sample size “n” to calculate the sampling interval “i”. If this value is in decimals, it must be rounded to the nearest whole number/integer.\nThen, a random starting point, “r”, may be chosen from where the sampling interval “i” is used in order to choose respondents from the target population.\n\nBefore selecting the sample group, researchers must ensure that the list of the sample frame is not organized in a cyclical or periodic way in order to avoid selecting a biased sample group.\n\n\nTo illustrate the idea, let’s take a 1-in-4 systematic sample from our FakeSchool population.\n\nSystematic Sampling in R\n\n# randomly selecting our starting element.\nstart=sample(1:4,1)\nstart\n\n[1] 3\n\n# Now find every 4th row index starting with start\nsample_rows &lt;- seq(start, nrow(FakeSchool), by = 4)\nsample_rows\n\n[1]  3  7 11 15 19 23 27\n\n# Now choose the data corresponding to the row indexes\nsys_samp &lt;- FakeSchool %&gt;% \n        filter(row_number() %in% sample_rows)\n\n\n# print the systematic sample\nsys_samp\n\n# A tibble: 7 × 5\n  Students Sex   class   GPA Honors\n  &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; \n1 Caleb    M     Fr     2.25 No    \n2 Georg    M     Fr     1.4  No    \n3 Dylan    M     So     3.5  Yes   \n4 Adam     M     Jr     3.98 Yes   \n5 Faith    F     Jr     2.5  Yes   \n6 Bob      M     Sr     3.8  Yes   \n7 Ed       M     Sr     1.5  No    \n\n# find the mean GPA from the sys_samp\nsys_samp %&gt;% \n  pull(GPA) %&gt;% \n  mean\n\n[1] 2.704286\n\n\n\n\nStrength vs Weaknesses\nStrengths\n\nAssures an even, random sampling of the population.\nIt is especially useful when the population that you are studying is arranged in time. For example, suppose you are interested in the average amount of money that people spend at the grocery store on a Wednesday evening. A systematic sample could be used by selecting every 10th person that walks into the store.\n\nWeaknesses\n\nNot every combination has an equal chance of being selected. Many combinations will never be selected using a systematic sample!\nBeware of periodicity in the population! If, the selections match some pattern then the sample may not be representative of the population.\n\n\n\nNoticing patterns in data\n\nThe FakeSchool data is ordered according to the student’s year in school (freshmen, sophomore, junior, senior) and then by GPA (highest - lowest)\nTaking a systematic sample ensures that we have a person from each class represented in our sample.\nBut, what would happen if we took a systematic sample where k = 7 and the sample started at 1?"
  },
  {
    "objectID": "sampling.html#stratified-sampling",
    "href": "sampling.html#stratified-sampling",
    "title": "Sampling Principles and Strategies",
    "section": "Stratified Sampling",
    "text": "Stratified Sampling\nIn a stratified sample, the population must first be separated into homogeneous groups, or strata. Each element only belongs to one stratum and the stratum consist of elements that are alike in some way. A simple random sample is then drawn from each stratum, which is combined to make the stratified sample.\nLet’s take a stratified sample of 7 elements from FakeSchool using the following strata: Honors, Not Honors.\n\nStratified Sampling in R\n\n# determine how many elements belong to each strata\nFakeSchool %&gt;% \n  count(Honors) \n\n# A tibble: 2 × 2\n  Honors     n\n  &lt;fct&gt;  &lt;int&gt;\n1 No        16\n2 Yes       12\n\n# get the data for each strata\n# honors\nhon_strata &lt;- FakeSchool %&gt;% \n                filter(Honors == \"Yes\")\n\n# not honors\nnonhon_strata &lt;- FakeSchool %&gt;% \n                filter(Honors == \"No\")\n\nTry to divide the sampling evenly, the sample size is odd so use the bigger number for the larger strata\n\nhon_samp &lt;- hon_strata %&gt;% \n              sample_n(3)\n\nnonhon_samp &lt;- nonhon_strata %&gt;% \n                sample_n(4)\n\nstrat_samp &lt;- full_join(hon_samp, nonhon_samp)\n\nJoining with `by = join_by(Students, Sex, class, GPA, Honors)`\n\n\n\n# print the stratified sample\nstrat_samp\n\n# A tibble: 7 × 5\n  Students Sex   class   GPA Honors\n  &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; \n1 Angela   F     Sr     4    Yes   \n2 Adam     M     Jr     3.98 Yes   \n3 Cassie   F     Jr     3.75 Yes   \n4 Garth    M     Jr     1.1  No    \n5 Brittany F     Jr     3.9  No    \n6 Frank    M     Sr     2    No    \n7 Eva      F     Fr     1.8  No    \n\n# get the mean GPA from the stratified sample\nstrat_samp %&gt;% \n  pull(GPA) %&gt;% \n  mean\n\n[1] 2.932857\n\n\n\n\nStrength vs Weaknesses\nStrengths\n\nRepresentative of the population, because elements from all strata are included in the sample.\nEnsures that specific groups are represented, sometimes even proportionally, in the sample.\nAllows comparisons to be made between strata, if necessary. For example, a stratified sample allows you to easily compare the mean GPA of Honors students to the mean GPA of non-Honors students.\n\nWeaknesses\n\nRequires prior knowledge of the population. You have to know something about the population to be able to split into strata!"
  },
  {
    "objectID": "sampling.html#cluster-sampling",
    "href": "sampling.html#cluster-sampling",
    "title": "Sampling Principles and Strategies",
    "section": "Cluster Sampling",
    "text": "Cluster Sampling\nCluster sampling is a sampling method used when natural groups are evident in the population. The clusters should all be similar each other: each cluster should be a small scale representation of the population. To take a cluster sample, a random sample of the clusters is chosen. The elements of the randomly chosen clusters make up the sample.\nLet’s take a cluster sample using the grade level (freshmen, sophomore, junior, senior) of FakeSchool as the clusters. Let’s take a random sample of 2 of them.\n\nCluster Sampling in R\n\ncluster_samp &lt;- FakeSchool %&gt;% \n                  group_nest(class) %&gt;% \n                  sample_n(size = 2) %&gt;% \n                  unnest(data)\n\n## what class groups were used \ncluster_samp$class %&gt;% unique()\n\n[1] Jr So\nLevels: Fr Jr So Sr\n\n# calculate the mean GPA from the cluster sample\ncluster_samp %&gt;% \n  pull(GPA) %&gt;% \n  mean\n\n[1] 3.057857\n\n\n\n\nStrength vs Weaknesses\nStrengths\nMakes it possible to sample if there is no list of the entire population, but there is a list of subpopulations. For example, there is not a list of all school members in the United States. However, there is a list of schools that you could sample and then acquire the members list from each of the selected schools.\nWeaknesses\nNot always representative of the population. Elements within clusters tend to be similar to one another based on some characteristic(s). This can lead to over-representation or under-representation of those characteristics in the sample."
  },
  {
    "objectID": "sampling.html#sampling-principles-and-strategies",
    "href": "sampling.html#sampling-principles-and-strategies",
    "title": "Sampling",
    "section": "Sampling Principles and Strategies",
    "text": "Sampling Principles and Strategies\n\nResearch Question(s)\nResearch Question: Over the last 5 years, how many MU Data Science or Statistics graduates have gone on to get a job in a field directly related to their degree.\nPopulation: All DS or Statistics graduates from MU from the last 5 years.\nQ. Can we survey the entire population?\nA. This would likely be very difficult. It is more realistic to assume that we can work with a fraction of the population.\nQ. How can we ensure the sample is an accurate reflection of the population?\nA. Appropriate sampling.\nMost research questions actually break down into 2 parts:\n\nDescriptive Statistics: What relationship can we observe between the variables in the sample?\nInferential Statistics: Supposing we see a relationship in the sample data, how much evidence is provided for a relationship in the population? Does the data provide lots of evidence for a relationship in the population, or could the relationship we see in the sample be due just to chance variation in the sampling process that gave us the data?\n\n\n\nAnecdotal Evidence\n“I met two students who did a Data Science degree in Maynooth but they are not working as data scientists. The degree must not get you a Data Science job.”\nThere are two problems here. First, the data only represent two cases. Second, and more importantly, it is unclear whether these cases are actually representative of the population. Data collected in this haphazard fashion are called anecdotal evidence."
  },
  {
    "objectID": "sampling.html#sampling-from-the-population",
    "href": "sampling.html#sampling-from-the-population",
    "title": "Sampling Principles and Strategies",
    "section": "Sampling from the population",
    "text": "Sampling from the population\nA population is the set of subjects of interest.\nA sample is the subset of the population for which we have data.\nSuppose that we were able to choose an appropriate sample that provides an accurate representation of the DS and Statistics Graduates:\n\nNow we have two different summaries to consider for our research question:\nProportion of the sample that went into a directly related field\n\nStatistic - describes the sample\nCan be known, but it changes depending on the sample\nSymbol - \\(\\hat{p}\\)\n\nProportion of the population that went into a directly related field*\n\nParameter - describes the population\nUsually unknown - but we wish we knew it!\nSymbol - \\(\\pi\\)\n\n\n\n\nOur goal is to use the information we’ve gathered from the sample to infer, or predict, something about the population.\nFor our example, we want to predict the population proportion, using our knowledge of the sample.\nThe accuracy of our sample proportion relies heavily upon how well our sample represents the population at large.\nIf our sample does a poor job at representing the population, then any inferences that we make about the population are also going to be poor."
  },
  {
    "objectID": "sampling.html#sampling-procedures",
    "href": "sampling.html#sampling-procedures",
    "title": "Sampling Principles and Strategies",
    "section": "Sampling Procedures",
    "text": "Sampling Procedures\nAlmost all statistical methods are based on the notion of implied randomness. If data are not collected in a random framework from a population, these statistical methods – the estimates and errors associated with the estimates – are not reliable.\nThere are four different methods of random sampling that we will introduce:\n\nSimple Random Sampling (SRS)\nSystematic Sampling\nStratified Sampling\nCluster Sampling"
  },
  {
    "objectID": "4-sampling.html",
    "href": "4-sampling.html",
    "title": "Sampling Principles and Strategies",
    "section": "",
    "text": "The first step in conducting research is to identify topics or questions that are to be investigated. A clearly laid out research question is helpful in identifying what subjects or cases should be studied and what variables are important. It is also important to consider how data are collected so that the data are reliable and help achieve the research goals."
  },
  {
    "objectID": "4-sampling.html#sampling-from-the-population",
    "href": "4-sampling.html#sampling-from-the-population",
    "title": "Sampling Principles and Strategies",
    "section": "Sampling from the population",
    "text": "Sampling from the population\nA population is the set of subjects of interest.\nA sample is the subset of the population for which we have data.\nSuppose that we were able to choose an appropriate sample that provides an accurate representation of the DS and Statistics Graduates:\n\nNow we have two different summaries to consider for our research question:\nProportion of the sample that went into a directly related field\n\nStatistic - describes the sample\nCan be known, but it changes depending on the sample\nSymbol - \\(\\hat{p}\\)\n\nProportion of the population that went into a directly related field\n\nParameter - describes the population\nUsually unknown - but we wish we knew it!\nSymbol - \\(\\pi\\)\n\n\n\n\nOur goal is to use the information we’ve gathered from the sample to infer, or predict, something about the population.\nFor our example, we want to predict the population proportion, using our knowledge of the sample.\nThe accuracy of our sample proportion relies heavily upon how well our sample represents the population at large.\nIf our sample does a poor job at representing the population, then any inferences that we make about the population are also going to be poor."
  },
  {
    "objectID": "4-sampling.html#sampling-procedures",
    "href": "4-sampling.html#sampling-procedures",
    "title": "Sampling Principles and Strategies",
    "section": "Sampling Procedures",
    "text": "Sampling Procedures\nAlmost all statistical methods are based on the notion of implied randomness. If data are not collected in a random framework from a population, these statistical methods – the estimates and errors associated with the estimates – are not reliable.\nThere are four different methods of random sampling that we will introduce:\n\nSimple Random Sampling (SRS)\nSystematic Sampling\nStratified Sampling\nCluster Sampling"
  },
  {
    "objectID": "4-sampling.html#example-data-fakeschool",
    "href": "4-sampling.html#example-data-fakeschool",
    "title": "Sampling Principles and Strategies",
    "section": "Example Data: FakeSchool",
    "text": "Example Data: FakeSchool\n\nWe will use the FakeSchool data to compare the sampling methods.\nThe dataset contains information on 28 students from FakeSchool. We will assume that this is the population from which we will sample.\n\n\nlibrary(tigerstats)\nlibrary(tidyverse)\ndata(\"FakeSchool\")\nFakeSchool &lt;- as_tibble(FakeSchool)\n\nHere is a snippet of the data:\n\n\n# A tibble: 4 × 5\n  Students Sex   class   GPA Honors\n  &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; \n1 Alice    F     Fr     3.8  Yes   \n2 Brad     M     Fr     2.6  Yes   \n3 Caleb    M     Fr     2.25 No    \n4 Daisy    F     Fr     2.1  No    \n\n\n\nLet’s say that we are interested in mean GPA\nWe can compute the true mean GPA\n\n\nFakeSchool %&gt;% \n  pull(GPA) %&gt;% \n  mean\n\n[1] 2.766429\n\n\n\nRemember this value is not typically known!"
  },
  {
    "objectID": "4-sampling.html#simple-random-sampling",
    "href": "4-sampling.html#simple-random-sampling",
    "title": "Sampling Principles and Strategies",
    "section": "Simple Random Sampling",
    "text": "Simple Random Sampling\nIn simple random sampling (SRS), for a given sample size, n, each member of the population has an equal chance of being selected.\nLet’s select a simple random sample of 7, without replacement. We can accomplish this easily with the dplyr function sample_n() in R. This function requires two pieces of information:\n\nthe size of the sample\nthe dataset from which to draw the sample\n\n\nSimple Random Sampling in R\n\n## create a simple random sample (n = 7)\nsrs &lt;- FakeSchool %&gt;% \n          sample_n(7)\nsrs\n\n# A tibble: 7 × 5\n  Students Sex   class   GPA Honors\n  &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; \n1 Daisy    F     Fr      2.1 No    \n2 Bob      M     Sr      3.8 Yes   \n3 Frank    M     Sr      2   No    \n4 Garth    M     Jr      1.1 No    \n5 Eliott   M     Jr      1.9 No    \n6 Angela   F     Sr      4   Yes   \n7 Chris    M     So      4   Yes   \n\n## calculate the mean of the srs\nsrs %&gt;% \n  pull(GPA) %&gt;% \n  mean\n\n[1] 2.7\n\n\n\n\nSRS Strength vs Weaknesses\nStrengths\n\nThe selection of one element does not affect the selection of others.\nEach possible sample, of a given size, has an equal chance of being selected.\nSimple random samples tend to be good representations of the population.\nRequires little knowledge of the population.\n\nWeaknesses\n\nIf there are small subgroups within the population, a SRS may not give an accurate representation of that subgroup. In fact, it may not include it at all! This is especially true if the sample size is small.\nIf the population is large and widely dispersed, it can be costly (both in time and money) to collect the data."
  },
  {
    "objectID": "4-sampling.html#systematic-sampling",
    "href": "4-sampling.html#systematic-sampling",
    "title": "Sampling Principles and Strategies",
    "section": "Systematic Sampling",
    "text": "Systematic Sampling\nIn a systematic sample, the sample members from a larger population are selected according to a random starting point but with a fixed, periodic interval, i.\nThese are the two main steps required to implement systematic sampling:\n\nDivide the size of the target population “N” by sample size “n” to calculate the sampling interval “i”. If this value is in decimals, it must be rounded to the nearest whole number/integer.\nThen, a random starting point, “r”, may be chosen from where the sampling interval “i” is used in order to choose respondents from the target population.\n\nBefore selecting the sample group, researchers must ensure that the list of the sample frame is not organized in a cyclical or periodic way in order to avoid selecting a biased sample group.\n\n\nTo illustrate the idea, let’s take a 1-in-4 systematic sample from our FakeSchool population.\n\nSystematic Sampling in R\n\n# randomly selecting our starting element.\nstart=sample(1:4,1)\nstart\n\n[1] 3\n\n# Now find every 4th row index starting with start\nsample_rows &lt;- seq(start, nrow(FakeSchool), by = 4)\nsample_rows\n\n[1]  3  7 11 15 19 23 27\n\n# Now choose the data corresponding to the row indexes\nsys_samp &lt;- FakeSchool %&gt;% \n        filter(row_number() %in% sample_rows)\n\n\n# print the systematic sample\nsys_samp\n\n# A tibble: 7 × 5\n  Students Sex   class   GPA Honors\n  &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; \n1 Caleb    M     Fr     2.25 No    \n2 Georg    M     Fr     1.4  No    \n3 Dylan    M     So     3.5  Yes   \n4 Adam     M     Jr     3.98 Yes   \n5 Faith    F     Jr     2.5  Yes   \n6 Bob      M     Sr     3.8  Yes   \n7 Ed       M     Sr     1.5  No    \n\n# find the mean GPA from the sys_samp\nsys_samp %&gt;% \n  pull(GPA) %&gt;% \n  mean\n\n[1] 2.704286\n\n\n\n\nStrength vs Weaknesses\nStrengths\n\nAssures an even, random sampling of the population.\nIt is especially useful when the population that you are studying is arranged in time. For example, suppose you are interested in the average amount of money that people spend at the grocery store on a Wednesday evening. A systematic sample could be used by selecting every 10th person that walks into the store.\n\nWeaknesses\n\nNot every combination has an equal chance of being selected. Many combinations will never be selected using a systematic sample!\nBeware of periodicity in the population! If, the selections match some pattern then the sample may not be representative of the population.\n\n\n\nNoticing patterns in data\n\nThe FakeSchool data is ordered according to the student’s year in school (freshmen, sophomore, junior, senior) and then by GPA (highest - lowest)\nTaking a systematic sample ensures that we have a person from each class represented in our sample.\nBut, what would happen if we took a systematic sample where k = 7 and the sample started at 1?"
  },
  {
    "objectID": "4-sampling.html#stratified-sampling",
    "href": "4-sampling.html#stratified-sampling",
    "title": "Sampling Principles and Strategies",
    "section": "Stratified Sampling",
    "text": "Stratified Sampling\nIn a stratified sample, the population must first be separated into homogeneous groups, or strata. Each element only belongs to one stratum and the stratum consist of elements that are alike in some way. A simple random sample is then drawn from each stratum, which is combined to make the stratified sample.\nLet’s take a stratified sample of 7 elements from FakeSchool using the following strata: Honors, Not Honors.\n\nStratified Sampling in R\n\n# determine how many elements belong to each strata\nFakeSchool %&gt;% \n  count(Honors) \n\n# A tibble: 2 × 2\n  Honors     n\n  &lt;fct&gt;  &lt;int&gt;\n1 No        16\n2 Yes       12\n\n# get the data for each strata\n# honors\nhon_strata &lt;- FakeSchool %&gt;% \n                filter(Honors == \"Yes\")\n\n# not honors\nnonhon_strata &lt;- FakeSchool %&gt;% \n                filter(Honors == \"No\")\n\nTry to divide the sampling evenly, the sample size is odd so use the bigger number for the larger strata\n\nhon_samp &lt;- hon_strata %&gt;% \n              sample_n(3)\n\nnonhon_samp &lt;- nonhon_strata %&gt;% \n                sample_n(4)\n\nstrat_samp &lt;- full_join(hon_samp, nonhon_samp)\n\nJoining with `by = join_by(Students, Sex, class, GPA, Honors)`\n\n\n\n# print the stratified sample\nstrat_samp\n\n# A tibble: 7 × 5\n  Students Sex   class   GPA Honors\n  &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; \n1 Angela   F     Sr     4    Yes   \n2 Adam     M     Jr     3.98 Yes   \n3 Cassie   F     Jr     3.75 Yes   \n4 Garth    M     Jr     1.1  No    \n5 Brittany F     Jr     3.9  No    \n6 Frank    M     Sr     2    No    \n7 Eva      F     Fr     1.8  No    \n\n# get the mean GPA from the stratified sample\nstrat_samp %&gt;% \n  pull(GPA) %&gt;% \n  mean\n\n[1] 2.932857\n\n\n\n\nStrength vs Weaknesses\nStrengths\n\nRepresentative of the population, because elements from all strata are included in the sample.\nEnsures that specific groups are represented, sometimes even proportionally, in the sample.\nAllows comparisons to be made between strata, if necessary. For example, a stratified sample allows you to easily compare the mean GPA of Honors students to the mean GPA of non-Honors students.\n\nWeaknesses\n\nRequires prior knowledge of the population. You have to know something about the population to be able to split into strata!"
  },
  {
    "objectID": "4-sampling.html#cluster-sampling",
    "href": "4-sampling.html#cluster-sampling",
    "title": "Sampling Principles and Strategies",
    "section": "Cluster Sampling",
    "text": "Cluster Sampling\nCluster sampling is a sampling method used when natural groups are evident in the population. The clusters should all be to similar each other: each cluster should be a small scale representation of the population. To take a cluster sample, a random sample of the clusters is chosen. The elements of the randomly chosen clusters make up the sample.\nLet’s assume that we have a cluster variable (with clusters 1-4) in the FakeSchool data.\n\n\n# A tibble: 28 × 6\n   Students Sex   class   GPA Honors cluster\n   &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt;    &lt;int&gt;\n 1 Alice    F     Fr     3.8  Yes          1\n 2 Brad     M     Fr     2.6  Yes          1\n 3 Caleb    M     Fr     2.25 No           2\n 4 Daisy    F     Fr     2.1  No           4\n 5 Faye     F     Fr     2    No           2\n 6 Eva      F     Fr     1.8  No           4\n 7 Georg    M     Fr     1.4  No           2\n 8 Andrea   F     So     4    Yes          2\n 9 Betsy    F     So     4    Yes          1\n10 Chris    M     So     4    Yes          1\n# ℹ 18 more rows\n\n\n\nCluster Sampling in R\nLet’s take a random sample of 2 of two clusters.\n\ncluster_samp &lt;- FakeSchool %&gt;% \n                  group_nest(cluster) %&gt;% \n                  sample_n(size = 2) %&gt;% \n                  unnest(data)\n\n## what class groups were used \ncluster_samp$cluster %&gt;% unique()\n\n[1] 4 1\n\n# calculate the mean GPA from the cluster sample\ncluster_samp %&gt;% \n  pull(GPA) %&gt;% \n  mean\n\n[1] 2.82\n\n\n\n\nStrength vs Weaknesses\nStrengths\nMakes it possible to sample if there is no list of the entire population, but there is a list of subpopulations. For example, there is not a list of all school members in the United States. However, there is a list of schools that you could sample and then acquire the members list from each of the selected schools.\nWeaknesses\nNot always representative of the population. Elements within clusters tend to be similar to one another based on some characteristic(s). This can lead to over-representation or under-representation of those characteristics in the sample."
  },
  {
    "objectID": "5-experiments.html",
    "href": "5-experiments.html",
    "title": "Experimental Studies",
    "section": "",
    "text": "Studies where the researchers assign treatments to cases are called experiments. When this assignment includes randomization, e.g., using a coin flip to decide which treatment a patient receives, it is called a randomized experiment. Randomized experiments are fundamentally important when trying to show a causal connection between two variables."
  },
  {
    "objectID": "5-experiments.html#establishing-causality",
    "href": "5-experiments.html#establishing-causality",
    "title": "Experimental Studies",
    "section": "Establishing Causality",
    "text": "Establishing Causality\n\nA treatment will often be a binary variable that is either 0 or 1. It is 0 if the person is not treated, which is to say they are in the control group, and 1 if they are treated.\nWe will typically have some outcome of interest, Y, for each person or observational unit, and that could be categorical or continuous.\nA treatment is causal if the outcome for a person, given they were not treated, is different to their outcome given they were treated.\nIf we could both treat and control the one individual at the one time, then we would know that it was only the treatment that had caused any change. However, the fundamental problem of causal inference is that we cannot both treat and control the same individual at the same time.\n\nwe instead compare the average of two groups — all those treated and all those not.\nwe estimate the counterfactual at a group level because of the impossibility of doing it at an individual level.\nthis trade-off allows us to move forward but comes at the cost of certainty.\nwe must instead rely on randomization, probabilities, and expectations."
  },
  {
    "objectID": "5-experiments.html#controlled-experiments",
    "href": "5-experiments.html#controlled-experiments",
    "title": "Experimental Studies",
    "section": "Controlled Experiments",
    "text": "Controlled Experiments\nControlled experiments are a scientific test (or a series of tests) to verify how one or more conditions (treatments - variables that are controlled by the scientist) affect one or more outcome (response) variables. We usually consider a default of there being no effect and we look for evidence that would cause us to change our mind.\nDesign of experiments can produce an experiment that efficiently answers the questions of interest, optimizing the information for the available resources. The scientific method involves:\n(a) Stating a hypothesis\n(b) Planning an experiment to objectively test the hypothesis\n(c) Observing and carefully collecting data\n(d) Interpreting experimental results (test your hypothesis)"
  },
  {
    "objectID": "5-experiments.html#general-considerations",
    "href": "5-experiments.html#general-considerations",
    "title": "Experimental Studies",
    "section": "General Considerations",
    "text": "General Considerations\n\nBasic considerations of experimentation\n\nProvide a valid comparison between treatments\nProvide valid information on the relationship between variables of interest\n\nBasic requisites\n\nThe experimental conditions must represent real conditions of the problem of interest\nTreatment comparison must be made free from other possible explanations due to the presence of other variables (confounding)\nTreatment comparison must be made with the lowest possible influence from random variation\nThe uncertainty level of the conclusions must be known\nThe experiment must be the simplest possible"
  },
  {
    "objectID": "5-experiments.html#steps-of-experimentation",
    "href": "5-experiments.html#steps-of-experimentation",
    "title": "Experimental Studies",
    "section": "Steps of Experimentation",
    "text": "Steps of Experimentation\n\n1. Define the Problem and Objectives\n\nClearly state the research question and objectives.\n\nEstablish a logical progression: Objectives → Scientific Hypotheses → Statistical Hypotheses.\n\nExamples:\n\nInvestigate the effect of Vitamin C on odontoblast length (cells responsible for tooth growth).\n\nDetermine the lethal dose of a pesticide for an agricultural pest.\n\nIdentify the optimal temperature settings for efficient machine operation.\n\n\n\n\n2. Define Experimental Conditions\n\nControlled conditions: Experiments conducted in controlled environments (e.g., greenhouses, laboratories, experimental stations).\n\nLess controlled conditions: Field studies in natural settings (e.g., farms, forests).\n\n\n\n3. Identify Variables of Interest\n\nResponse Variable (dependent variable): The outcome being measured.\n\nExample: Odontoblast length in guinea pigs, blood pressure rates.\n\n\nTreatment Factors and Levels (independent variables): Variables manipulated in the experiment.\n\nQuantitative example: Vitamin C dose (0.5, 1, 2 mg/day).\n\nQualitative example: Diet type (natural vs. artificial).\n\n\nLocal Control (Blocking) Factors: Variables controlled to reduce variability (e.g., environmental conditions, batch effects).\n\n\n\n4. Identify Experimental and Observational Units\n\nExperimental Unit: The smallest unit to which a treatment is randomly assigned.\n\nExamples: 60 guinea pigs, a plot of land with 200 trees, a section of a laboratory bench with 20 Petri dishes.\n\n\nObservational Unit: The entity from which data is collected.\n\nExamples: A single insect, tree, or Petri dish.\n\n\n\n\n5. Define Observations to be Made\n\nQualitative Observations: Presence or absence of a feature (e.g., morphological traits).\n\nQuantitative Observations: Measurable data (e.g., weight, height, count, proportion).\n\nOrdered Observations: Ranked data with an inherent order (e.g., disease severity, grading scales).\n\n\n\n6. Select the Experimental Design\n\nChoose the simplest design that meets the study’s needs without oversimplifying.\n\n\n\n7. Conduct the Experiment\n\nEnsure bias-free procedures by considering:\n\nWho is conducting the experiment.\n\nThe location of the experiment.\n\nStart and end dates.\n\nThe relevance and feasibility of the experiment.\n\nAssociated costs.\n\n\n\n\n8. Analyze Data and Interpret Results\n\nConduct analysis according to the experimental design.\n\nInterpret results within the context of:\n\nExperimental conditions.\n\nTested hypotheses.\n\nEstablished scientific knowledge.\n\n\nConsider the potential consequences of incorrect conclusions."
  },
  {
    "objectID": "5-experiments.html#determination-of-the-levels-of-a-factor",
    "href": "5-experiments.html#determination-of-the-levels-of-a-factor",
    "title": "Experimental Studies",
    "section": "Determination of the levels of a factor",
    "text": "Determination of the levels of a factor\n\nSelection: obtain the best treatments within a big set of treatments\nComparison: a small group of treatments that qualitatively differ is compared to establish differences\n\ne.g. comparison between the effects of two different diets on blood pressure\ne.g., comparison of delivery method of vitamin C (orange juice or ascorbic acid)\n\nOptimization: find the optimal level within a group of treatments\n\ne.g. dose-response experiments\n\n\n(4) Identification of the experimental and observational unit\n\nExperimental unit: individual or group of individuals in which a treatment is randomly applied; may give rise to one or more experimental units\n\ne.g. 60 guinea pigs, a plot of land with 200 trees, a section of a laboratory bench with 20 Petri dishes, etc\n\nObservational unit: physical entity that produces a unique value for the response variable\n\ne.g. a single insect, a single tree, a single Petri dish\n\n\n(5) Definition of the observations to be made\n\nObservations: essential or accessory\nQualitative: presence or absence of a morphological feature,\nQuantitative: weight, counts, proportion survived, proportion damaged\nOrdered: degree of severity of a disease, grading scale \\(\\rightarrow\\) relationship of order\nImportant things to be aware of:\n\nThe way observations will be taken\nThe way they will be registered\nIf sampling: define the number and size of samples (consider representativeness)\n\n\n(6) Selection of the experimental scheme\n\nMust be the simplest possible\nSystematic experiments: must not be used\nRandomized experiments: allow for the comparison of treatments, free from confounding"
  },
  {
    "objectID": "5-experiments.html#the-three-key-principles-of-experimentation",
    "href": "5-experiments.html#the-three-key-principles-of-experimentation",
    "title": "Experimental Studies",
    "section": "The Three Key Principles of Experimentation",
    "text": "The Three Key Principles of Experimentation\n\nRandomization: is the assignment of treatments to experimental units so that every unit has the same probability of receiving each treatment.\nReplication: provides a measurement of uncontrolled variation; it is the application of each treatment several times, i.e. to several experimental units\nLocal control: is the grouping of experimental units into groups called BLOCKS, the units within a group being as similar as possible (e.g. different growth chambers, people, time periods, different trays in a lab bench, etc)\n\n\nRandomization\n\nThe key to telling a causal story is the counterfactual: what would have happened in the absence of the treatment.\n\nThis means that establishing the control group is critical because when we do that, we establish the counterfactual.\n\nWhat we hope to be able to do is to find treatment and control groups that are the same, but for the treatment.\n\nWe might be worried about, say, underlying trends, which is one issue with a before-and-after comparison, or selection bias, which could occur when we allow self-selection. Either of these issues could result in biased estimators.\nWe use randomization to go some way to addressing these.\n\n\n\n\nRandomization\nTo explore ideas of randomization, we simulate a population, and then randomly sample from it. We will set it up so that 20% of the population are smokers, and the rest are not.\n\nset.seed(853)\n\nnumber_of_people &lt;- 5000\n\npopulation &lt;-\n  tibble(\n    person = c(1:number_of_people),\n    smoking_status = sample(\n      x = c(\"Smoker\", \"Non-Smoker\"),\n      size  = number_of_people,\n      replace = TRUE,\n      prob = c(0.2,0.8)\n    )\n  )\n\nLet’s look at population characteristics\n\npopulation %&gt;% \n  count(smoking_status)\n\n# A tibble: 2 × 2\n  smoking_status     n\n  &lt;chr&gt;          &lt;int&gt;\n1 Non-Smoker      4003\n2 Smoker           997\n\n\nNow let’s sample from the population and randomly assign a treatment and control group.\n\nset.seed(853)\n\nsample_size &lt;- 1000\n\nsample &lt;-\n  population %&gt;% \n  sample_n(sample_size) %&gt;% \n  mutate(group = sample(\n    x = c(\"Treatment\", \"Control\"),\n    size  = sample_size,\n    replace = TRUE\n  ))\n\nNow let’s look at sample characteristics within each group (treatment and control). Is the distribution of population characteristics reflected within each group?\n\nsample %&gt;% \n  count(group,  smoking_status) %&gt;% \n  group_by(group) %&gt;% \n  mutate(prop = n / sum(n)) \n\n# A tibble: 4 × 4\n# Groups:   group [2]\n  group     smoking_status     n  prop\n  &lt;chr&gt;     &lt;chr&gt;          &lt;int&gt; &lt;dbl&gt;\n1 Control   Non-Smoker       364 0.747\n2 Control   Smoker           123 0.253\n3 Treatment Non-Smoker       415 0.809\n4 Treatment Smoker            98 0.191\n\n\n\n\nInternal validity\n\nIf the treated and control groups are the same in all ways and remain that way, but for the treatment, then we have internal validity, which is to say that our control will work as a counterfactual and our results can speak to a difference between the groups in that study.\nInternal validity means that our estimates of the effect of the treatment are speaking to the treatment and not some other aspect.\nThey mean that we can use our results to make claims about what happened in the experiment.\n\n\n\nExternal validity\n\nIf the group to which we applied our randomization were representative of the broader population, and the experimental set-up were fairly similar to outside conditions, then we further could have external validity.\n\nThat would mean that the difference we find does not just apply in our own experiment, but also in the broader population.\nExternal validity means that we can use our experiment to make claims about what would happen outside the experiment.\nIt is randomization that has allowed that to happen.\n\nBut this means we need randomization twice. Firstly, into the group that was subject to the experiment, and then secondly, between treatment and control.\n\n\n\nBlocking\n\nOften there are covariates in the experimental units that are known to affect the response variable and must be taken into account.\nIdeally an experimenter can group the experimental units into blocks where the within block variance is small, but the block to block variability is large.\n\nFor example, in testing a drug to prevent heart disease, we know that gender and age also impact the outcome. We may want to partition our study participants into gender and age groups and then randomly assign the treatment (placebo vs drug) within the group.\n\nOften blocking variables are not the variables that we are primarily interested in, but must nevertheless be considered. We call these nuisance variables.\n\n\n\nBlocking Example\nExample 1. An agricultural field study has three fields in which the researchers will evaluate the quality of three different varieties of barley. Due to how they harvest the barley, we can only create a maximum of three plots in each field. In this example we will block on field since there might be differences in soil type, drainage, etc from field to field. In each field, we will plant all three varieties so that we can tell the difference between varieties without the block effect of field confounding our inference. In this example, the varieties are nested within the fields.\n\n\n# A tibble: 9 × 4\n  Field  Plot Variety Quality\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;  \n1     1     1 A       value  \n2     1     2 B       .      \n3     1     3 C       .      \n4     2     1 A       .      \n5     2     2 B       .      \n6     2     3 C       .      \n7     3     1 A       .      \n8     3     2 B       .      \n9     3     3 C       .      \n\n\n\n\nMain types of blocks\n\nNatural divisions\n\nYoung animals – litters, egg masses\nPeople or animals – gender\nContinuous gradient of change\n\nPlots on the field – declivity, humidity, fertility\nPeople or animals – age, weight, size\nSeverity of disease on the field\n\nExperimental management\n\nLaboratory procedures – technician, day, stand\nMaterial availability\n\n\n\n\n\nRandomized Complete Block Design\n\nThe aim is to have heterogeneity between blocks and homogeneity within\n\ne.g. a greenhouse with a temperature gradient, different days, different observers\nBlock is the replicate\nThe number of experimental units per block is equal to the number of treatments and every treatment occurs once in each block, the order of the treatments within a block being randomized\n\n\n\n\n\n\n\nLaboratory chamber with a humidity gradient inside\n\nLaboratory chambers with homogeneous temperature and humidity inside\n\nA field with a fertility gradient\n\nA greenhouse with a temperature gradient during the day\n\nThe dataset oatvar in the faraway library contains information about an experiment on eight different varieties of oats.\n\nThe area in which the experiment was done had some systematic variability and the researchers divided the area up into five different blocks in which they felt the area inside a block was uniform while acknowledging that some blocks are likely superior to others for growing crops.\nWithin each block, the researchers created eight plots and randomly assigned a variety to a plot. This type of design is called a Randomized Complete Block Design (RCBD) because each block contains all possible levels of the factor of primary interest.\n\n\n\nRandomized Complete Block Design - Example\n\ndata('oatvar', package='faraway')\nggplot(oatvar, aes(y=yield, x=block, color=variety)) + \n    geom_point(size=5) +\n    geom_line(aes(x=as.integer(block)))\n\n\n\n\n\n\n\n\n\n\nFactorial Experiments\n\nThere may often be more than one factor of interest to the experimenter\nExperiments that involve more than one randomized or treatment factor are called factorial experiments\nIn general, the number of treatments in a factorial experiment is the product of the numbers of levels of the treatment factors.\nThe disadvantage of this is that the number of treatments increase very quickly.\n\nExample: Pest control\n\nFactor 1: two pesticides (A and B)\nFactor 2: two doses\nThe experiment has a total of \\(2\\times 2 = 4\\) treatments\n\nExample: The Tooth Growth data\n\nFactor 1: two deliveries of vitamin C (orange juice, ascorbic acid)\nFactor 2: three doses (0.5, 1, and 2 mg/day)\nThe experiment has a total of \\(2\\times 3 = 6\\) treatments\n\n\n\n\nFor the Pesticide example, suppose 3 replicates and a completely randomized design\n\nFor the Pesticide example, suppose 3 replicates and a randomized complete block design\n\n\nThe major advantage of factorial experiments is that they allow for the detection of interactions.\nTwo factors are said to interact if the effect of one, on the response variable, depends upon the level of the other.\nIf they do not interact, they are said to be independent.\nOther terms that are synonymous with “interacting factors” are dependent and nonadditive.\n\n\n\nUsing an Interaction Plot\n\n\nA set of parallel lines indicates no interaction\n\n\n\nClearly an interaction as lines have different slopes"
  },
  {
    "objectID": "5-experiments.html#steps-of-experimentation-1",
    "href": "5-experiments.html#steps-of-experimentation-1",
    "title": "Experimental Studies",
    "section": "Steps of Experimentation",
    "text": "Steps of Experimentation\n(7) Conduction of the experiment\n\nBias-free procedures\nAlso consider\n\nEntity carrying out the experiment\nPlace of execution\nStart and end dates\nExperiment relevance\nExpenses\n\n\n(8) Data analysis and interpretation of results\n\nAnalysis in accordance with the experimental design\nInterpretation of the results within the experimental conditions, tested hypotheses and related to the previously established facts\nConsequences and probabilities of a wrong decision"
  },
  {
    "objectID": "Tutorial1.html",
    "href": "Tutorial1.html",
    "title": "DS152 Tutorial Sheet 1",
    "section": "",
    "text": "Attempt the questions below before your tutorial in the week beginning 24th February 2025."
  },
  {
    "objectID": "Tutorial1.html#instructions",
    "href": "Tutorial1.html#instructions",
    "title": "DS152 Tutorial Sheet 1",
    "section": "",
    "text": "Attempt the questions below before your tutorial in the week beginning 24th February 2025."
  },
  {
    "objectID": "Tutorial1.html#exercise-1",
    "href": "Tutorial1.html#exercise-1",
    "title": "DS152 Tutorial Sheet 1",
    "section": "Exercise 1",
    "text": "Exercise 1\nThe table below displays the number of tuberculosis cases documented by the World Health Organization in Afghanistan, Brazil, and China between 1999 and 2000. The data contains values associated with four variables (country, year, cases, and population). The tidy format for the data is:\n\ntable1\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n\nThe data table below organizes the same data in an untidy format.\n\n\nuntidy_tab1\n\n# A tibble: 12 × 4\n   country      year type            count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\n\n\nWhat are the characteristics of this dataset that make it not tidy?\nIn the code below, what should [A], [B], [C], [D] and [E] be replaced with to make this dataset tidy?\n\n\ntidy_tab1 &lt;- untidy_tab1 %&gt;% \n                pivot_[A]([B] = [C],\n                          [D] = [E])\n\n[A] =\n[B] =\n[C] =\n[D] =\n[E] =\n\nThe data table below, which includes only the cases data, is organized in an untidy format.\n\n\nuntidy_tab2a\n\n# A tibble: 3 × 3\n  country     `1999` `2000`\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\n\n\nWhat are the characteristics of this dataset that make it not tidy?\nIn the code below, what should [A], [B], [C], [D], [E] and [F] be replaced with to make this dataset tidy?\n\n\ntidy_tab2a &lt;- untidy_tab2a %&gt;%  \n                pivot_[A](-[B],\n                           [C] = [D],\n                           [E] = [F])\n\n[A] =\n[B] =\n[C] =\n[D] =\n[E] =\n[F] =\n\nOnce tidy_tab2a has been created you should make the year variable numeric. Replace [G] in the code below to achieve this.\n\n\ntidy_tab2a &lt;- tidy_tab2a %&gt;% mutate(year = [G](year))\n\n[G] =\n\nThe data table below includes the population data organized in a tidy format (note: China has an additional year of data):\n\n\ntidy_tab2b\n\n# A tibble: 7 × 3\n  country      year population\n  &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999   19987071\n2 Afghanistan  2000   20595360\n3 Brazil       1999  172006362\n4 Brazil       2000  174504898\n5 China        1999 1272915272\n6 China        2000 1280428583\n7 China        2009 1339125595\n\n\nAssume you want to join tidy_tab2a and tidy_tab2b in order to produce table1. Would you use left_join or right_join? Give a reason for your answer."
  },
  {
    "objectID": "Tutorial1.html#exercise-2",
    "href": "Tutorial1.html#exercise-2",
    "title": "DS152 Tutorial Sheet 1",
    "section": "Exercise 2",
    "text": "Exercise 2\nIt is thought that the growth of a particular type of insect can be predicted by the temperature at which it is living in. In an experiment, eight insects of the same weight were given living temperatures ranging between 2C and 16C and their weight gain was recorded after three days. The data have been put into the following table.\n\ninsect_id &lt;- c(1, 2,3,4,5,6,7,8)\ntemperature &lt;- c(2 ,4 ,6 ,8 ,10,12,14,16)\nweight_gain &lt;- c(1.21,0.96,1.31,1.52,1.41,1.43,1.87,1.67)\n\ninsect_dat &lt;- tibble(insect_id, temperature, weight_gain)\ninsect_dat\n\n# A tibble: 8 × 3\n  insect_id temperature weight_gain\n      &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1         1           2        1.21\n2         2           4        0.96\n3         3           6        1.31\n4         4           8        1.52\n5         5          10        1.41\n6         6          12        1.43\n7         7          14        1.87\n8         8          16        1.67\n\n\n\nThe correlation between temperature and weight gain is 0.84. What code would you use to do this calculation in R?\nInterpret the correlation result.\nWhat does it mean to say that two variables are negatively correlated?\nWhy should you inspect a scatter plot of the data even though you have information on the correlation between two variables?"
  },
  {
    "objectID": "Assignment1.html",
    "href": "Assignment1.html",
    "title": "DS152 Introduction to Data Science (2)",
    "section": "",
    "text": "Answer all questions for continuous assessment. Due: 4pm on Friday, 28th February 2025."
  },
  {
    "objectID": "Assignment1.html#instructions",
    "href": "Assignment1.html#instructions",
    "title": "DS152 Introduction to Data Science (2)",
    "section": "",
    "text": "Answer all questions for continuous assessment. Due: 4pm on Friday, 28th February 2025."
  },
  {
    "objectID": "Assignment1.html#question",
    "href": "Assignment1.html#question",
    "title": "DS152 Introduction to Data Science (2)",
    "section": "Question",
    "text": "Question\nSynopsis: US Department of Commerce, National Oceanic & Atmospheric Administration provides information on fatalities and injuries associated with natural calamities/disasters across different US states.\nIf you want to view the data in R you can run the following:\n\nnat_disaster_dat &lt;- read_csv(\"https://www.dropbox.com/scl/fi/kepof8k55mzlefy7nzfrj/nat_disaster_dat.csv?rlkey=7nx78bcpnj6ufm0hdq91ke8np&raw=1\")\n\nA glimpse if the data is shown below\n\nglimpse(nat_disaster_dat)\n\nRows: 902,297\nColumns: 4\n$ STATE      &lt;chr&gt; \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\",…\n$ EVTYPE     &lt;chr&gt; \"TORNADO\", \"TORNADO\", \"TORNADO\", \"TORNADO\", \"TORNADO\", \"TOR…\n$ FATALITIES &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 4, 0, 0, 0, 0,…\n$ INJURIES   &lt;dbl&gt; 15, 0, 2, 2, 2, 6, 1, 0, 14, 0, 3, 3, 26, 12, 6, 50, 2, 0, …\n\n\nNote:\n\nSTATE = US State\nEVTYPE = event type\nFATALITIES = # of fatalities\nINJURIES = # if injuries.\n\n\nHow many observations are in the dataset?\nAssume you want to create the summary below, which sums up the number of fatalities within each event type and then arranges by the total fatalities, in descending order.\n\n\n\n# A tibble: 977 × 2\n   EVTYPE         total_fatalities\n   &lt;chr&gt;                     &lt;dbl&gt;\n 1 TORNADO                    5633\n 2 EXCESSIVE HEAT             1903\n 3 FLASH FLOOD                 978\n 4 HEAT                        937\n 5 LIGHTNING                   816\n 6 TSTM WIND                   504\n 7 FLOOD                       470\n 8 RIP CURRENT                 368\n 9 HIGH WIND                   248\n10 AVALANCHE                   224\n# ℹ 967 more rows\n\n\nIn the code below, what should [A], [B], [C] and [D] be replaced with to create this summary?\n\nnat_disaster__summary &lt;- nat_disaster_dat %&gt;% \n                          group_by([A]) %&gt;% \n                          [B](total_fatalities = [C])) %&gt;% \n                          arrange([D](total_fatalities))\n\n[A] =\n[B] =\n[C] =\n[D] =\n\nNow assume you want to filter the summarised data from (ii) so that you have a dataset with only events where the total fatalities are greater than 200, as shown.\n\n\n\n# A tibble: 12 × 2\n   EVTYPE         total_fatalities\n   &lt;chr&gt;                     &lt;dbl&gt;\n 1 TORNADO                    5633\n 2 EXCESSIVE HEAT             1903\n 3 FLASH FLOOD                 978\n 4 HEAT                        937\n 5 LIGHTNING                   816\n 6 TSTM WIND                   504\n 7 FLOOD                       470\n 8 RIP CURRENT                 368\n 9 HIGH WIND                   248\n10 AVALANCHE                   224\n11 WINTER STORM                206\n12 RIP CURRENTS                204\n\n\nIn the code below, what should [A] and [B] be replaced with to create the filtered dataet?\n\nnat_disaster_filter &lt;- nat_disaster_summary %&gt;% \n                        [A]([B])\n\n[A] =\n[B] =\n\nNow assume you want to create the following barplot which shows the total fatalities per event type, using the filtered data from part (iii).\n\n\n\n\n\n\n\n\n\n\nIn the code below, what should [A], [B] and [C] be replaced with to create the plot?\n\nggplot(nat_disaster_filter, aes(x = [A], y = [B])) +\n  [C](stat=\"identity\") +\n  theme(axis.text.x = element_text(angle = 90)) +\n  xlab(\"\") +\n  ylab(\"# fatalities\")\n\n[A] =\n[B] =\n[C] ="
  },
  {
    "objectID": "Tutorial1_soln.html",
    "href": "Tutorial1_soln.html",
    "title": "DS152 Tutorial Sheet 1 with Solutions",
    "section": "",
    "text": "The table below displays the number of tuberculosis cases documented by the World Health Organization in Afghanistan, Brazil, and China between 1999 and 2000. The data contains values associated with four variables (country, year, cases, and population). The tidy format for the data is:\n\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n\nThe data table below organizes the same data in an untidy format.\n\n\nuntidy_tab1\n\n# A tibble: 12 × 4\n   country      year type            count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\n\n\nWhat are the characteristics of this dataset that make it not tidy?\n\nThe dataset is untidy because the observations are spread over multiple rows.\n\nIn the code below, what should [A], [B], [C], [D] and [E] be replaced with to make this dataset tidy?\n\n\ntidy_tab1 &lt;- untidy_tab1 %&gt;% \n                pivot_wider(names_from = type ,\n                            values_from = count)\ntidy_tab1\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n[A] = wider, [B] = names_from, [C] = type, [D] = values_from, [E] = count,\n\nThe data table below, which includes the cases data, is organized in an untidy format.\n\n\nuntidy_tab2a\n\n# A tibble: 3 × 3\n  country     `1999` `2000`\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\n\n\nWhat are the characteristics of this dataset that make it not tidy?\n\nColumn names are values of the year variable.\nThe cases variable is spread over two columns.\n\nIn the code below, what should [A], [B], [C], [D], [E] and [F] be replaced with to make this dataset tidy?\n\n\ntidy_tab2a &lt;- untidy_tab2a %&gt;%  \n                pivot_longer(-country,\n                              names_to = \"year\",\n                              values_to = \"cases\")\ntidy_tab2a\n\n# A tibble: 6 × 3\n  country     year   cases\n  &lt;chr&gt;       &lt;chr&gt;  &lt;dbl&gt;\n1 Afghanistan 1999     745\n2 Afghanistan 2000    2666\n3 Brazil      1999   37737\n4 Brazil      2000   80488\n5 China       1999  212258\n6 China       2000  213766\n\n\n[A] = longer, [B] = country, [C] = names_to, [D] = “year” (note “” around year is important here because you are creating a new variable), [E] = values_to, [F] = “cases” (note as with year, “” around cases is important here)\n\nOnce tidy_tab2a has been created you should make the year variable numeric. Replace [G] in the code below to achieve this.\n\n\ntidy_tab2a &lt;- tidy_tab2a %&gt;% mutate(year = as.numeric(year))\n\n[G] = as.numeric\n\nThe data table below includes the population data organized in a tidy format (note: China has an additional year of data):\n\n\ntidy_tab2b\n\n# A tibble: 7 × 3\n  country      year population\n  &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999   19987071\n2 Afghanistan  2000   20595360\n3 Brazil       1999  172006362\n4 Brazil       2000  174504898\n5 China        1999 1272915272\n6 China        2000 1280428583\n7 China        2009 1339125595\n\n\nAssume you want to join tidy_tab2a and tidy_tab2b in order to produce table1. Would you use left_join or right_join? Give a reason for your answer.\n\nleft_join(tidy_tab2a,tidy_tab2b)\n\nJoining with `by = join_by(country, year)`\n\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\nright_join would include the additional year for China with an NA for the cases data and so would not produce table1."
  },
  {
    "objectID": "Tutorial1_soln.html#exercise-2",
    "href": "Tutorial1_soln.html#exercise-2",
    "title": "DS152 Tutorial Sheet 1 with Solutions",
    "section": "Exercise 2",
    "text": "Exercise 2\nIt is thought that the growth of a particular type of insect can be predicted by the temperature at which it is living in. In an experiment, eight insects of the same weight were given living temperatures ranging between 2C and 16C and their weight gain was recorded after three days. The data have been put into the following table.\n\ninsect_id &lt;- c(1, 2,3,4,5,6,7,8)\ntemperature &lt;- c(2 ,4 ,6 ,8 ,10,12,14,16)\nweight_gain &lt;- c(1.21,0.96,1.31,1.52,1.41,1.43,1.87,1.67)\n\ninsect_dat &lt;- tibble(insect_id, temperature, weight_gain)\ninsect_dat\n\n# A tibble: 8 × 3\n  insect_id temperature weight_gain\n      &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1         1           2        1.21\n2         2           4        0.96\n3         3           6        1.31\n4         4           8        1.52\n5         5          10        1.41\n6         6          12        1.43\n7         7          14        1.87\n8         8          16        1.67\n\n\n\nThe correlation between temperature and weight gain is 0.84. What code would you use to do this calculation in R?\n\n\ncor(insect_dat$temperature, insect_dat$weight_gain)\n\n[1] 0.8397303\n\n\n\nInterpret the correlation result.\n\nThere is a strong positive correlation between temperature and weight gain.\n\nWhat does it mean to say that two variables are negatively correlated?\n\nAs one variable increases in value the other decreases.\n\nWhy should you inspect a scatter plot of the data even though you have information on the correlation between two variables?\n\n\nggplot(insect_dat, aes(x = temperature, y = weight_gain)) +\n  geom_point()\n\n\n\n\n\n\n\n\nInspecting a scatter plot of the data is valuable for several reasons, even if you already know the correlation between two variables:\n1. Visual Confirmation: While correlation coefficients provide a numerical measure of the strength and direction of the relationship between two variables, a scatter plot allows you to visually confirm this relationship. Sometimes, the correlation coefficient might indicate a weak relationship, but upon visual inspection, you might notice a clear pattern or non-linear trend in the data that was not apparent from the correlation alone.\n2. Outlier Detection: Scatter plots help in identifying outliers, which are data points that deviate significantly from the overall pattern of the data. Outliers can have a disproportionate influence on the correlation, potentially skewing the interpretation of the relationship between variables.\n3. Contextual Understanding: Viewing the actual data points on a scatter plot provides a richer understanding of the relationship between variables within the specific context of the dataset. This understanding can be crucial for making informed decisions or drawing meaningful conclusions from the data.\n4. Communication: Scatter plots are often used for communication purposes, as they provide a clear and intuitive way to visualize the relationship between variables. They can be included in reports, presentations, or academic papers to effectively convey insights from the data to a broader audience."
  },
  {
    "objectID": "Tutorial1_soln.html#exercise-1",
    "href": "Tutorial1_soln.html#exercise-1",
    "title": "DS152 Tutorial Sheet 1 with Solutions",
    "section": "",
    "text": "The table below displays the number of tuberculosis cases documented by the World Health Organization in Afghanistan, Brazil, and China between 1999 and 2000. The data contains values associated with four variables (country, year, cases, and population). The tidy format for the data is:\n\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n\nThe data table below organizes the same data in an untidy format.\n\n\nuntidy_tab1\n\n# A tibble: 12 × 4\n   country      year type            count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\n\n\nWhat are the characteristics of this dataset that make it not tidy?\n\nThe dataset is untidy because the observations are spread over multiple rows.\n\nIn the code below, what should [A], [B], [C], [D] and [E] be replaced with to make this dataset tidy?\n\n\ntidy_tab1 &lt;- untidy_tab1 %&gt;% \n                pivot_wider(names_from = type ,\n                            values_from = count)\ntidy_tab1\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n[A] = wider, [B] = names_from, [C] = type, [D] = values_from, [E] = count,\n\nThe data table below, which includes the cases data, is organized in an untidy format.\n\n\nuntidy_tab2a\n\n# A tibble: 3 × 3\n  country     `1999` `2000`\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\n\n\nWhat are the characteristics of this dataset that make it not tidy?\n\nColumn names are values of the year variable.\nThe cases variable is spread over two columns.\n\nIn the code below, what should [A], [B], [C], [D], [E] and [F] be replaced with to make this dataset tidy?\n\n\ntidy_tab2a &lt;- untidy_tab2a %&gt;%  \n                pivot_longer(-country,\n                              names_to = \"year\",\n                              values_to = \"cases\")\ntidy_tab2a\n\n# A tibble: 6 × 3\n  country     year   cases\n  &lt;chr&gt;       &lt;chr&gt;  &lt;dbl&gt;\n1 Afghanistan 1999     745\n2 Afghanistan 2000    2666\n3 Brazil      1999   37737\n4 Brazil      2000   80488\n5 China       1999  212258\n6 China       2000  213766\n\n\n[A] = longer, [B] = country, [C] = names_to, [D] = “year” (note “” around year is important here because you are creating a new variable), [E] = values_to, [F] = “cases” (note as with year, “” around cases is important here)\n\nOnce tidy_tab2a has been created you should make the year variable numeric. Replace [G] in the code below to achieve this.\n\n\ntidy_tab2a &lt;- tidy_tab2a %&gt;% mutate(year = as.numeric(year))\n\n[G] = as.numeric\n\nThe data table below includes the population data organized in a tidy format (note: China has an additional year of data):\n\n\ntidy_tab2b\n\n# A tibble: 7 × 3\n  country      year population\n  &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999   19987071\n2 Afghanistan  2000   20595360\n3 Brazil       1999  172006362\n4 Brazil       2000  174504898\n5 China        1999 1272915272\n6 China        2000 1280428583\n7 China        2009 1339125595\n\n\nAssume you want to join tidy_tab2a and tidy_tab2b in order to produce table1. Would you use left_join or right_join? Give a reason for your answer.\n\nleft_join(tidy_tab2a,tidy_tab2b)\n\nJoining with `by = join_by(country, year)`\n\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\nright_join would include the additional year for China with an NA for the cases data and so would not produce table1."
  },
  {
    "objectID": "index.html#tutorial-sheets",
    "href": "index.html#tutorial-sheets",
    "title": "Welcome to Introduction to Data Science (2)!",
    "section": "Tutorial Sheets",
    "text": "Tutorial Sheets\nTutorial Sheet 1: Tidyverse Recap\nTutorial Sheet 2: Logistic Regression"
  },
  {
    "objectID": "5-experiments.html#randomization",
    "href": "5-experiments.html#randomization",
    "title": "Experimental Studies",
    "section": "Randomization",
    "text": "Randomization\n\nThe key to telling a causal story is the counterfactual: what would have happened in the absence of the treatment.\n\nThis means that establishing the control group is critical because when we do that, we establish the counterfactual.\n\nWhat we hope to be able to do is to find treatment and control groups that are the same, but for the treatment.\n\nWe might be worried about, say, underlying trends, which is one issue with a before-and-after comparison, or selection bias, which could occur when we allow self-selection. Either of these issues could result in biased estimators.\nWe use randomization to go some way to addressing these.\n\n\nTo explore ideas of randomization, we simulate a population, and then randomly sample from it. We will set it up so that 20% of the population are smokers, and the rest are not.\n\nset.seed(853)\n\nnumber_of_people &lt;- 5000\n\npopulation &lt;-\n  tibble(\n    person = c(1:number_of_people),\n    smoking_status = sample(\n      x = c(\"Smoker\", \"Non-Smoker\"),\n      size  = number_of_people,\n      replace = TRUE,\n      prob = c(0.2,0.8)\n    )\n  )\n\nLet’s look at population characteristics\n\npopulation %&gt;% \n  count(smoking_status)\n\n# A tibble: 2 × 2\n  smoking_status     n\n  &lt;chr&gt;          &lt;int&gt;\n1 Non-Smoker      4003\n2 Smoker           997\n\n\nNow let’s sample from the population and randomly assign a treatment and control group.\n\nset.seed(853)\n\nsample_size &lt;- 1000\n\nsample &lt;-\n  population %&gt;% \n  sample_n(sample_size) %&gt;% \n  mutate(group = sample(\n    x = c(\"Treatment\", \"Control\"),\n    size  = sample_size,\n    replace = TRUE\n  ))\n\nNow let’s look at sample characteristics within each group (treatment and control). Is the distribution of population characteristics reflected within each group?\n\nsample %&gt;% \n  count(group,  smoking_status) %&gt;% \n  group_by(group) %&gt;% \n  mutate(prop = n / sum(n)) \n\n# A tibble: 4 × 4\n# Groups:   group [2]\n  group     smoking_status     n  prop\n  &lt;chr&gt;     &lt;chr&gt;          &lt;int&gt; &lt;dbl&gt;\n1 Control   Non-Smoker       364 0.747\n2 Control   Smoker           123 0.253\n3 Treatment Non-Smoker       415 0.809\n4 Treatment Smoker            98 0.191\n\n\n\nInternal validity\n\nIf the treated and control groups are the same in all ways and remain that way, but for the treatment, then we have internal validity, which is to say that our control will work as a counterfactual and our results can speak to a difference between the groups in that study.\nInternal validity means that our estimates of the effect of the treatment are speaking to the treatment and not some other aspect.\nThey mean that we can use our results to make claims about what happened in the experiment.\n\n\n\nExternal validity\n\nIf the group to which we applied our randomization were representative of the broader population, and the experimental set-up were fairly similar to outside conditions, then we further could have external validity.\n\nThat would mean that the difference we find does not just apply in our own experiment, but also in the broader population.\nExternal validity means that we can use our experiment to make claims about what would happen outside the experiment.\nIt is randomization that has allowed that to happen.\n\nBut this means we need randomization twice. Firstly, into the group that was subject to the experiment, and then secondly, between treatment and control."
  },
  {
    "objectID": "5-experiments.html#blocking",
    "href": "5-experiments.html#blocking",
    "title": "Experimental Studies",
    "section": "Blocking",
    "text": "Blocking\n\nOften there are covariates in the experimental units that are known to affect the response variable and must be taken into account.\nIdeally an experimenter can group the experimental units into blocks where the within block variance is small, but the block to block variability is large.\n\nFor example, in testing a drug to prevent heart disease, we know that gender and age also impact the outcome. We may want to partition our study participants into gender and age groups and then randomly assign the treatment (placebo vs drug) within the group.\n\nOften blocking variables are not the variables that we are primarily interested in, but must nevertheless be considered. We call these nuisance variables.\n\n\nBlocking Example\nExample 1. An agricultural field study has three fields in which the researchers will evaluate the quality of three different varieties of barley. Due to how they harvest the barley, we can only create a maximum of three plots in each field. In this example we will block on field since there might be differences in soil type, drainage, etc from field to field. In each field, we will plant all three varieties so that we can tell the difference between varieties without the block effect of field confounding our inference. In this example, the varieties are nested within the fields.\n\n\n# A tibble: 9 × 4\n  Field  Plot Variety Quality\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;  \n1     1     1 A       value  \n2     1     2 B       .      \n3     1     3 C       .      \n4     2     1 A       .      \n5     2     2 B       .      \n6     2     3 C       .      \n7     3     1 A       .      \n8     3     2 B       .      \n9     3     3 C       .      \n\n\n\nMain types of blocks\n\nNatural divisions\n\nYoung animals – litters, egg masses\nPeople or animals – gender\nContinuous gradient of change\n\nPlots on the field – declivity, humidity, fertility\nPeople or animals – age, weight, size\nSeverity of disease on the field\n\nExperimental management\n\nLaboratory procedures – technician, day, stand\nMaterial availability\n\n\n\n\n\n\nRandomized Complete Block Design\n\nThe aim is to have heterogeneity between blocks and homogeneity within\n\ne.g. a greenhouse with a temperature gradient, different days, different observers\nBlock is the replicate\nThe number of experimental units per block is equal to the number of treatments and every treatment occurs once in each block, the order of the treatments within a block being randomized\n\n\n\n\n\n\n\nLaboratory chamber with a humidity gradient inside\n\nLaboratory chambers with homogeneous temperature and humidity inside\n\nA field with a fertility gradient\n\nA greenhouse with a temperature gradient during the day\n\nThe dataset oatvar in the faraway library contains information about an experiment on eight different varieties of oats.\n\nThe area in which the experiment was done had some systematic variability and the researchers divided the area up into five different blocks in which they felt the area inside a block was uniform while acknowledging that some blocks are likely superior to others for growing crops.\nWithin each block, the researchers created eight plots and randomly assigned a variety to a plot. This type of design is called a Randomized Complete Block Design (RCBD) because each block contains all possible levels of the factor of primary interest.\n\n\n\nRandomized Complete Block Design - Example\n\ndata('oatvar', package='faraway')\nggplot(oatvar, aes(y=yield, x=block, color=variety)) + \n    geom_point(size=5) +\n    geom_line(aes(x=as.integer(block)))"
  },
  {
    "objectID": "5-experiments.html#factorial-experiments",
    "href": "5-experiments.html#factorial-experiments",
    "title": "Experimental Studies",
    "section": "Factorial Experiments",
    "text": "Factorial Experiments\nExperiments that involve more than one treatment factor are called factorial experiments. In general, the number of treatments in a factorial experiment is the product of the numbers of levels of the treatment factors. The disadvantage of this is that the number of treatments increase very quickly.\nExample: Pest control\n\nFactor 1: two pesticides (A and B)\nFactor 2: two doses\nThe experiment has a total of \\(2\\times 2 = 4\\) treatments\n\nExample: The Tooth Growth data\n\nFactor 1: two deliveries of vitamin C (orange juice, ascorbic acid)\nFactor 2: three doses (0.5, 1, and 2 mg/day)\nThe experiment has a total of \\(2\\times 3 = 6\\) treatments\n\n\n\n\n\nDesigns\nFor the Pesticide example, suppose 3 replicates and a completely randomized design\n\nFor the Pesticide example, suppose 3 replicates and a randomized complete block design\n\n\n\nInteractions\nThe major advantage of factorial experiments is that they allow for the detection of interactions.\n\nTwo factors are said to interact if the effect of one, on the response variable, depends upon the level of the other.\nIf they do not interact, they are said to be independent.\n\n\nInteraction Plots\nA set of parallel lines indicates no interaction\n\nThe crossing over of lines indicates an interaction"
  },
  {
    "objectID": "5-experiments.html#principles-of-experimental-design",
    "href": "5-experiments.html#principles-of-experimental-design",
    "title": "Experimental Studies",
    "section": "Principles of Experimental Design",
    "text": "Principles of Experimental Design\nControlling. Researchers assign treatments to cases, and they do their best to control any other differences in the groups.\nRandomization. Researchers randomize patients into treatment groups to account for variables that cannot be controlled. For example, some patients may be more susceptible to a disease than others due to their dietary habits. In this example dietary habit is a confounding variable, which is defined as a variable that is associated with both the explanatory and response variables. Randomizing patients into the treatment or control group helps even out such differences.\n\nConfounding variables\nConfounding variables: Extraneous variables that affect both the exposure and the outcome variables, and that make it seem like there is a relationship between them are called confounding variables.\n\nReplication. The more cases researchers observe, the more accurately they can estimate the effect of the explanatory variable on the response. In a single study, we replicate by collecting a sufficiently large sample. What is considered sufficiently large varies from experiment to experiment, but at a minimum we want to have multiple subjects (experimental units) per treatment group.\nBlocking. Researchers sometimes know or suspect that variables, other than the treatment, influence the response. Under these circumstances, they may first group individuals based on this variable into blocks and then randomize cases within each block to the treatment groups. This strategy is often referred to as blocking. For instance, if we are looking at the effect of a drug on heart attacks, we might first split patients in the study into low-risk and high-risk blocks, then randomly assign half the patients from each block to the control group and the other half to the treatment group. This strategy ensures that each treatment group has the same number of low-risk patients and the same number of high-risk patients."
  },
  {
    "objectID": "5-experiments.html#more-on-randomization",
    "href": "5-experiments.html#more-on-randomization",
    "title": "Experimental Studies",
    "section": "More on Randomization",
    "text": "More on Randomization\nThe key to telling a causal story is the counterfactual: what would have happened in the absence of the treatment. This means that establishing the control group is critical because when we do that, we establish the counterfactual. What we hope to be able to do is to find treatment and control groups that are the same, but for the treatment.\n\nWe might be worried about, say, underlying trends, which is one issue with a before-and-after comparison, or selection bias, which could occur when we allow self-selection. Either of these issues could result in biased estimators.\nWe use randomization to go some way to addressing these.\n\nTo explore ideas of randomization, we simulate a population, and then randomly sample from it. We will set it up so that 20% of the population are smokers, and the rest are not.\n\nset.seed(853)\n\nnumber_of_people &lt;- 5000\n\npopulation &lt;-\n  tibble(\n    person = c(1:number_of_people),\n    smoking_status = sample(\n      x = c(\"Smoker\", \"Non-Smoker\"),\n      size  = number_of_people,\n      replace = TRUE,\n      prob = c(0.2,0.8)\n    )\n  )\n\nLet’s look at population characteristics\n\npopulation %&gt;% \n  count(smoking_status)\n\n# A tibble: 2 × 2\n  smoking_status     n\n  &lt;chr&gt;          &lt;int&gt;\n1 Non-Smoker      4003\n2 Smoker           997\n\n\nNow let’s sample from the population and randomly assign a treatment and control group.\n\nset.seed(853)\n\nsample_size &lt;- 1000\n\nsample &lt;-\n  population %&gt;% \n  sample_n(sample_size) %&gt;% \n  mutate(group = sample(\n    x = c(\"Treatment\", \"Control\"),\n    size  = sample_size,\n    replace = TRUE\n  ))\n\nNow let’s look at sample characteristics within each group (treatment and control). Is the distribution of population characteristics reflected within each group?\n\nsample %&gt;% \n  count(group,  smoking_status) %&gt;% \n  group_by(group) %&gt;% \n  mutate(prop = n / sum(n)) \n\n# A tibble: 4 × 4\n# Groups:   group [2]\n  group     smoking_status     n  prop\n  &lt;chr&gt;     &lt;chr&gt;          &lt;int&gt; &lt;dbl&gt;\n1 Control   Non-Smoker       364 0.747\n2 Control   Smoker           123 0.253\n3 Treatment Non-Smoker       415 0.809\n4 Treatment Smoker            98 0.191\n\n\n\n\nInternal Validity\n\nInternal validity is achieved when the only difference between the treatment and control groups is the treatment itself.\n\nThis ensures that the control group serves as a true counterfactual, allowing us to attribute differences in outcomes solely to the treatment.\n\nIn other words, our estimates reflect the true effect of the treatment rather than other confounding factors.\n\nWith strong internal validity, we can confidently make claims about causal relationships within the experiment.\n\n\n\nExternal Validity\n\nExternal validity refers to whether our experimental findings generalize beyond the study.\n\nThis requires:\n\nA sample that represents the broader population.\n\nAn experimental setup that mirrors real-world conditions.\n\n\nIf these conditions hold, our results are applicable outside the experiment.\n\nRandomization is key to achieving external validity:\n\nFirst, random selection ensures that the study group is representative.\n\nSecond, random assignment to treatment and control ensures comparability within the experiment.\n\n\n\n\n\nExample: Tooth Growth\nToothGrowth is a built-in R dataset from a study that examined the effect of Vitamin C dose and delivery on the length of the odontoplasts, the cells responsible for teeth growth, in 60 guinea pigs, where tooth length was the measured outcome variable.\n\nRandomization\nRandomization of subjects in an experiment helps spread any variability that exists naturally between subjects evenly across groups.\nIn the experiment that yielded the ToothGrowth dataset, guinea pigs were randomized to receive Vitamin C either through orange juice or ascorbic acid, indicated in the dataset by the supp variable.\n\ndata(\"ToothGrowth\")\n\n\n\nReplication\nReplication means you need to conduct an experiment with an adequate number of subjects to achieve an acceptable statistical power.\nLet’s examine the ToothGrowth dataset to make sure they followed the principle of replication. We’ll use a dplyr function called count to help us.\n\nToothGrowth %&gt;% count(supp)\n\n  supp  n\n1   OJ 30\n2   VC 30\n\n\n\n\nAnalysis\nSuppose you know from previous studies that the average length of a guinea pigs odontoplasts is 18 micrometers. Therefore, you hypothesize that odontoplast length is equal to 18. To “test” this hypothesis, let’s see how the value of 18 compares to our sample data by looking at the mean and standard deviation.\n\nToothGrowth %&gt;% pull(len) %&gt;% mean\n\n[1] 18.81333\n\n\nIt’s natural to wonder if there is a difference in tooth length by supplement type. Use group_by and summarise to explore this and create an appropriate visualization using ggplot.\n\nToothGrowth %&gt;%"
  },
  {
    "objectID": "5-experiments.html#more-on-blocking",
    "href": "5-experiments.html#more-on-blocking",
    "title": "Experimental Studies",
    "section": "More on Blocking",
    "text": "More on Blocking\nOften there are covariates in the experimental units that are known to affect the response variable and must be taken into account. Ideally an experimenter can group the experimental units into blocks where the within block variance is small, but the block to block variability is large.\n\nFor example, in testing a drug to prevent heart disease, we know that gender and age also impact the outcome. We may want to partition our study participants into gender and age groups and then randomly assign the treatment (placebo vs drug) within the group.\n\nOften blocking variables are not the variables that we are primarily interested in, but must nevertheless be considered.\n\nRandomized Complete Block Design (RCBD)\n\nThe goal of RCBD is to maximize differences between blocks while ensuring similarity within each block.\n\nThis design helps control for known sources of variability, such as:\n\nTemperature differences within a greenhouse.\n\nVariation across different days.\n\nVariation in fertility or drainage differences in a field\n\n\nKey features:\n\nEach block acts as a replicate.\n\nThe number of experimental units per block equals the number of treatments.\n\nEach treatment appears exactly once per block, with the order randomized within the block.\n\n\n\n\n\n\n\n\nExamples of blocks\n\nLaboratory chamber with a humidity gradient inside\n\n\n\nLaboratory chambers with homogeneous temperature and humidity inside\n\n\n\nA field with a fertility gradient\n\n\n\nA greenhouse with a temperature gradient during the day\n\n\n\n\n\nExample: Oatvar\nThe dataset oatvar in the faraway library contains information about an experiment on eight different varieties of oats.\n\nThe area in which the experiment was done had some systematic variability and the researchers divided the area up into five different blocks in which they felt the area inside a block was uniform while acknowledging that some blocks are likely superior to others for growing crops.\nWithin each block, the researchers created eight plots and randomly assigned a variety to a plot. This type of design is called a Randomized Complete Block Design (RCBD) because each block contains all possible levels of the factor of primary interest.\n\nVisualise the data\n\nggplot(oatvar, aes(y=yield, x=block, color=variety)) + \n    geom_point(size=5) +\n    geom_line(aes(x=as.integer(block)))\n\n\n\n\n\n\n\n\nUse group_by and summarise to look at the average yield within each variety.\n\noatvar %&gt;% \n\nLook at the results of using lm with variety and block as predictors.\n\nlm(....)"
  },
  {
    "objectID": "5-experiments.html#factorial-experiments-tooth-growth",
    "href": "5-experiments.html#factorial-experiments-tooth-growth",
    "title": "Experimental Studies",
    "section": "Factorial Experiments: Tooth Growth",
    "text": "Factorial Experiments: Tooth Growth\nWe’ve already seen to Tooth Growth data that examined the effect of Vitamin C delivery and dose on the length of the odontoplasts, the cells responsible for teeth growth, in 60 guinea pigs, where tooth length was the measured outcome variable.\nWe initially looked at how the Vitamin C delivery impacted tooth growth. Now lets consider what happens when we bring dose into the analysis. In this case we have two factors to consider instead of 1.\n\nVitamin C delivery has 2 levels\ndose has 3 levels (0.5, 1, and 2 mg/day)\n\n\ndata(\"ToothGrowth\")"
  },
  {
    "objectID": "4-sampling.html#research-questions",
    "href": "4-sampling.html#research-questions",
    "title": "Sampling Principles and Strategies",
    "section": "Research Question(s)",
    "text": "Research Question(s)\nResearch Question: Over the last 5 years, how many MU Data Science or Statistics graduates have gone on to get a job in a field directly related to their degree.\nPopulation: All DS or Statistics graduates from MU from the last 5 years.\nQ. Can we survey the entire population?\nA. This would likely be very difficult. It is more realistic to assume that we can work with a fraction of the population.\nQ. How can we ensure the sample is an accurate reflection of the population?\nA. Appropriate sampling.\nOnce we have an appropriate sample, most research questions actually break down into 2 parts:\n\nDescriptive Statistics: What relationship can we observe between the variables in the sample?\nInferential Statistics: Supposing we see a relationship in the sample data, how much evidence is provided for a relationship in the population? Does the data provide lots of evidence for a relationship in the population, or could the relationship we see in the sample be due just to chance variation in the sampling process that gave us the data?\n\n\nAnecdotal Evidence\n“I met two students who did a Data Science degree in Maynooth but they are not working as data scientists. The degree must not get you a Data Science job.”\nThere are two problems here. First, the data only represent two cases. Second, and more importantly, it is unclear whether these cases are actually representative of the population. Data collected in this haphazard fashion only provides anecdotal evidence."
  },
  {
    "objectID": "ObservationalStudies.html#observational-study",
    "href": "ObservationalStudies.html#observational-study",
    "title": "Observational Studies",
    "section": "",
    "text": "Observational Study: A study which observes individuals and measures variables, but does not attempt to influence the responses.\n\nAn observational study on individuals from a random sample allows one to generalize conclusions about the sample to the population.\nAn observational study cannot show cause-and-effect relationships because there is the possibility that the response is affected by some variable(s) other than the ones being measured. That is, confounding variables may be present. “It ain’t what you don’t know that gets you into trouble. It’s what you know for sure that just ain’t so.” - Mark Twain\nIn prospective observational studies, investigators choose a sample and collect new data generated from that sample. That is, the investigators “look forward in time.”\nIn retrospective observational studies, investigators “look backwards in time” and use data that have already been collected. Retrospective studies are often criticized for having more confounding and bias compared to prospective studies."
  },
  {
    "objectID": "Tutorial2.html",
    "href": "Tutorial2.html",
    "title": "DS152 Tutorial Sheet 2",
    "section": "",
    "text": "Attempt the questions below before your tutorial in the week beginning 3rd March 2025."
  },
  {
    "objectID": "Tutorial2.html#instructions",
    "href": "Tutorial2.html#instructions",
    "title": "DS152 Tutorial Sheet 2",
    "section": "",
    "text": "Attempt the questions below before your tutorial in the week beginning 3rd March 2025."
  },
  {
    "objectID": "Tutorial2.html#exercise-1",
    "href": "Tutorial2.html#exercise-1",
    "title": "DS152 Tutorial Sheet 2",
    "section": "Exercise 1",
    "text": "Exercise 1\nRun the code below to read a dataset called car_evaluation into R. Look at the data and the variables that you have to work with. More details about the data are given below.\n\nlibrary(tidyverse)\ncar_evaluation &lt;- read_csv(\"https://www.dropbox.com/s/s54upw5jhoe6r4q/car_evaluation.csv?raw=1\")\n\nThere are 1594 cars in this database that are classified as acceptable (acc) or unacceptable (unacc) according to the following attributes:\n\nbuying_price: buying price (low, medium, high, vhigh)\nmaintenance_cost: (“vhigh”, “high”, “med”, “low”)\ndoors: (“2”,“3”,“4”,“5more”)\npersons: capacity in terms of persons to carry (2,4,more)\nlugboot: the size of luggage boot (small, med, big)\nsafety: estimated safety of the car (low, med, high)\n\n(a) Use the code below to create a bar plot that shows the number of observations within each buying_price category. What do you learn from this visualisation?\n\nggplot(car_evaluation, aes(x = buying_price)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\nNow add fill = decision to the aes() of the plot. What do you learn from this visualisation?\nNext facet using the safety variable. What do you learn from this visualisation?\n\n(b) Run the following code which aims to fit a logistic regression model using buying_price and safety as predictor variables for the decision. This code will give you an error. What is causing the error?\n\nfit_logistic &lt;- glm(decision ~ buying_price + safety, family = \"binomial\", data = car_evaluation)\nfit_logistic\n\n(c) Complete the code below to create a variable in the dataset called decision_binary with “acc” as the reference group.\n\ncar_evaluation &lt;- car_evaluation %&gt;%\n  mutate(decision_binary = ??)\n\n\nNow run the logistic regression with decision_binary as the outcome. Does this give you an error? What do you note about the coefficients?\n\n\nfit_logistic &lt;- glm(??)\nfit_logistic\n\n\nOnce you have fit the model, run the following code to obtain the accuracy of the decision classification. (Note: The code is similar to the code you used in Lab 3 but here we use summarise when doing the calculation for percentage_correct). Change to eval = TRUE when you are ready to run this code. Did your model do a good job?\n\n\nthreshold &lt;- 0.5\n\ncar_predict &lt;- car_evaluation %&gt;%\n                   mutate(p_hat = predict(fit_logistic, type = \"response\") %&gt;% round(2),\n                          type_pred = ifelse(p_hat &gt;= threshold, 1,0))\n \ncar_predict %&gt;% summarise(n_correct = sum(type_pred == decision_binary),\n                          n_total = n(),\n                          percentage_correct = n_correct*100/n_total)\n\n\nNow use group_by to breakdown the accuracy results by decision category (Note: because we set up the code using summarise it’s easy to add group_by() in here and get the breakdown within each decision group). Change to eval = TRUE when you are ready to run this code. What do you learn from these results?\n\n\ncar_predict %&gt;% group_by(decision) %&gt;% \n                summarise(n_correct = sum(type_pred == decision_binary),\n                          n_total = n(),\n                          percentage_correct = n_correct*100/n_total)"
  },
  {
    "objectID": "Tutorial2.html#exercise",
    "href": "Tutorial2.html#exercise",
    "title": "DS152 Tutorial Sheet 2",
    "section": "Exercise",
    "text": "Exercise\nRun the code below to read a dataset called car_evaluation into R. Look at the data and the variables that you have to work with. More details about the data are given below.\n\nlibrary(tidyverse)\ncar_evaluation &lt;- read_csv(\"https://www.dropbox.com/s/s54upw5jhoe6r4q/car_evaluation.csv?raw=1\")\n\nThere are 1594 cars in this database that are classified as acceptable (acc) or unacceptable (unacc) according to the following attributes:\n\nbuying_price: buying price (low, medium, high, vhigh)\nmaintenance_cost: (“vhigh”, “high”, “med”, “low”)\ndoors: (“2”,“3”,“4”,“5more”)\npersons: capacity in terms of persons to carry (2,4,more)\nlugboot: the size of luggage boot (small, med, big)\nsafety: estimated safety of the car (low, med, high)\n\n(a) Complete the code below to create a bar plot that shows the number of observations within each buying_price category. What do you learn from this visualisation?\n\nggplot(car_evaluation, aes(??)) +\n  geom_??\n\n\nNow add fill = decision to the aes() of the plot. What do you learn from this visualisation?\nNext facet using the safety variable. What do you learn from this visualisation?\n\n(b) Run the following code which aims to fit a logistic regression model using buying_price and safety as predictor variables for the decision. This code will give you an error. What is causing the error?\n\nfit_logistic &lt;- glm(decision ~ buying_price + safety, \n                    family = \"binomial\", \n                    data = car_evaluation)\nfit_logistic\n\n(c) Complete the code below to create a variable in the dataset called decision_binary with “acc” as the reference group (there’s an example of this in your lecture notes).\n\ncar_evaluation &lt;- car_evaluation %&gt;%\n  mutate(decision_binary = ??)\n\n\nNow complete the code below to run the logistic regression with decision_binary as the outcome. Does this give you an error? What do you note about the coefficients?\n\n\nfit_logistic &lt;- glm(??)\nfit_logistic\n\n\nOnce you have fit the model, run the following code to obtain the accuracy of the decision classification. (Note: The code is similar to the code you used in Lab 3 but here we use summarise when doing the calculation for percentage_correct). Change to eval = TRUE when you are ready to run this code. Did your model do a good job?\n\n\nthreshold &lt;- 0.5\n\ncar_predict &lt;- car_evaluation %&gt;%\n                   mutate(p_hat = predict(fit_logistic, type = \"response\") %&gt;% round(2),\n                          type_pred = ifelse(p_hat &gt;= threshold, 1,0))\n \naccuracy_results &lt;- car_predict %&gt;% summarise(n_correct = sum(type_pred == decision_binary),\n                                              n_total = n(),\n                                              percentage_correct = n_correct*100/n_total)\naccuracy_results\n\n\nNow modify the summary code above to add a group_by function that will breakdown the accuracy results by decision category. Change to eval = TRUE when you are ready to run this code. What do you learn from these results?\n\n\naccuracy_results &lt;- car_predict %&gt;%"
  },
  {
    "objectID": "Tutorial2_soln.html",
    "href": "Tutorial2_soln.html",
    "title": "DS152 Tutorial Sheet 2 with Solutions",
    "section": "",
    "text": "Run the code below to read a dataset called car_evaluation into R.\n\nlibrary(tidyverse)\ncar_evaluation &lt;- read_csv(\"https://www.dropbox.com/s/s54upw5jhoe6r4q/car_evaluation.csv?raw=1\")\n\nThere are 1594 cars in this database that are classified as acceptable (acc) or unacceptable (unacc) according to the following attributes:\n\nbuying_price: buying price (low, medium, high, vhigh)\nmaintenance_cost: (“vhigh”, “high”, “med”, “low”)\ndoors: (“2”,“3”,“4”,“5more”)\npersons: capacity in terms of persons to carry (2,4,more)\nlugboot: the size of luggage boot (small, med, big)\nsafety: estimated safety of the car (low, med, high)\n\n(a) Complete the code below to create a bar plot that shows the number of observations within each buying_price category. What do you learn from this visualisation?\n\nggplot(car_evaluation, aes(x = buying_price)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nNote for Tutors: Key thing to mention here is that this plot clearly shows that there’s differences in counts within each buying price category.\n\nNow add fill = decision to the aes() of the plot. What do you learn from this visualisation?\n\n\nggplot(car_evaluation, aes(x = buying_price, fill = decision)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nNote for Tutors: Key thing to mention here is that the majority of the decisions are “unacceptable” but the proportion of “acceptable” seems to be lowest in the very high buying category. So, perhaps the buying price can be of some use as a predictor of the decision.\n\nNow facet using the safety variable. What do you learn from this visualisation?\n\n\nggplot(car_evaluation, aes(x = buying_price, fill = decision)) +\n  geom_bar() +\n  facet_wrap(~safety)\n\n\n\n\n\n\n\n\nNote for Tutors: Key things to mention here is that low safety results in a decision of “unacceptable”. Also having “high” safety seems to increase the proportion of “acceptable” in the “high” and “very high” buying categories, relative to medium safety. So perhaps both variables together would be useful as predictors of the decision.\n(b) Run the following code which aims to fit a logistic regression model using buying_price and safety as predictor variables for the decision. This code will give you an error. What is causing the error?\n\nfit_logistic &lt;- glm(decision ~ buying_price + safety, family = \"binomial\", data = car_evaluation)\nfit_logistic\n\nNote for Tutors: The problem was not having the decision variable as binary.\n(c) Complete the code below to create a variable in the dataset called decision_binary with “acc” as the reference group (there’s an example of this in your lecture notes).\n\ncar_evaluation &lt;- car_evaluation %&gt;%\n  mutate(decision_binary = as.numeric(decision == \"acc\"))\n\nNote for Tutors: This means that “acc” will be 1 and “unacc” will be 0.\n\nNow complete the code below to run the logistic regression with decision_binary as the outcome. Does this give you an error? What do you note about the coefficients?\n\n\nfit_logistic &lt;- glm(decision_binary ~ buying_price + safety, family = \"binomial\", data = car_evaluation)\nfit_logistic\n\n\nCall:  glm(formula = decision_binary ~ buying_price + safety, family = \"binomial\", \n    data = car_evaluation)\n\nCoefficients:\n      (Intercept)    buying_pricelow    buying_pricemed  buying_pricevhigh  \n          -0.2998             0.3018             0.4615            -0.5937  \n        safetylow          safetymed  \n         -19.3525            -0.4338  \n\nDegrees of Freedom: 1593 Total (i.e. Null);  1588 Residual\nNull Deviance:      1760 \nResidual Deviance: 1304     AIC: 1316\n\n\nNote for Tutors: Just give a broad overview of results here (i.e., don’t get into exponential transformations etc). Reminder that “acc” has been chosen is the reference group for the outcome. Mention that all the predictors are categorical and so a “reference” category is also chosen for each predictor. For the buying_price the reference group is “high”. So based on the output, moving from “high” buying price to the “low” buying price will increase the chance (technically speaking the log odds but don’t get wrapped up in that) off an “acceptable” decision (although the coefficient is small). Whereas, moving from the high buying price to the very high buy price will decrease the chance of an “acceptable” decision (but again the coefficient is small). Similarly, the reference category for the safety predictor is “high”. Results show that moving from “high” safety to “low” safety decreases the chance of an “acceptable” decision (large coefficient here). Moving from high to medium does not have as big of an impact (small coefficient).\n\nOnce you have fit the model, run the following code to obtain the accuracy of the decision classification. (Note: The code is similar to the code you used in Lab 3 but here we use summarise when doing the calculation for percentage_correct). Change to eval = TRUE when you are ready to run this code. Did your model do a good job?\n\n\nthreshold &lt;- 0.5\n\ncar_predict &lt;- car_evaluation %&gt;%\n                   mutate(p_hat = predict(fit_logistic, type = \"response\") %&gt;% round(2),\n                          type_pred = ifelse(p_hat &gt;= threshold, 1,0))\n \naccuracy_results &lt;- car_predict %&gt;% summarise(n_correct = sum(type_pred == decision_binary),\n                                              n_total = n(),\n                                              percentage_correct = n_correct*100/n_total)\naccuracy_results\n\n# A tibble: 1 × 3\n  n_correct n_total percentage_correct\n      &lt;int&gt;   &lt;int&gt;              &lt;dbl&gt;\n1      1195    1594               75.0\n\n\nNote for Tutors: Just talk through what summarise is doing here and note the accuracy.\n\nNow modify the summary code above to add a group_by function that will breakdown the accuracy results by decision category. Change to eval = TRUE when you are ready to run this code. What do you learn from these results?\n\n\naccuracy_results &lt;-car_predict %&gt;% \n                      group_by(decision) %&gt;% \n                      summarise(n_correct = sum(type_pred ==decision_binary),\n                                n_total = n(),\n                                percentage_correct = n_correct*100/n_total)\naccuracy_results\n\n# A tibble: 2 × 4\n  decision n_correct n_total percentage_correct\n  &lt;chr&gt;        &lt;int&gt;   &lt;int&gt;              &lt;dbl&gt;\n1 acc             89     384               23.2\n2 unacc         1106    1210               91.4\n\n\nNote for Tutors: Just talk through what the addition of group_by achieves and then explain that the accuracy is much better in the “unacceptable” group. Note to students that this happens because of the imbalance in the dataset (much higher proportion of “unacceptable” in the dataset) which was noted in one of the plots earlier."
  },
  {
    "objectID": "Tutorial2_soln.html#exercise",
    "href": "Tutorial2_soln.html#exercise",
    "title": "DS152 Tutorial Sheet 2 with Solutions",
    "section": "",
    "text": "Run the code below to read a dataset called car_evaluation into R.\n\nlibrary(tidyverse)\ncar_evaluation &lt;- read_csv(\"https://www.dropbox.com/s/s54upw5jhoe6r4q/car_evaluation.csv?raw=1\")\n\nThere are 1594 cars in this database that are classified as acceptable (acc) or unacceptable (unacc) according to the following attributes:\n\nbuying_price: buying price (low, medium, high, vhigh)\nmaintenance_cost: (“vhigh”, “high”, “med”, “low”)\ndoors: (“2”,“3”,“4”,“5more”)\npersons: capacity in terms of persons to carry (2,4,more)\nlugboot: the size of luggage boot (small, med, big)\nsafety: estimated safety of the car (low, med, high)\n\n(a) Complete the code below to create a bar plot that shows the number of observations within each buying_price category. What do you learn from this visualisation?\n\nggplot(car_evaluation, aes(x = buying_price)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nNote for Tutors: Key thing to mention here is that this plot clearly shows that there’s differences in counts within each buying price category.\n\nNow add fill = decision to the aes() of the plot. What do you learn from this visualisation?\n\n\nggplot(car_evaluation, aes(x = buying_price, fill = decision)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nNote for Tutors: Key thing to mention here is that the majority of the decisions are “unacceptable” but the proportion of “acceptable” seems to be lowest in the very high buying category. So, perhaps the buying price can be of some use as a predictor of the decision.\n\nNow facet using the safety variable. What do you learn from this visualisation?\n\n\nggplot(car_evaluation, aes(x = buying_price, fill = decision)) +\n  geom_bar() +\n  facet_wrap(~safety)\n\n\n\n\n\n\n\n\nNote for Tutors: Key things to mention here is that low safety results in a decision of “unacceptable”. Also having “high” safety seems to increase the proportion of “acceptable” in the “high” and “very high” buying categories, relative to medium safety. So perhaps both variables together would be useful as predictors of the decision.\n(b) Run the following code which aims to fit a logistic regression model using buying_price and safety as predictor variables for the decision. This code will give you an error. What is causing the error?\n\nfit_logistic &lt;- glm(decision ~ buying_price + safety, family = \"binomial\", data = car_evaluation)\nfit_logistic\n\nNote for Tutors: The problem was not having the decision variable as binary.\n(c) Complete the code below to create a variable in the dataset called decision_binary with “acc” as the reference group (there’s an example of this in your lecture notes).\n\ncar_evaluation &lt;- car_evaluation %&gt;%\n  mutate(decision_binary = as.numeric(decision == \"acc\"))\n\nNote for Tutors: This means that “acc” will be 1 and “unacc” will be 0.\n\nNow complete the code below to run the logistic regression with decision_binary as the outcome. Does this give you an error? What do you note about the coefficients?\n\n\nfit_logistic &lt;- glm(decision_binary ~ buying_price + safety, family = \"binomial\", data = car_evaluation)\nfit_logistic\n\n\nCall:  glm(formula = decision_binary ~ buying_price + safety, family = \"binomial\", \n    data = car_evaluation)\n\nCoefficients:\n      (Intercept)    buying_pricelow    buying_pricemed  buying_pricevhigh  \n          -0.2998             0.3018             0.4615            -0.5937  \n        safetylow          safetymed  \n         -19.3525            -0.4338  \n\nDegrees of Freedom: 1593 Total (i.e. Null);  1588 Residual\nNull Deviance:      1760 \nResidual Deviance: 1304     AIC: 1316\n\n\nNote for Tutors: Just give a broad overview of results here (i.e., don’t get into exponential transformations etc). Reminder that “acc” has been chosen is the reference group for the outcome. Mention that all the predictors are categorical and so a “reference” category is also chosen for each predictor. For the buying_price the reference group is “high”. So based on the output, moving from “high” buying price to the “low” buying price will increase the chance (technically speaking the log odds but don’t get wrapped up in that) off an “acceptable” decision (although the coefficient is small). Whereas, moving from the high buying price to the very high buy price will decrease the chance of an “acceptable” decision (but again the coefficient is small). Similarly, the reference category for the safety predictor is “high”. Results show that moving from “high” safety to “low” safety decreases the chance of an “acceptable” decision (large coefficient here). Moving from high to medium does not have as big of an impact (small coefficient).\n\nOnce you have fit the model, run the following code to obtain the accuracy of the decision classification. (Note: The code is similar to the code you used in Lab 3 but here we use summarise when doing the calculation for percentage_correct). Change to eval = TRUE when you are ready to run this code. Did your model do a good job?\n\n\nthreshold &lt;- 0.5\n\ncar_predict &lt;- car_evaluation %&gt;%\n                   mutate(p_hat = predict(fit_logistic, type = \"response\") %&gt;% round(2),\n                          type_pred = ifelse(p_hat &gt;= threshold, 1,0))\n \naccuracy_results &lt;- car_predict %&gt;% summarise(n_correct = sum(type_pred == decision_binary),\n                                              n_total = n(),\n                                              percentage_correct = n_correct*100/n_total)\naccuracy_results\n\n# A tibble: 1 × 3\n  n_correct n_total percentage_correct\n      &lt;int&gt;   &lt;int&gt;              &lt;dbl&gt;\n1      1195    1594               75.0\n\n\nNote for Tutors: Just talk through what summarise is doing here and note the accuracy.\n\nNow modify the summary code above to add a group_by function that will breakdown the accuracy results by decision category. Change to eval = TRUE when you are ready to run this code. What do you learn from these results?\n\n\naccuracy_results &lt;-car_predict %&gt;% \n                      group_by(decision) %&gt;% \n                      summarise(n_correct = sum(type_pred ==decision_binary),\n                                n_total = n(),\n                                percentage_correct = n_correct*100/n_total)\naccuracy_results\n\n# A tibble: 2 × 4\n  decision n_correct n_total percentage_correct\n  &lt;chr&gt;        &lt;int&gt;   &lt;int&gt;              &lt;dbl&gt;\n1 acc             89     384               23.2\n2 unacc         1106    1210               91.4\n\n\nNote for Tutors: Just talk through what the addition of group_by achieves and then explain that the accuracy is much better in the “unacceptable” group. Note to students that this happens because of the imbalance in the dataset (much higher proportion of “unacceptable” in the dataset) which was noted in one of the plots earlier."
  },
  {
    "objectID": "ObservationalStudies.html#example-1-student-happiness",
    "href": "ObservationalStudies.html#example-1-student-happiness",
    "title": "Observational Studies",
    "section": "Example 1: Student Happiness",
    "text": "Example 1: Student Happiness\nThe gcfeeling data contains results of a survey conducted by Georgetown College students on 47 Georgetown College upper-class students in relation to feelings about Georgetown College.\n\n# Load necessary packages\nlibrary(tidyverse)\nlibrary(tigerstats)\n\n# Load the dataset\ndata(\"gcfeeling\")\n\n# View basic structure\nglimpse(gcfeeling)\n\nRows: 47\nColumns: 6\n$ rating.fresh &lt;int&gt; 9, 8, 5, 5, 8, 7, 9, 9, 7, 9, 8, 10, 8, 8, 9, 9, 6, 10, 1…\n$ rating.js    &lt;int&gt; 7, 10, 8, 8, 10, 9, 9, 8, 8, 8, 8, 10, 8, 10, 9, 7, 7, 10…\n$ greek        &lt;fct&gt; y, n, y, n, n, y, y, y, y, y, n, n, n, n, n, n, n, y, n, …\n$ athlete      &lt;fct&gt; n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, y, y, …\n$ rating.diff  &lt;int&gt; -2, 2, 3, 3, 2, 2, 0, -1, 1, -1, 0, 0, 0, 2, 0, -2, 1, 0,…\n$ happier      &lt;fct&gt; NO, YES, YES, YES, YES, YES, YES, NO, YES, NO, YES, YES, …\n\n\n\n1. Exploring Categorical Variables\ngcfeeling contains categorical variables. We can visualize their distributions:\n\n# Count and visualize categorical variables\ngcfeeling %&gt;%\n  select_if(is.factor) %&gt;% \n  pivot_longer(everything(),names_to = \"Variable\", values_to = \"Value\") %&gt;%\n  ggplot(aes(x = Value)) +\n  geom_bar() +\n  facet_wrap(~ Variable, scales = \"free\") +\n  theme_minimal() +\n  coord_flip()\n\n\n\n\n\n\n\n\n\n\n2. Exploring Numeric Variables\ngcfeeling contains numeric variables. We can visualize their distributions:\n\n# Check numeric variable distributions\ngcfeeling %&gt;%\n  select_if(is.numeric) %&gt;%\n  pivot_longer(everything(), names_to = \"Variable\", values_to = \"Value\") %&gt;%\n  ggplot(aes(x = Variable,y = Value)) +\n  geom_boxplot() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n3. Relationships Between Variables\n\nContingency Table (For Categorical Variables)\nA contingency table shows the relationship between two categorical variables by displaying their frequency distribution.\n\ngcfeeling %&gt;%\n  select(happier, athlete) %&gt;% \n  table()\n\n       athlete\nhappier  n  y\n    NO  10  1\n    YES 25 11\n\n\n\n\nPairwise Correlation (For Numeric Variables)\n\n# Compute correlation matrix\ncor(gcfeeling$rating.fresh, gcfeeling$rating.js)\n\n[1] 0.402741\n\n\n\n\nScatterplots (For Numeric Variables)\n\nggplot(gcfeeling, aes(x = rating.fresh, y = rating.js)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\nBoxplots of Numerical Variables by Categories\n\n# Boxplots grouped by a categorical variable (assuming `groupvar` is categorical)\ngcfeeling %&gt;%\n  ggplot(aes(x = athlete, y = rating.js)) +\n  geom_boxplot() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n4. Modelling\nWe can use multiple regression to model student happiness.\n\nlm(rating.js ~ rating.fresh + athlete + greek, data = gcfeeling)\n\n\nCall:\nlm(formula = rating.js ~ rating.fresh + athlete + greek, data = gcfeeling)\n\nCoefficients:\n (Intercept)  rating.fresh      athletey        greeky  \n      5.0287        0.3277        0.7594        0.8272"
  },
  {
    "objectID": "ObservationalStudies.html#example-2-does-working-out-increase-energy-levels",
    "href": "ObservationalStudies.html#example-2-does-working-out-increase-energy-levels",
    "title": "Observational Studies",
    "section": "Example 2: Does working out increase energy levels?",
    "text": "Example 2: Does working out increase energy levels?\nWe want to evaluate if regularly working out has any impact on energy levels.\n\nIn an observational study, we sample two types of people from the population, those who choose to work out regularly and those who don’t.\nWe ask the people in each group to rate their energy levels from 1-10.\nThen, we find the average “energy level” for the two groups of people and compare.\n\n\nCan we conclude from this that working out is the cause of increased energy levels?\n\nThere may be other variables that we didn’t control for in this study that contribute to the observed difference.\nFor example, people who have young children might have less time to work out and also have lower energy levels.\nThis is known as confounding.\nThis study allows us to make correlation statements. But, we cannot make a causal statement attributing increased energy levels to working out!\n\n\nConfounding variables\nConfounding variables: Extraneous variables that affect both the exposure (e.g., working out) and the outcome variables (e.g., increased energy), and that make it seem like there is a relationship between them are called confounding variables."
  },
  {
    "objectID": "ObservationalStudies.html#example-3",
    "href": "ObservationalStudies.html#example-3",
    "title": "Observational Studies",
    "section": "Example 3",
    "text": "Example 3\nMany years ago, investigators reported an association between coffee drinking and pancreatic cancer in an observational study (MacMahon B, Yen S, Trichopoulos D, Warren K, Nardi G. Coffee and cancer of the pancreas. N Eng J Med 1981; 304: 630-3).\n\nIf we take coffee as our exposure of interest and correlate it with an increased development of pancreatic cancer there is the potential, as was the case with these investigators, to be misled if there is a third causal factor, such as cigarette smoking, that was more common among those who reported drinking coffee.\n\nOnce the confounding variable, smoking is taken into account the correlation between coffee and pancreatic cancer disappears.\n\nReducing confounding: Matching\nMatching is a technique that involves selecting study participants with similar characteristics outside the outcome or exposure variables.\n\nRather than using random assignment to equalize the experimental groups, the experimenters do it by matching observable characteristics.\nFor every participant in the exposed group, the researchers find a participant with comparable traits to include in the control group.\nMatching subjects facilitates valid comparisons between those groups.\nThe researchers use subject-area knowledge to identify characteristics that are critical to match.\n\n\n\nReducing confounding: Multiple Regression\nMultiple regression models specify the way in which different characteristics/variables (exposure and confounders) affects the outcome, thereby isolating the effect of each variable.\nchance of cancer = a x (coffee) + b x (smoking) + c x (gender) + d x (age)\n\nthis allows us to make a statement about what would happen if one variable (i.e., the exposure) were to change while all the others (i.e., the confounders) remained the same.\nObtaining isolated exposure effects conditional on the other variables remaining constant is said to adjust for (or control for) the effect of these confounders"
  },
  {
    "objectID": "ObservationalStudies.html#example-4",
    "href": "ObservationalStudies.html#example-4",
    "title": "Observational Studies",
    "section": "Example 4",
    "text": "Example 4\nYou and your friend are trying to find the perfect restaurant for dinner. You can’t decide so you want to instead rely on some observational data (i.e., restaurant reviews).\nYou find two worthy restaurants, Carla’s and Sophia’s each with 400 reviews and an indicator of whether the restaurant is recommended or not recommended.\nYou find that\n\nrecommended for Sophia’s = 250/400\nrecommended for Carla’s = 216/400\n\nSo what we have is a conditional probability:\n\np(recommended|Sophia’s) = 62.5%\np(recommended|Carla’s) = 54%\n\n\nWhat if we consider age as a factor here?\n\nrecommended for 18-35 yr old diners at Sophia’s = 50/150\nrecommended for 35+ diners at Sophia’s = 200/250\nrecommended for 18-35 yr old diners at Carla’s = 180/360\nrecommended for 35+ diners at Carla’s = 36/40\n\nSo what we have is:\n\np(recommended|Sophia’s, younger) = 30%\np(recommended|Sophia’s, older) = 80%\np(recommended|Carla’s, younger) = 50%\np(recommended|Carla’s, older) = 90%"
  },
  {
    "objectID": "ObservationalStudies.html#example-5",
    "href": "ObservationalStudies.html#example-5",
    "title": "Observational Studies",
    "section": "Example 5",
    "text": "Example 5\nSay we have data on the number of hours of exercise per week versus the risk of developing a disease for two sets of patients, those below the age of 50 and those over the age of 50. Here are individual plots showing the relationship between exercise and risk of disease.\n\n\n\n\n\n\n\n\n\nWe clearly see a negative correlation, indicating that increased levels of exercise per week are correlated with a lower risk of developing the disease for both groups. Now, let’s combine the data together on a single plot:\n\n\n\n\n\n\n\n\n\n❌ Surprise! The overall correlation is positive: More exercise → Higher risk.\n\nExplanation of the Paradox:\nWithin each age group: More exercise is associated with lower risk (negative correlation).\nAcross both groups: Older individuals exercise more…\n\n\n\n\n\n\n\n\n\n…but have higher baseline risk due to age.\n\n\n\n\n\n\n\n\n\nWhen aggregated, the higher risk of the older group skews the data, making it seem like more exercise increases risk. This is Simpson’s Paradox — where a trend appears in subgroups but reverses when the data is combined!"
  },
  {
    "objectID": "ObservationalStudies.html#example-3-pancreatic-cancer-study",
    "href": "ObservationalStudies.html#example-3-pancreatic-cancer-study",
    "title": "Observational Studies",
    "section": "Example 3: Pancreatic Cancer Study",
    "text": "Example 3: Pancreatic Cancer Study\nMany years ago, investigators reported an association between coffee drinking and pancreatic cancer in an observational study (MacMahon B, Yen S, Trichopoulos D, Warren K, Nardi G. Coffee and cancer of the pancreas. N Eng J Med 1981; 304: 630-3).\n\nIf we take coffee as our exposure of interest and correlate it with an increased development of pancreatic cancer there is the potential, as was the case with these investigators, to be misled if there is a third causal factor, such as cigarette smoking, that was more common among those who reported drinking coffee.\n\nOnce the confounding variable, smoking is taken into account the correlation between coffee and pancreatic cancer disappears.\n\nReducing confounding: Matching\nMatching is a technique that involves selecting study participants with similar characteristics outside the outcome or exposure variables.\n\nRather than using random assignment to equalize the experimental groups, the experimenters do it by matching observable characteristics.\nFor every participant in the exposed group, the researchers find a participant with comparable traits to include in the control group.\nMatching subjects facilitates valid comparisons between those groups.\nThe researchers use subject-area knowledge to identify characteristics that are critical to match.\n\n\n\nReducing confounding: Multiple Regression\nMultiple regression models specify the way in which different characteristics/variables (exposure and confounders) affects the outcome, thereby isolating the effect of each variable.\nchance of cancer = a x (coffee) + b x (smoking) + c x (gender) + d x (age)\n\nthis allows us to make a statement about what would happen if one variable (i.e., the exposure) were to change while all the others (i.e., the confounders) remained the same.\nObtaining isolated exposure effects conditional on the other variables remaining constant is said to adjust for (or control for) the effect of these confounders"
  },
  {
    "objectID": "ObservationalStudies.html#example-4-restaurant-reviews",
    "href": "ObservationalStudies.html#example-4-restaurant-reviews",
    "title": "Observational Studies",
    "section": "Example 4: Restaurant Reviews",
    "text": "Example 4: Restaurant Reviews\nYou and your friend are trying to find the perfect restaurant for dinner. You can’t decide so you want to instead rely on some observational data (i.e., restaurant reviews).\nYou find two worthy restaurants, Carla’s and Sophia’s each with 400 reviews and an indicator of whether the restaurant is recommended or not recommended.\nYou find that\n\nrecommended for Sophia’s = 250/400\nrecommended for Carla’s = 216/400\n\nSo what we have is a conditional probability:\n\np(recommended|Sophia’s) = 62.5%\np(recommended|Carla’s) = 54%\n\n\nWhat if we consider age as a factor here?\n\nrecommended for 18-35 yr old diners at Sophia’s = 50/150\nrecommended for 35+ diners at Sophia’s = 200/250\nrecommended for 18-35 yr old diners at Carla’s = 180/360\nrecommended for 35+ diners at Carla’s = 36/40\n\nSo what we have is:\n\np(recommended|Sophia’s, younger) = 30%\np(recommended|Sophia’s, older) = 80%\np(recommended|Carla’s, younger) = 50%\np(recommended|Carla’s, older) = 90%"
  },
  {
    "objectID": "ObservationalStudies.html#example-5-exercise-and-disease",
    "href": "ObservationalStudies.html#example-5-exercise-and-disease",
    "title": "Observational Studies",
    "section": "Example 5: Exercise and Disease",
    "text": "Example 5: Exercise and Disease\nSay we have (made up) data on the number of hours of exercise per week versus the risk of developing a disease for two sets of patients, those below the age of 50 and those over the age of 50. Here are individual plots showing the relationship between exercise and risk of disease.\n\n\n\n\n\n\n\n\n\nWe clearly see a negative correlation, indicating that increased levels of exercise per week are correlated with a lower risk of developing the disease for both groups. Now, let’s combine the data together on a single plot:\n\n\n\n\n\n\n\n\n\n❌ Surprise! The overall correlation is positive: More exercise → Higher risk.\n\nExplanation of the Paradox:\nWithin each age group: More exercise is associated with lower risk (negative correlation).\nAcross both groups: Older individuals exercise more…\n\n\n\n\n\n\n\n\n\n…but have higher baseline risk due to age.\n\n\n\n\n\n\n\n\n\nWhen aggregated, the higher risk of the older group skews the data, making it seem like more exercise increases risk. This is Simpson’s Paradox — where a trend appears in subgroups but reverses when the data is combined!"
  },
  {
    "objectID": "4-sampling.html#non-random-sampling",
    "href": "4-sampling.html#non-random-sampling",
    "title": "Sampling Principles and Strategies",
    "section": "Non random sampling",
    "text": "Non random sampling\n\nExpert Elicitation\nExpert elicitation is a form of non-random survey sampling where experts in a specific field are intentionally selected to provide insights or judgments on a topic.\n\n✅ Used when empirical data is scarce or uncertain (e.g., climate change risks).\n❌ Relies on expert judgment, which can introduce biases based on individual perspectives.\n\n\n\nExample: Estimating global mean sea-level rise and its uncertainties by 2100 and 2300 from an expert survey\n\nA survey selected experts who (co-)authored sea-level related papers (&gt;6).\n\nTo objectively select sea-level experts, we used the scientific publication database Web of Science of Clarivate to identify the most active publishers of sea-level papers.\nOn 15 February 2019, we searched for all papers published in peer-reviewed journals since (and including) 2014 where the term “sea level” appeared in the title, keywords or “KeyWords Plus” (an algorithm used to review words or phrases that appear in the cited references of an article) to identify scientists who (co-)authored the greatest number of these papers.\nWe obtained a sample of 878 experts who published at least six papers on “sea level” since 2014.\nWe found e-mail addresses for 817 of these experts and accordingly invited them to participate in the survey on 18 March 2019, using a unique identifier to ensure anonymity and avoid duplicate responses.\nA total of 458 experts opened the e-mail invitation, and of these 112 completed the survey, which is typical for this type of internet survey.\nThe main reason given for declining to participate was a (perceived) lack of expertise in projecting GMSL rise.\nWe closed the survey on 30 June 2019. We could not analyze six responses from participants because they either left all boxes blank or filled with a question mark. Not all survey respondents completed every percentile box.\n\n\nThus, a total of 106 sea-level experts from 817 invites (13%) provided their probabilistic assessment of GMSL rise, given two temperature projection scenarios.\nThe paper can be found here.\n\n\nAnalysis\n\nglimpse(expert_survey_dat)\n\nRows: 112\nColumns: 23\n$ name         &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17…\n$ Expertise_A  &lt;fct&gt; 1) Statistical or physical process modeling, 1) Statistic…\n$ Expertise    &lt;chr&gt; \"Ice-sheet modeling, Glacial-isostatic adjustment modelin…\n$ Blue_95_2100 &lt;dbl&gt; 80, 80, NA, 100, 126, 10, 75, 60, 55, 50, 60, NA, 55, 100…\n$ Blue_83_2100 &lt;dbl&gt; NA, 68, 59, 90, 98, 8, 67, 50, 45, 40, 55, NA, NA, 60, 70…\n$ Blue_50_2100 &lt;dbl&gt; 45, 50, 43, 70, 69, 5, 50, 40, 40, 30, 45, NA, 40, 50, 50…\n$ Blue_17_2100 &lt;dbl&gt; NA, 41, 29, 40, 49, 3, 37, 30, 35, 20, 35, NA, NA, 40, 30…\n$ Blue_5_2100  &lt;dbl&gt; 25, 35, NA, 21, 36, 1, 25, 20, 30, 10, 30, NA, 25, 30, 20…\n$ Red_95_2100  &lt;dbl&gt; 210, 200, NA, 125, 238, 300, 150, 80, 150, 200, 190, NA, …\n$ Red_83_2100  &lt;dbl&gt; NA, 160, 110, 110, 174, 200, 125, 70, 120, 120, 170, NA, …\n$ Red_50_2100  &lt;dbl&gt; 100, 100, 84, 80, 111, 100, 100, 60, 80, 80, 110, NA, 70,…\n$ Red_17_2100  &lt;dbl&gt; NA, 70, 61, 50, 79, 50, 75, 50, 50, 50, 100, NA, NA, 80, …\n$ Red_5_2100   &lt;dbl&gt; 60, 50, NA, 40, 62, 30, 50, 40, 40, 30, 85, NA, 55, 60, 8…\n$ Blue_95_2300 &lt;dbl&gt; NA, 250, NA, 130, 300, 20, 150, 60, 180, 120, 190, NA, 80…\n$ Blue_83_2300 &lt;dbl&gt; NA, 180, 110, 90, 230, 10, 125, 50, 140, 100, 170, NA, NA…\n$ Blue_50_2300 &lt;dbl&gt; NA, 100, 85, 70, 142, 5, 100, 40, 120, 80, 150, NA, 50, 1…\n$ Blue_17_2300 &lt;dbl&gt; NA, 70, 60, 30, 83, 3, 75, 30, 100, 60, 135, NA, NA, 80, …\n$ Blue_5_2300  &lt;dbl&gt; 25, 50, NA, 20, 50, 2, 50, 20, 80, 40, 100, NA, 30, 50, 3…\n$ Red_95_2300  &lt;dbl&gt; NA, 1000, NA, 450, 700, 250, 250, 120, 700, 6, 750, NA, 1…\n$ Red_83_2300  &lt;dbl&gt; NA, 800, 540, 400, 672, 200, 225, 100, 500, 4, 650, NA, N…\n$ Red_50_2300  &lt;dbl&gt; NA, 500, 385, 290, 466, 100, 200, 80, 300, 3, 500, NA, 14…\n$ Red_17_2300  &lt;dbl&gt; NA, 380, 230, 100, 336, 50, 150, 60, 150, 2, 475, NA, NA,…\n$ Red_5_2300   &lt;dbl&gt; 140, 300, NA, 90, 220, 30, 125, 40, 100, 1, 400, NA, 55, …\n\nexpert_survey_dat_long &lt;- expert_survey_dat %&gt;% \n                            pivot_longer(Blue_95_2100:Red_5_2300,\n                                         names_to = \"scenario\",\n                                         values_to = \"prediction\") %&gt;% \n  separate(col = scenario, into = c(\"Scenario\",\"Percentile\",\"Year\")) \n\n  \n  \nexpert_survey_dat_long %&gt;% filter(Scenario == \"Red\",Year == 2100) %&gt;% \n  ggplot(aes(x=Percentile,y=prediction)) +\n  geom_boxplot(fill=\"firebrick\")+\n  labs(x=\"Percentile\",y=\"GMSL Rise (cm)\",caption=\"Red 2100\")+\n  ylim(-200,600)+\n  theme_bw()\n\n\n\n\nParticipants were asked to estimate likely (17th to 83rd percentiles) and very likely (5th to 95th percentiles) sea-level rise under a high emission temperature scenarios for 2100.\n\n\n\n\n\n\nSnowball Sampling\nSnowball sampling is a non-random sampling technique where existing participants help recruit new participants, forming a growing “snowball” effect.\n\n✅ Useful for hard-to-reach populations\n❌ Relies on social networks, which can lead to bias since participants are likely to refer similar individuals.\n\n\n\nExample: Global survey shows planners use widely varying sea-level rise projections for coastal adaptation\n\nCollaborating researchers known to be involved in sea level rise planning were contacted, sometimes targeting specific regions and cities.\nRegional leads sent emails to their contact lists.\n\nMore details for this snowball sampling method can be found here.\n\n\nAnalysis\n\nsl_planning_dat &lt;- readRDS(\"data/sl_planning_dat.rds\")\nsl_planning_dat\n\n# A tibble: 21 × 4\n   Continent relative     n group\n   &lt;fct&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n 1 Asia          9.09     1 B    \n 2 Europe        5        1 D    \n 3 Oceania       3.57     1 B    \n 4 Africa       33.3      2 C    \n 5 Asia         18.2      2 C    \n 6 Oceania       7.14     2 C    \n 7 Oceania      10.7      3 D    \n 8 Africa       66.7      4 A    \n 9 Europe       20        4 C    \n10 Europe       25        5 B    \n# ℹ 11 more rows\n\n## create barplot with % in each category \n\n## The colour palette\ncbPalette &lt;- c(\"#E69F00\",\"#009E73\",\"#999999\",\"#56B4E9\", \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\n## The plot\nggplot(sl_planning_dat, aes(x = fct_rev(Continent),y = relative, fill = group)) + \n  geom_bar(stat = \"identity\")+\n  geom_text(aes(x = Continent, label = paste0(round(relative,1),'% \\n (',n,')')),\n            colour = 'white', position=position_stack(vjust=0.5), size = 3) +\n  ggtitle('')+\n  coord_flip() +\n  ylab(\"Percentage (Count)\")+\n  xlab(\"\")+\n  labs(fill = \"Structure\") +\n  scale_y_reverse() +\n  theme_classic() +\n  theme(legend.position = \"bottom\",\n        plot.title = element_text(size=12),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank()) +\n  scale_fill_manual(values=cbPalette) \n\n\n\n\nRespondents formally structure the use of sea-level rise projections for planning purposes in four ways: A is a singular estimate, B is a low and a high estimate, C is a low, intermediate, and high estimate, and D is a low, intermediate, high, and high-end estimate. Shown are aggregated responses for five distinct geographical regions and the globe."
  },
  {
    "objectID": "4-sampling.html#survey-example",
    "href": "4-sampling.html#survey-example",
    "title": "Sampling Principles and Strategies",
    "section": "Survey Example",
    "text": "Survey Example\n\nA survey selected experts who (co-)authored sea-level related &gt;6 papers.\n\nTo objectively select sea-level experts, we used the scientific publication database Web of Science of Clarivate to identify the most active publishers of sea-level papers.\nOn 15 February 2019, we searched for all papers published in peer-reviewed journals since (and including) 2014 where the term “sea level” appeared in the title, keywords or “KeyWords Plus” (an algorithm used to review words or phrases that appear in the cited references of an article) to identify scientists who (co-)authored the greatest number of these papers.\nWe obtained a sample of 878 experts who published at least six papers on “sea level” since 2014.\nWe found e-mail addresses for 817 of these experts and accordingly invited them to participate in the survey on 18 March 2019, using a unique identifier to ensure anonymity and avoid duplicate responses.\nA total of 458 experts opened the e-mail invitation, and of these 112 completed the survey, which is typical for this type of internet survey.\nThe main reason given for declining to participate was a (perceived) lack of expertise in projecting GMSL rise.\nWe closed the survey on 30 June 2019. We could not analyze six responses from participants because they either left all boxes blank or filled with a question mark. Not all survey respondents completed every percentile box.\n\n\nThus, a total of 106 sea-level experts from 817 invites (13%) provided their probabilistic assessment of GMSL rise, given two temperature scenarios derived from the upper and lower extremes of the RCP scenarios.\nhttps://www.nature.com/articles/s41612-020-0121-5\n\nexpert_survey_dat &lt;- readRDS(\"data/expert_survey_dat.rds\")\nglimpse(expert_survey_dat)\n\nRows: 112\nColumns: 23\n$ name         &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17…\n$ Expertise_A  &lt;fct&gt; 1) Statistical or physical process modeling, 1) Statistic…\n$ Expertise    &lt;chr&gt; \"Ice-sheet modeling, Glacial-isostatic adjustment modelin…\n$ Blue_95_2100 &lt;dbl&gt; 80, 80, NA, 100, 126, 10, 75, 60, 55, 50, 60, NA, 55, 100…\n$ Blue_83_2100 &lt;dbl&gt; NA, 68, 59, 90, 98, 8, 67, 50, 45, 40, 55, NA, NA, 60, 70…\n$ Blue_50_2100 &lt;dbl&gt; 45, 50, 43, 70, 69, 5, 50, 40, 40, 30, 45, NA, 40, 50, 50…\n$ Blue_17_2100 &lt;dbl&gt; NA, 41, 29, 40, 49, 3, 37, 30, 35, 20, 35, NA, NA, 40, 30…\n$ Blue_5_2100  &lt;dbl&gt; 25, 35, NA, 21, 36, 1, 25, 20, 30, 10, 30, NA, 25, 30, 20…\n$ Red_95_2100  &lt;dbl&gt; 210, 200, NA, 125, 238, 300, 150, 80, 150, 200, 190, NA, …\n$ Red_83_2100  &lt;dbl&gt; NA, 160, 110, 110, 174, 200, 125, 70, 120, 120, 170, NA, …\n$ Red_50_2100  &lt;dbl&gt; 100, 100, 84, 80, 111, 100, 100, 60, 80, 80, 110, NA, 70,…\n$ Red_17_2100  &lt;dbl&gt; NA, 70, 61, 50, 79, 50, 75, 50, 50, 50, 100, NA, NA, 80, …\n$ Red_5_2100   &lt;dbl&gt; 60, 50, NA, 40, 62, 30, 50, 40, 40, 30, 85, NA, 55, 60, 8…\n$ Blue_95_2300 &lt;dbl&gt; NA, 250, NA, 130, 300, 20, 150, 60, 180, 120, 190, NA, 80…\n$ Blue_83_2300 &lt;dbl&gt; NA, 180, 110, 90, 230, 10, 125, 50, 140, 100, 170, NA, NA…\n$ Blue_50_2300 &lt;dbl&gt; NA, 100, 85, 70, 142, 5, 100, 40, 120, 80, 150, NA, 50, 1…\n$ Blue_17_2300 &lt;dbl&gt; NA, 70, 60, 30, 83, 3, 75, 30, 100, 60, 135, NA, NA, 80, …\n$ Blue_5_2300  &lt;dbl&gt; 25, 50, NA, 20, 50, 2, 50, 20, 80, 40, 100, NA, 30, 50, 3…\n$ Red_95_2300  &lt;dbl&gt; NA, 1000, NA, 450, 700, 250, 250, 120, 700, 6, 750, NA, 1…\n$ Red_83_2300  &lt;dbl&gt; NA, 800, 540, 400, 672, 200, 225, 100, 500, 4, 650, NA, N…\n$ Red_50_2300  &lt;dbl&gt; NA, 500, 385, 290, 466, 100, 200, 80, 300, 3, 500, NA, 14…\n$ Red_17_2300  &lt;dbl&gt; NA, 380, 230, 100, 336, 50, 150, 60, 150, 2, 475, NA, NA,…\n$ Red_5_2300   &lt;dbl&gt; 140, 300, NA, 90, 220, 30, 125, 40, 100, 1, 400, NA, 55, …\n\nexpert_survey_dat_long &lt;- expert_survey_dat %&gt;% \n                            pivot_longer(Blue_95_2100:Red_5_2300,\n                                         names_to = \"scenario\",\n                                         values_to = \"prediction\") %&gt;% \n  separate(col = scenario, into = c(\"Scenario\",\"Percentile\",\"Year\")) \n\n  \n  \nexpert_survey_dat_long %&gt;% filter(Scenario == \"Red\",Year == 2100) %&gt;% ggplot(aes(x=Percentile,y=prediction))+\n  geom_boxplot(fill=\"firebrick\")+\n  labs(x=\"Percentile\",y=\"GMSL Rise (cm)\",caption=\"Blue 2100\")+\n  ylim(-200,600)+\n  theme_bw()\n\nWarning: Removed 75 rows containing non-finite outside the scale range\n(`stat_boxplot()`)."
  }
]