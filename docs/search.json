[
  {
    "objectID": "1-TidyverseRecap.html",
    "href": "1-TidyverseRecap.html",
    "title": "Tidyverse Recap",
    "section": "",
    "text": "The tidyverse is a collection of R packages designed for data science.\nAll packages share an underlying design philosophy, grammar, and data structures.\nIt emphasizes tidy data in data frames, performs operations one step at a time, connects with pipes and makes code human readable."
  },
  {
    "objectID": "1-TidyverseRecap.html#tidyr-pivot",
    "href": "1-TidyverseRecap.html#tidyr-pivot",
    "title": "Tidyverse Recap",
    "section": "tidyr: pivot",
    "text": "tidyr: pivot\n\n# Load the tidyr package\nlibrary(tidyr)\n\n\n# Assume we have a dataset 'data' with 'ID1', 'ID2', 'x', and 'y' columns\ndata_ex1 &lt;- tibble(ID1 = rep(LETTERS[1:4],times = 3), \n                   ID2 = rep(letters[1:3], each = 4), \n                   x = 1:12, \n                   y = 21:32)\n\nprint(data_ex1)\n\n# A tibble: 12 × 4\n   ID1   ID2       x     y\n   &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt;\n 1 A     a         1    21\n 2 B     a         2    22\n 3 C     a         3    23\n 4 D     a         4    24\n 5 A     b         5    25\n 6 B     b         6    26\n 7 C     b         7    27\n 8 D     b         8    28\n 9 A     c         9    29\n10 B     c        10    30\n11 C     c        11    31\n12 D     c        12    32\n\n# Use pivot_longer() to convert wide data to long format\ndata_long &lt;- data_ex1 %&gt;% pivot_longer(cols = c(\"x\", \"y\"), \n                                       names_to = \"Variable\", \n                                       values_to = \"Value\")\n\n# Print the long format data\nprint(data_long)\n\n# A tibble: 24 × 4\n   ID1   ID2   Variable Value\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;    &lt;int&gt;\n 1 A     a     x            1\n 2 A     a     y           21\n 3 B     a     x            2\n 4 B     a     y           22\n 5 C     a     x            3\n 6 C     a     y           23\n 7 D     a     x            4\n 8 D     a     y           24\n 9 A     b     x            5\n10 A     b     y           25\n# ℹ 14 more rows\n\n# Use pivot_wider() to convert long data back to wide format\ndata_wide &lt;- data_long %&gt;% pivot_wider(names_from = Variable,\n                                       values_from = Value)\n\n# Print the wide format data\nprint(data_wide)\n\n# A tibble: 12 × 4\n   ID1   ID2       x     y\n   &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt;\n 1 A     a         1    21\n 2 B     a         2    22\n 3 C     a         3    23\n 4 D     a         4    24\n 5 A     b         5    25\n 6 B     b         6    26\n 7 C     b         7    27\n 8 D     b         8    28\n 9 A     c         9    29\n10 B     c        10    30\n11 C     c        11    31\n12 D     c        12    32\n\n\nIn this example, pivot_longer is used to convert the wide format data to long format, where each row is a single observation associated with the variables ID1, ID2, Variable (containing the original column names ‘x’ and ‘y’), and Value (containing the values from ‘x’ and ‘y’ columns). We can then also convert back to wide format using pivot_wider."
  },
  {
    "objectID": "1-TidyverseRecap.html#tidyr-separate",
    "href": "1-TidyverseRecap.html#tidyr-separate",
    "title": "Tidyverse Recap",
    "section": "tidyr: separate",
    "text": "tidyr: separate\n\n# Load the tidyr package\nlibrary(tidyr)\n\n# Assume we have a dataset 'dataNew' with a 'datetime' column\ndata_ex2 &lt;- tibble(datetime = \n                    c(\"2016-01-01 07:30:29\", \"2016-01-02 09:43:36\", \"2016-01-03 13:59:00\"), \n                   event = c(\"u\", \"a\", \"l\"))\n\n# Use the separate() function from tidyr to separate the 'datetime' column into \n# 'date' and 'time'\n# Then separate 'time' into 'hour', 'min', 'second'\ndata_sep &lt;- data_ex2 %&gt;% \n              separate(datetime, c('date', 'time'), sep = ' ') %&gt;% \n              separate(time, c('hour', 'min', 'second'), sep = ':')\n\n# Print the new dataset\nprint(data_sep)\n\n# A tibble: 3 × 5\n  date       hour  min   second event\n  &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;\n1 2016-01-01 07    30    29     u    \n2 2016-01-02 09    43    36     a    \n3 2016-01-03 13    59    00     l    \n\n# change hour, min, second to numeric values\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ndata_sep %&gt;% mutate_at(vars(hour, min, second), as.numeric)\n\n# A tibble: 3 × 5\n  date        hour   min second event\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;\n1 2016-01-01     7    30     29 u    \n2 2016-01-02     9    43     36 a    \n3 2016-01-03    13    59      0 l"
  },
  {
    "objectID": "1-TidyverseRecap.html#example-dplyr",
    "href": "1-TidyverseRecap.html#example-dplyr",
    "title": "Tidyverse Recap",
    "section": "Example: dplyr",
    "text": "Example: dplyr\n\n# Load the dplyr package\nlibrary(dplyr)\n\n# Assume we have a dataset 'data' with 'ID', 'Age', 'Gender', and 'Income' columns\ndata_ex3 &lt;- tibble(ID = 1:4, \n                   Age = c(21, 35, 58, 40), \n                   Gender = c(\"Male\", \"Female\", \"Male\", \"Female\"), \n                   Income = c(50000, 80000, 120000, 75000))\n\n# Use select() to choose the 'ID' and 'Age' columns\nselected_data &lt;- data_ex3 %&gt;% select(ID, Age)\nselected_data\n\n# A tibble: 4 × 2\n     ID   Age\n  &lt;int&gt; &lt;dbl&gt;\n1     1    21\n2     2    35\n3     3    58\n4     4    40\n\n# Use filter() to get rows where 'Age' is greater than 30\nfiltered_data &lt;- data_ex3 %&gt;% filter(Age &gt; 30)\nfiltered_data\n\n# A tibble: 3 × 4\n     ID   Age Gender Income\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1     2    35 Female  80000\n2     3    58 Male   120000\n3     4    40 Female  75000\n\n# Use mutate() to create a new column 'IncomeInThousands'\nmutated_data &lt;- data_ex3 %&gt;% mutate(IncomeInThousands = Income / 1000)\nmutated_data\n\n# A tibble: 4 × 5\n     ID   Age Gender Income IncomeInThousands\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;             &lt;dbl&gt;\n1     1    21 Male    50000                50\n2     2    35 Female  80000                80\n3     3    58 Male   120000               120\n4     4    40 Female  75000                75\n\n# Use arrange() to sort data by 'Income'\narranged_data &lt;- data_ex3 %&gt;% arrange(Income)\narranged_data\n\n# A tibble: 4 × 4\n     ID   Age Gender Income\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1     1    21 Male    50000\n2     4    40 Female  75000\n3     2    35 Female  80000\n4     3    58 Male   120000\n\n# Use summarise() to get the mean 'Income'\nsummary_data &lt;- data_ex3 %&gt;% summarise(MeanIncome = mean(Income))\nsummary_data\n\n# A tibble: 1 × 1\n  MeanIncome\n       &lt;dbl&gt;\n1      81250\n\n# Use group_by() and summarise() to get the mean 'Income' for each 'Gender'\ngrouped_data &lt;- data_ex3 %&gt;% \n                  group_by(Gender) %&gt;% \n                  summarise(MeanIncome = mean(Income))\ngrouped_data\n\n# A tibble: 2 × 2\n  Gender MeanIncome\n  &lt;chr&gt;       &lt;dbl&gt;\n1 Female      77500\n2 Male        85000\n\n\nIn these examples, select is used to choose specific columns, filter is used to select rows based on a condition, mutate is used to create a new column, arrange is used to sort data, summarise is used to calculate summary statistics, and group_by is used to perform operations on groups of data."
  },
  {
    "objectID": "1-TidyverseRecap.html#example-ggplot",
    "href": "1-TidyverseRecap.html#example-ggplot",
    "title": "Tidyverse Recap",
    "section": "Example: ggplot",
    "text": "Example: ggplot"
  },
  {
    "objectID": "1-TidyverseRecap.html#question",
    "href": "1-TidyverseRecap.html#question",
    "title": "Tidyverse Recap",
    "section": "Question",
    "text": "Question\nSuppose we have a dataset called penguins and suppose we would like to study how the ratio of penguin body mass to flipper size differs across the species in the dataset. Rearrange the following steps in the pipeline into an order that accomplishes this goal.\n\n# a\narrange(avg_mass_flipper_ratio)\n\n# b\ngroup_by(species)\n\n# c\npenguins \n  \n# d\nsummarise(\n  avg_mass_flipper_ratioo = median(mass_flipper_ratio)\n)\n  \n# e\nmutate(\n  mass_flipper_ratio = body_mass_g/flipper_length_mm\n)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Introduction to Data Science (2)!",
    "section": "",
    "text": "Welcome to the course website for DS152 Introduction to Data Science (2).\nModule information\nLecture material (slides, notes, videos) are licensed under CC-BY-NC 4.0.\nContact: Niamh Cahill (niamh.cahill@mu.ie)"
  },
  {
    "objectID": "index.html#lecture-slides",
    "href": "index.html#lecture-slides",
    "title": "Welcome to Bayesian Data Analysis!",
    "section": "Lecture Slides",
    "text": "Lecture Slides\n\nWeek 1\n1a: Bayes Rule\n1b: Inferring a Binomial Probability\n\n\nWeek 2\n2a: Beta Binomial\n2b: Bayesian Inference\n\n\nWeek 3\n3: Single Parameter Normal\n\n\nWeek 4\n4: MCMC Sampling\n\n\nWeek 5\n5a: MCMC Diagnostics\n5b: Just Another Gibbs Sampler\n\n\nWeek 6\n6: Bayesian Linear Regression\n\n\nWeek 7\n7: Model Checking\n\n\nWeek 8\n8: Introducing Bayesian Hierarchical Modelling\n\n\nWeek 9\n9: Bayesian Hierarchical Regression Modelling\n\n\nWeek 10\n10: Bayesian Generalised Linear Models (GLMs)\n\n\nWeek 11\n11: Bayesian Hierarchical Modelling - GLM"
  },
  {
    "objectID": "index.html#tutorials",
    "href": "index.html#tutorials",
    "title": "Welcome to Bayesian Data Analysis!",
    "section": "Tutorials",
    "text": "Tutorials\nTutorial Sheet 1: Bayesian inference using binomial and Poisson models\n\nTutorial Sheet 2: Bayesian Model for Multiple Proportions - Email Campaign Click-Through Rates\nTutorial Sheet 3: Bayesian Regression Model - Fisherys Data\nTutorial Sheet 4: Bayesian Hierarchical Regression Modelling - Simulation"
  },
  {
    "objectID": "index.html#assignments",
    "href": "index.html#assignments",
    "title": "Welcome to Bayesian Data Analysis!",
    "section": "Assignments",
    "text": "Assignments\nAssignment 1: Bayesian Inference on Particle Emission Energy\nAssignment 2: Comparing Video Game Playing Among Students in 5 Different Countries\nAssignment 3: Football outcomes vs point spread\nAssignment 4: Radon Analysis"
  },
  {
    "objectID": "index.html#lecture-notes",
    "href": "index.html#lecture-notes",
    "title": "Welcome to Introduction to Data Science (2)!",
    "section": "Lecture Notes",
    "text": "Lecture Notes\n1: Tidyverse Recap\n2: Correlation (and Causation)\n3: Multiple Regression Models"
  },
  {
    "objectID": "0-Information.html#course-organisation",
    "href": "0-Information.html#course-organisation",
    "title": "Module Information",
    "section": "",
    "text": "Lecture and Lab Timetable\n\nMonday 11am (Lecture, CB8), Wednesday 2pm (Lecture, CH)\nTuesday 3pm (Lab, TSI239), Friday 9am (Lab, TSI239)\n\nLabs will start in week 2\nPlease confirm your choice on Moodle.\n\n\nTutorials\n\nTuesdays @ 9am, 10am, 2pm, 3pm, 5pm, Wednesdays @ 12pm, 1pm\nTutorials – starting week 4 (24/02/2025)\n\nOffice hours\n\nBy appointment"
  },
  {
    "objectID": "0-Information.html#course-organisation-1",
    "href": "0-Information.html#course-organisation-1",
    "title": "Module Information",
    "section": "Course organisation",
    "text": "Course organisation\nMarks\n\n10% – compulsory assignments\n10% – quizzes\n15% – mid-term exam (March 28th, MCQ)\n65% – final exam (date TBC)\n\nNotes\n\nNotes will be a combination of handouts posted on Moodle and notes taken down in class\n\nAssignments\n\nWill be uploaded PDFs\n\nRecommended reading\n\nR for Data Science (https://r4ds.had.co.nz)"
  },
  {
    "objectID": "0-Information.html#diversity-inclusion",
    "href": "0-Information.html#diversity-inclusion",
    "title": "Module Information",
    "section": "Diversity & inclusion",
    "text": "Diversity & inclusion\nIt is my intent to present materials and activities that are respectful of diversity: gender identity, sexuality, disability, age, socioeconomic status, ethnicity, race, nationality, religion, and culture. I may not always get this right so please let me know ways to improve the effectiveness of the course for you personally, or for other students or student groups.\nTo help with this:\n\nIf you have a name that differs from those that appear in your official University records, please let me know!\nPlease let me know your preferred pronouns if you wish to do so.\nI (like many people) am still in the process of learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please talk to me about it."
  },
  {
    "objectID": "0-Information.html",
    "href": "0-Information.html",
    "title": "Module Information",
    "section": "",
    "text": "Lecture and Lab Timetable\n\nMonday 11am (Lecture, CB8), Wednesday 2pm (Lecture, CH)\nTuesday 3pm (Lab, TSI239), Friday 9am (Lab, TSI239)\n\nLabs will start in week 2\nPlease confirm your choice on Moodle.\n\n\nTutorials\n\nTuesdays @ 9am, 10am, 2pm, 3pm, 5pm, Wednesdays @ 12pm, 1pm\nTutorials – starting week 4 (24/02/2025)\n\nOffice hours\n\nBy appointment"
  },
  {
    "objectID": "1-TidyverseRecap.html#what-is-tidyverse",
    "href": "1-TidyverseRecap.html#what-is-tidyverse",
    "title": "Tidyverse Recap",
    "section": "",
    "text": "The tidyverse is a collection of R packages designed for data science.\nAll packages share an underlying design philosophy, grammar, and data structures.\nIt emphasizes tidy data in data frames, performs operations one step at a time, connects with pipes and makes code human readable."
  },
  {
    "objectID": "1-TidyverseRecap.html#key-packages-in-tidyverse",
    "href": "1-TidyverseRecap.html#key-packages-in-tidyverse",
    "title": "Tidyverse Recap",
    "section": "Key Packages in tidyverse",
    "text": "Key Packages in tidyverse\n\nreadr: Used for importing data.\ntidyr: Used for tidying and reshaping data.\ndplyr: Used for data transformation.\nggplot2: Used for data visualization.\nmagrittr: Provides the pipe operator (%&gt;%) or (|&gt;) which is used to chain together sequences of operations."
  },
  {
    "objectID": "1-TidyverseRecap.html#importing-data-with-readr",
    "href": "1-TidyverseRecap.html#importing-data-with-readr",
    "title": "Tidyverse Recap",
    "section": "Importing Data with readr",
    "text": "Importing Data with readr\n\nreadr provides faster and consistent replacements for data import functions in base R.\nIt fits into the tidyverse naturally and extends neatly into other data types.\nExample: read_csv(file, show_col_types = FALSE)."
  },
  {
    "objectID": "1-TidyverseRecap.html#tidying-data-with-tidyr",
    "href": "1-TidyverseRecap.html#tidying-data-with-tidyr",
    "title": "Tidyverse Recap",
    "section": "Tidying Data with tidyr",
    "text": "Tidying Data with tidyr\n\ntidyr provides a set of functions that help to tidy data.\nTidy data is data where every column is a variable, every row is an observation, and every cell is a single value.\n\n\ntidyr: pivot\n\n# Load the tidyr package\nlibrary(tidyr)\n\n\n# Assume we have a dataset 'data' with 'ID1', 'ID2', 'x', and 'y' columns\ndata_ex1 &lt;- tibble(ID1 = rep(LETTERS[1:4],times = 3), \n                   ID2 = rep(letters[1:3], each = 4), \n                   x = 1:12, \n                   y = 21:32)\n\nprint(data_ex1)\n\n# A tibble: 12 × 4\n   ID1   ID2       x     y\n   &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt;\n 1 A     a         1    21\n 2 B     a         2    22\n 3 C     a         3    23\n 4 D     a         4    24\n 5 A     b         5    25\n 6 B     b         6    26\n 7 C     b         7    27\n 8 D     b         8    28\n 9 A     c         9    29\n10 B     c        10    30\n11 C     c        11    31\n12 D     c        12    32\n\n# Use pivot_longer() to convert wide data to long format\ndata_long &lt;- data_ex1 %&gt;% pivot_longer(cols = c(\"x\", \"y\"), \n                                       names_to = \"Variable\", \n                                       values_to = \"Value\")\n\n# Print the long format data\nprint(data_long)\n\n# A tibble: 24 × 4\n   ID1   ID2   Variable Value\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;    &lt;int&gt;\n 1 A     a     x            1\n 2 A     a     y           21\n 3 B     a     x            2\n 4 B     a     y           22\n 5 C     a     x            3\n 6 C     a     y           23\n 7 D     a     x            4\n 8 D     a     y           24\n 9 A     b     x            5\n10 A     b     y           25\n# ℹ 14 more rows\n\n# Use pivot_wider() to convert long data back to wide format\ndata_wide &lt;- data_long %&gt;% pivot_wider(names_from = Variable,\n                                       values_from = Value)\n\n# Print the wide format data\nprint(data_wide)\n\n# A tibble: 12 × 4\n   ID1   ID2       x     y\n   &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt;\n 1 A     a         1    21\n 2 B     a         2    22\n 3 C     a         3    23\n 4 D     a         4    24\n 5 A     b         5    25\n 6 B     b         6    26\n 7 C     b         7    27\n 8 D     b         8    28\n 9 A     c         9    29\n10 B     c        10    30\n11 C     c        11    31\n12 D     c        12    32\n\n\nIn this example, pivot_longer is used to convert the wide format data to long format, where each row is a single observation associated with the variables ID1, ID2, Variable (containing the original column names ‘x’ and ‘y’), and Value (containing the values from ‘x’ and ‘y’ columns). We can then also convert back to wide format using pivot_wider.\n\n\ntidyr: separate\n\n# Load the tidyr package\nlibrary(tidyr)\n\n# Assume we have a dataset 'dataNew' with a 'datetime' column\ndata_ex2 &lt;- tibble(datetime = \n                    c(\"2016-01-01 07:30:29\", \"2016-01-02 09:43:36\", \"2016-01-03 13:59:00\"), \n                   event = c(\"u\", \"a\", \"l\"))\n\n# Use the separate() function from tidyr to separate the 'datetime' column into \n# 'date' and 'time'\n# Then separate 'time' into 'hour', 'min', 'second'\ndata_sep &lt;- data_ex2 %&gt;% \n              separate(datetime, c('date', 'time'), sep = ' ') %&gt;% \n              separate(time, c('hour', 'min', 'second'), sep = ':')\n\n# Print the new dataset\nprint(data_sep)\n\n# A tibble: 3 × 5\n  date       hour  min   second event\n  &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;\n1 2016-01-01 07    30    29     u    \n2 2016-01-02 09    43    36     a    \n3 2016-01-03 13    59    00     l    \n\n# change hour, min, second to numeric values\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ndata_sep %&gt;% mutate_at(vars(hour, min, second), as.numeric)\n\n# A tibble: 3 × 5\n  date        hour   min second event\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;\n1 2016-01-01     7    30     29 u    \n2 2016-01-02     9    43     36 a    \n3 2016-01-03    13    59      0 l"
  },
  {
    "objectID": "1-TidyverseRecap.html#transforming-data-with-dplyr",
    "href": "1-TidyverseRecap.html#transforming-data-with-dplyr",
    "title": "Tidyverse Recap",
    "section": "Transforming Data with dplyr",
    "text": "Transforming Data with dplyr\n\ndplyr is a grammar of data manipulation, providing a consistent set of verbs that solve the most common data manipulation challenges.\nExample: filter(data, condition).\n\n\nExample: dplyr\n\n# Load the dplyr package\nlibrary(dplyr)\n\n# Assume we have a dataset 'data' with 'ID', 'Age', 'Gender', and 'Income' columns\ndata_ex3 &lt;- tibble(ID = 1:4, \n                   Age = c(21, 35, 58, 40), \n                   Gender = c(\"Male\", \"Female\", \"Male\", \"Female\"), \n                   Income = c(50000, 80000, 120000, 75000))\n\n# Use select() to choose the 'ID' and 'Age' columns\nselected_data &lt;- data_ex3 %&gt;% select(ID, Age)\nselected_data\n\n# A tibble: 4 × 2\n     ID   Age\n  &lt;int&gt; &lt;dbl&gt;\n1     1    21\n2     2    35\n3     3    58\n4     4    40\n\n# Use filter() to get rows where 'Age' is greater than 30\nfiltered_data &lt;- data_ex3 %&gt;% filter(Age &gt; 30)\nfiltered_data\n\n# A tibble: 3 × 4\n     ID   Age Gender Income\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1     2    35 Female  80000\n2     3    58 Male   120000\n3     4    40 Female  75000\n\n# Use mutate() to create a new column 'IncomeInThousands'\nmutated_data &lt;- data_ex3 %&gt;% mutate(IncomeInThousands = Income / 1000)\nmutated_data\n\n# A tibble: 4 × 5\n     ID   Age Gender Income IncomeInThousands\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;             &lt;dbl&gt;\n1     1    21 Male    50000                50\n2     2    35 Female  80000                80\n3     3    58 Male   120000               120\n4     4    40 Female  75000                75\n\n# Use arrange() to sort data by 'Income'\narranged_data &lt;- data_ex3 %&gt;% arrange(Income)\narranged_data\n\n# A tibble: 4 × 4\n     ID   Age Gender Income\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1     1    21 Male    50000\n2     4    40 Female  75000\n3     2    35 Female  80000\n4     3    58 Male   120000\n\n# Use summarise() to get the mean 'Income'\nsummary_data &lt;- data_ex3 %&gt;% summarise(MeanIncome = mean(Income))\nsummary_data\n\n# A tibble: 1 × 1\n  MeanIncome\n       &lt;dbl&gt;\n1      81250\n\n# Use group_by() and summarise() to get the mean 'Income' for each 'Gender'\ngrouped_data &lt;- data_ex3 %&gt;% \n                  group_by(Gender) %&gt;% \n                  summarise(MeanIncome = mean(Income))\ngrouped_data\n\n# A tibble: 2 × 2\n  Gender MeanIncome\n  &lt;chr&gt;       &lt;dbl&gt;\n1 Female      77500\n2 Male        85000\n\n\nIn these examples, select is used to choose specific columns, filter is used to select rows based on a condition, mutate is used to create a new column, arrange is used to sort data, summarise is used to calculate summary statistics, and group_by is used to perform operations on groups of data."
  },
  {
    "objectID": "1-TidyverseRecap.html#visualizing-data-with-ggplot2",
    "href": "1-TidyverseRecap.html#visualizing-data-with-ggplot2",
    "title": "Tidyverse Recap",
    "section": "Visualizing Data with ggplot2",
    "text": "Visualizing Data with ggplot2\n\nggplot2 is a system for declaratively creating graphics, based on “The Grammar of Graphics”.\nYou provide the data, tell ggplot2 how to map variables to aesthetics, what graphic to use, and it takes care of the details.\n\n\n\n\nExample: ggplot\nBasic scatter plot with a regression line\n\n# Load the ggplot2 package\nlibrary(ggplot2)\n\np1 &lt;- ggplot(mtcars, aes(x = mpg, y = hp)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\")\np1\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nHistogram\n\n# Load the ggplot2 package\nlibrary(ggplot2)\n\np2 &lt;- ggplot(mtcars, aes(x = mpg)) +\n  geom_histogram(binwidth = 2, fill = \"blue\", color = \"white\")\np2\n\n\n\n\n\n\n\n\nBoxplot\n\n# Load the ggplot2 package\nlibrary(ggplot2)\n\np3 &lt;- ggplot(mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_boxplot(fill = \"orange\", color = \"darkred\")\np3\n\n\n\n\n\n\n\n\nBar chart\n\n# Load the ggplot2 package\nlibrary(ggplot2)\n\np4 &lt;- ggplot(mtcars, aes(x = factor(cyl))) +\n  geom_bar(fill = \"steelblue\") +\n  labs(x = \"Number of cylinders\", y = \"Frequency\")\np4\n\n\n\n\n\n\n\n\nIn these examples, geom_point is used to create a scatter plot, geom_smooth with method = \"lm\" is used to add a linear regression line, geom_histogram is used to create a histogram, geom_boxplot is used to create a boxplot, and geom_bar is used to create a bar chart."
  },
  {
    "objectID": "1-TidyverseRecap.html#class-exercise",
    "href": "1-TidyverseRecap.html#class-exercise",
    "title": "Tidyverse Recap",
    "section": "Class Exercise",
    "text": "Class Exercise\nSuppose we have a dataset called penguins and suppose we would like to study how the ratio of penguin body mass to flipper size differs across the species in the dataset. Rearrange the following steps in the pipeline into an order that accomplishes this goal.\n\n# a\narrange(avg_mass_flipper_ratio)\n\n# b\ngroup_by(species) %&gt;% \n\n# c\npenguins %&gt;% \n  \n# d\nsummarise(\n  avg_mass_flipper_ratio = median(mass_flipper_ratio, na.rm = TRUE)\n) %&gt;% \n  \n# e\nmutate(\n  mass_flipper_ratio = body_mass_g/flipper_length_mm\n) %&gt;%"
  },
  {
    "objectID": "2-Correlation.html",
    "href": "2-Correlation.html",
    "title": "Correlation (and Causation)",
    "section": "",
    "text": "When we see a pattern, we don’t just say “how extraordinary!” and move on; instead, we try and attribute a cause!\nhttps://www.tylervigen.com/spurious-correlations"
  },
  {
    "objectID": "2-Correlation.html#correlation-vs.-causation",
    "href": "2-Correlation.html#correlation-vs.-causation",
    "title": "Causation and Correlation",
    "section": "",
    "text": "When we see a pattern, we don’t just say “how extraordinary!” and move on; instead, we try and attribute a cause!\n\nWe all draw conclusions on the basis of what we see\nBut it is important for us to remember that just because there is a correlation between two facts, there isn’t necessarily a cause/effect relationship between them.\n\nlistening to loud music and acne\nice cream consumption and shark attacks\nhand size and reading ability in children\n…\n\nThese variables are correlated, but one does not cause the other!\n\nhttps://www.tylervigen.com/spurious-correlations"
  },
  {
    "objectID": "2-Correlation.html#correlation",
    "href": "2-Correlation.html#correlation",
    "title": "Correlation (and Causation)",
    "section": "Correlation",
    "text": "Correlation\nCorrelation (r) quantifies the linear association between two quantitative variables.\n\nThe value of \\(r\\) is between -1 and 1.\n\\(r &gt;\\) 0 when \\(x\\) and \\(y\\) have a positive association.\n\\(r &lt;\\) 0 when \\(x\\) and \\(y\\) have a negative association.\n\\(r\\) = 1 means a perfect positive linear association.\n\\(r\\) = -1 means a perfect negative linear association.\n\\(r\\) = 0 indicates no linear association between \\(x\\) and \\(y\\).\nThe value of \\(r\\) is a measure of the extent to which \\(x\\) and \\(y\\) are linearly related."
  },
  {
    "objectID": "2-Correlation.html#what-are-the-correlation-values-here",
    "href": "2-Correlation.html#what-are-the-correlation-values-here",
    "title": "Correlation (and Causation)",
    "section": "What are the correlation values here?",
    "text": "What are the correlation values here?\nTask: Match the plot panel number to the letter with the correct correlation value.\n\n\n\n\n\n\n\n\n\n          A           B           C           D           E           F \n-0.02770462  0.96643642 -0.69813746  0.04969697 -0.96735724  0.11948059 \n          G           H \n-0.39421900  0.67692544"
  },
  {
    "objectID": "2-Correlation.html#what-are-the-correlation-values-here-1",
    "href": "2-Correlation.html#what-are-the-correlation-values-here-1",
    "title": "Causation and Correlation",
    "section": "What are the correlation values here?",
    "text": "What are the correlation values here?"
  },
  {
    "objectID": "2-Correlation.html#what-are-the-correlation-values-here-2",
    "href": "2-Correlation.html#what-are-the-correlation-values-here-2",
    "title": "Causation and Correlation",
    "section": "What are the correlation values here?",
    "text": "What are the correlation values here?"
  },
  {
    "objectID": "2-Correlation.html#what-are-the-correlation-values-here-3",
    "href": "2-Correlation.html#what-are-the-correlation-values-here-3",
    "title": "Causation and Correlation",
    "section": "What are the correlation values here?",
    "text": "What are the correlation values here?"
  },
  {
    "objectID": "2-Correlation.html#what-are-the-correlation-values-here-4",
    "href": "2-Correlation.html#what-are-the-correlation-values-here-4",
    "title": "Causation and Correlation",
    "section": "What are the correlation values here?",
    "text": "What are the correlation values here?"
  },
  {
    "objectID": "2-Correlation.html#what-are-the-correlation-values-here-5",
    "href": "2-Correlation.html#what-are-the-correlation-values-here-5",
    "title": "Causation and Correlation",
    "section": "What are the correlation values here?",
    "text": "What are the correlation values here?"
  },
  {
    "objectID": "2-Correlation.html#what-are-the-correlation-values-here-6",
    "href": "2-Correlation.html#what-are-the-correlation-values-here-6",
    "title": "Causation and Correlation",
    "section": "What are the correlation values here?",
    "text": "What are the correlation values here?"
  },
  {
    "objectID": "2-Correlation.html#what-are-the-correlation-values-here-7",
    "href": "2-Correlation.html#what-are-the-correlation-values-here-7",
    "title": "Causation and Correlation",
    "section": "What are the correlation values here?",
    "text": "What are the correlation values here?"
  },
  {
    "objectID": "2-Correlation.html#calculating-the-correlation-coefficient",
    "href": "2-Correlation.html#calculating-the-correlation-coefficient",
    "title": "Correlation (and Causation)",
    "section": "Calculating the correlation coefficient",
    "text": "Calculating the correlation coefficient\nWe denote \\[\\begin{eqnarray*}\nS_{xx} &=& \\sum_{i=1}^{n}(x_i - \\bar{x})^2 = \\sum_{i=1}^{n}x_i^2 - n\\bar{x}^2 \\\\\nS_{yy} &=& \\sum_{i=1}^{n}(y_i - \\bar{y})^2 = \\sum_{i=1}^{n}y_i^2 - n\\bar{y}^2 \\\\\nS_{xy} &=& \\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y}) = \\sum_{i=1}^{n}x_iy_i - n\\bar{x}\\bar{y}\n\\end{eqnarray*}\\]\nThen \\[\\begin{eqnarray*}\nr &=& \\frac{Sxy}{\\sqrt{SxxSyy}}     \n\\end{eqnarray*}\\]\n\nExample: Calculate the correlation between \\(x\\) and \\(y\\)\n\n\n\n\n\nx\n2\n4\n1\n6\n7\n\n\ny\n3\n4\n0\n8\n8\n\n\n\n\n\n\n\\[\\begin{align*}\n&\\sum_{i=1}^{n}x_i^2=106, \\sum_{i=1}^{n}y_i^2=153, \\sum_{i=1}^{n}x_iy_i=126, \\bar{x}=4,\\bar{y}=4.6\\\\\n&S_{xx} = \\sum_{i=1}^{n}x_i^2 - n\\bar{x}^2 = 26, S_{yy} = \\sum_{i=1}^{n}y_i^2 - n\\bar{y}^2 = 47.2\\\\\n&S_{xy} = \\sum_{i=1}^{n}x_iy_i - n\\bar{x}\\bar{y} = 34\\\\\n\\\\\n&r = \\frac{Sxy}{\\sqrt{SxxSyy}} = \\frac{34}{\\sqrt{26 \\times 47.2}} = 0.97        \n\\end{align*}\\]\n\n\nExample: Change of scale and the correlation coefficient\nThe distance of the race and the time it took to complete was recorded for five races in kilometres and seconds respectively. The correlation was calculated between the two variables. The data set was also converted into miles (\\(\\times\\) 0.621371192) and minutes (/60) and the correlation was re-calculated.\n\n\n\n\n\nKilometres\nSeconds\nMiles\nMinutes\n\n\n\n\n0.1\n10\n0.0621371\n0.1666667\n\n\n0.4\n120\n0.2485485\n2.0000000\n\n\n0.8\n300\n0.4970970\n5.0000000\n\n\n1.6\n535\n0.9941939\n8.9166667\n\n\n3.0\n950\n1.8641136\n15.8333333\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nIf \\(x\\) and \\(y\\) measurement units are changed, correlation does not change.\nIf \\(x\\) and \\(y\\) are reversed, i.e. correlation of \\(y\\) and \\(x\\), the correlation does not change.\nCorrelation is a measure of linear association. It does not establish causation.\nTwo variables, x and y, could be highly correlated because there is another variable, z, having an impact on both x and y."
  },
  {
    "objectID": "2-Correlation.html#example-calculate-the-correlation-between-x-and-y",
    "href": "2-Correlation.html#example-calculate-the-correlation-between-x-and-y",
    "title": "Causation and Correlation",
    "section": "Example: Calculate the correlation between \\(x\\) and \\(y\\)",
    "text": "Example: Calculate the correlation between \\(x\\) and \\(y\\)\n\n\n\n\n\nx\n2\n4\n1\n6\n7\n\n\ny\n3\n4\n0\n8\n8\n\n\n\n\n\n\n\\[\\begin{align*}\n&\\sum_{i=1}^{n}x_i^2=106, \\sum_{i=1}^{n}y_i^2=153, \\sum_{i=1}^{n}x_iy_i=126, \\bar{x}=4,\\bar{y}=4.6\\\\\n&S_{xx} = \\sum_{i=1}^{n}x_i^2 - n\\bar{x}^2 = 26, S_{yy} = \\sum_{i=1}^{n}y_i^2 - n\\bar{y}^2 = 47.2\\\\\n&S_{xy} = \\sum_{i=1}^{n}x_iy_i - n\\bar{x}\\bar{y} = 34\\\\\n\\\\\n&r = \\frac{Sxy}{\\sqrt{SxxSyy}} = \\frac{34}{\\sqrt{26 \\times 47.2}} = 0.97        \n\\end{align*}\\]"
  },
  {
    "objectID": "2-Correlation.html#notes-on-correlation",
    "href": "2-Correlation.html#notes-on-correlation",
    "title": "Causation and Correlation",
    "section": "Notes on correlation",
    "text": "Notes on correlation\n\nIf \\(x\\) and \\(y\\) measurement units are changed, correlation does not change.\nIf \\(x\\) and \\(y\\) are reversed, i.e. correlation of \\(y\\) and \\(x\\), the correlation does not change.\nCorrelation is a measure of linear association. It does not establish causation.\nTwo variables, x and y, could be highly correlated because there is another variable, z, having an impact on both x and y.\n\nExample, we have not established that extra height causes bigger feet. In fact genetic factors cause both."
  },
  {
    "objectID": "2-Correlation.html#example-change-of-scale-and-the-correlation-coefficient",
    "href": "2-Correlation.html#example-change-of-scale-and-the-correlation-coefficient",
    "title": "Causation and Correlation",
    "section": "Example: Change of scale and the correlation coefficient",
    "text": "Example: Change of scale and the correlation coefficient\nThe distance of the race and the time it took to complete was recorded for five races in kilometres and seconds respectively. The correlation was calculated between the two variables. The data set was also converted into miles (\\(\\times\\) 0.621371192) and minutes (/60) and the correlation was re-calculated.\n\n\n\n\n\nKilometres\nSeconds\nMiles\nMinutes\n\n\n\n\n0.1\n10\n0.0621371\n0.1666667\n\n\n0.4\n120\n0.2485485\n2.0000000\n\n\n0.8\n300\n0.4970970\n5.0000000\n\n\n1.6\n535\n0.9941939\n8.9166667\n\n\n3.0\n950\n1.8641136\n15.8333333"
  },
  {
    "objectID": "2-Correlation.html#explore-spurious-correlations",
    "href": "2-Correlation.html#explore-spurious-correlations",
    "title": "Causation and Correlation",
    "section": "Explore Spurious Correlations",
    "text": "Explore Spurious Correlations\nFind two correlated variables from: https://www.tylervigen.com/spurious-correlations.\nCreate a scatter plot and find the correlation."
  },
  {
    "objectID": "2-Correlation.html#association-between-pga-golfers-accuracy-and-driving-distance",
    "href": "2-Correlation.html#association-between-pga-golfers-accuracy-and-driving-distance",
    "title": "Causation and Correlation",
    "section": "Association between PGA golfer’s accuracy and driving distance",
    "text": "Association between PGA golfer’s accuracy and driving distance\nDescription:\nThe data set `golf’ was taken from PGA Tour Recordsof 195 golf rounds by PGA players in an attempt to explain what golf attributes contribute the most to low scores."
  },
  {
    "objectID": "2-Correlation.html#spurious-correlations",
    "href": "2-Correlation.html#spurious-correlations",
    "title": "Correlation (and Causation)",
    "section": "Spurious Correlations",
    "text": "Spurious Correlations\nA spurious correlation is a statistical relationship between two variables that appears to be meaningful but is actually caused by coincidence or the influence of a third (confounding) variable. This misleading association can arise due to random chance, indirect causation, or omitted variables.\nFor example, there may be a strong correlation between ice cream sales and drowning incidents, but this does not mean one causes the other. Instead, a third factor—hot weather—increases both ice cream sales and swimming activity, which in turn raises the risk of drowning.\nSpurious correlations can often be identified through deeper statistical analysis, such as controlling for confounding variables or using causal inference techniques.\nTask: Find two correlated variables from: https://www.tylervigen.com/spurious-correlations. Create a scatter plot and find the correlation."
  },
  {
    "objectID": "2-Correlation.html#what-is-the-capital-of-france",
    "href": "2-Correlation.html#what-is-the-capital-of-france",
    "title": "Causation and Correlation",
    "section": "What is the capital of France?",
    "text": "What is the capital of France?\n\nLondon\nParis\nBerlin\nMadrid"
  },
  {
    "objectID": "2-Correlation.html#example-data-1-association-between-pga-golfers-accuracy-and-driving-distance",
    "href": "2-Correlation.html#example-data-1-association-between-pga-golfers-accuracy-and-driving-distance",
    "title": "Causation and Correlation",
    "section": "Example Data (1): Association between PGA golfer’s accuracy and driving distance",
    "text": "Example Data (1): Association between PGA golfer’s accuracy and driving distance\nDescription:\nThe data set `golf’ was taken from PGA Tour Recordsof 195 golf rounds by PGA players in an attempt to explain what golf attributes contribute the most to low scores."
  },
  {
    "objectID": "2-Correlation.html#example-data-2-association-between-pga-golfers-accuracy-and-driving-distance",
    "href": "2-Correlation.html#example-data-2-association-between-pga-golfers-accuracy-and-driving-distance",
    "title": "Causation and Correlation",
    "section": "Example Data (2): Association between PGA golfer’s accuracy and driving distance",
    "text": "Example Data (2): Association between PGA golfer’s accuracy and driving distance"
  },
  {
    "objectID": "2-Correlation.html#example-data-1-what-is-the-association-between-pga-golfers-accuracy-and-driving-distance",
    "href": "2-Correlation.html#example-data-1-what-is-the-association-between-pga-golfers-accuracy-and-driving-distance",
    "title": "Correlation (and Causation)",
    "section": "Example Data (1): What is the association between PGA golfer’s accuracy and driving distance?",
    "text": "Example Data (1): What is the association between PGA golfer’s accuracy and driving distance?\nThe data set `golf’ was taken from PGA Tour Recordsof 195 golf rounds by PGA players in an attempt to explain what golf attributes contribute the most to low scores."
  },
  {
    "objectID": "2-Correlation.html#example-data-2-what-is-the-relationship-between-cars-weights-and-their-mileage",
    "href": "2-Correlation.html#example-data-2-what-is-the-relationship-between-cars-weights-and-their-mileage",
    "title": "Correlation (and Causation)",
    "section": "Example Data (2): What is the relationship between cars’ weights and their mileage?",
    "text": "Example Data (2): What is the relationship between cars’ weights and their mileage?\nThe data was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973–74 models)."
  },
  {
    "objectID": "0-Information.html#assessment-and-materials",
    "href": "0-Information.html#assessment-and-materials",
    "title": "Module Information",
    "section": "Assessment and Materials",
    "text": "Assessment and Materials\nNotes\n\nNotes will be a combination of links posted on Moodle and notes taken down in class\n\nAssignments\n\nWill be uploaded PDFs.\n\nLab quizzes\n\nWill be Moodle Quiz format with multiple attempts allowed until the end of the semester.\n\nMidterm Exam\n\nWill be a Moodle Quiz MCQ.\n\nMarks\n\n10% – compulsory assignments\n10% – quizzes\n15% – mid-term exam (March 28th, MCQ)\n65% – final exam (date TBC)\n\nRecommended reading\n\nR for Data Science (https://r4ds.had.co.nz)"
  },
  {
    "objectID": "0-Information.html#topics-covered",
    "href": "0-Information.html#topics-covered",
    "title": "Module Information",
    "section": "Topics Covered",
    "text": "Topics Covered\n\nTidyverse Recap\nCorrelation (and Causation)\nObservational Studies\nControlled Experiments\nSurveys\nData Privacy and Anonymisation\nSupervised and Unsupervised Learning\nPredictive Analytics\nImage and Text Analysis"
  },
  {
    "objectID": "0-Information.html#materials-and-assessments",
    "href": "0-Information.html#materials-and-assessments",
    "title": "Module Information",
    "section": "Materials and Assessments",
    "text": "Materials and Assessments\nNotes\n\nNotes will be a combination of links posted on Moodle and notes taken down in class\n\nTutorial Questions\n\nPractice tutorial questions will be posted to Moodle.\n\nAssignments\n\nAssignment questions will be posted to Moodle and answers should be uploaded as PDFs.\n\nLab Quizzes\n\nEach lab will have an associated quiz that will be a Moodle Quiz format with multiple attempts allowed until the end of the semester.\n\nMidterm Exam\n\nThe midterm exam will be a Moodle Quiz MCQ.\n\nMarks\n\n10% – compulsory assignments\n10% – quizzes\n15% – mid-term exam (March 28th, MCQ)\n65% – final exam (date TBC)\n\nRecommended reading\n\nR for Data Science (https://r4ds.had.co.nz)"
  },
  {
    "objectID": "3-MultipleRegression.html",
    "href": "3-MultipleRegression.html",
    "title": "Multiple Regression",
    "section": "",
    "text": "We have seen last semester, in DS151, how to study the relationship between two variables using linear regression.\nWe can create a linear regression model that includes a predictor, such that \\[\\hat{\\mbox{y}}=\\beta_0+\\beta_1\\mbox{x}\\]"
  },
  {
    "objectID": "3-MultipleRegression.html#recap-simple-regression",
    "href": "3-MultipleRegression.html#recap-simple-regression",
    "title": "Multiple Regression",
    "section": "",
    "text": "We have seen last semester, in DS151, how to study the relationship between two variables using linear regression.\nWe can create a linear regression model that includes a predictor, such that \\[\\hat{\\mbox{y}}=\\beta_0+\\beta_1\\mbox{x}\\]"
  },
  {
    "objectID": "3-MultipleRegression.html#multiple-regression-example-1-the-crab-dataset",
    "href": "3-MultipleRegression.html#multiple-regression-example-1-the-crab-dataset",
    "title": "Multiple Regression",
    "section": "Multiple Regression Example 1: The Crab Dataset",
    "text": "Multiple Regression Example 1: The Crab Dataset\nIn many research and applied fields, many different variables are measured at the same time, for various reasons. In many cases, we may actually have too many variables and may end up selecting a subset of them (only the most important) to proceed with the analysis. (NB: there are many ways to define variable importance, and we will look at some of them in depth throughout the Data Science programme).\nLet’s have a look at an example where we have many predictor variables. The crabs dataset from package MASS has 200 observations on 2 qualitative variables (species colour and sex), and 5 morphological measurements (frontal lobe size, rear width, carapace length and width, and body depth) of crabs of the species Leptograspus variegatus.\n\n\n\ncrab_dat &lt;- MASS::crabs\nglimpse(crab_dat)\n\nRows: 200\nColumns: 8\n$ sp    &lt;fct&gt; B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B…\n$ sex   &lt;fct&gt; M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M…\n$ index &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 1…\n$ FL    &lt;dbl&gt; 8.1, 8.8, 9.2, 9.6, 9.8, 10.8, 11.1, 11.6, 11.8, 11.8, 12.2, 12.…\n$ RW    &lt;dbl&gt; 6.7, 7.7, 7.8, 7.9, 8.0, 9.0, 9.9, 9.1, 9.6, 10.5, 10.8, 11.0, 1…\n$ CL    &lt;dbl&gt; 16.1, 18.1, 19.0, 20.1, 20.3, 23.0, 23.8, 24.5, 24.2, 25.2, 27.3…\n$ CW    &lt;dbl&gt; 19.0, 20.8, 22.4, 23.1, 23.0, 26.5, 27.1, 28.4, 27.8, 29.3, 31.6…\n$ BD    &lt;dbl&gt; 7.0, 7.4, 7.7, 8.2, 8.2, 9.8, 9.8, 10.4, 9.7, 10.3, 10.9, 11.4, …\n\nhead(crab_dat)\n\n  sp sex index   FL  RW   CL   CW  BD\n1  B   M     1  8.1 6.7 16.1 19.0 7.0\n2  B   M     2  8.8 7.7 18.1 20.8 7.4\n3  B   M     3  9.2 7.8 19.0 22.4 7.7\n4  B   M     4  9.6 7.9 20.1 23.1 8.2\n5  B   M     5  9.8 8.0 20.3 23.0 8.2\n6  B   M     6 10.8 9.0 23.0 26.5 9.8\n\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\nlibrary(ggplot2)\nggpairs(crab_dat, columns = 4:8, ggplot2::aes(colour=sex)) +theme_bw()\n\n\n\n\n\n\n\n\nImagine we are interested in predicting the carapace length (CL) of this species of crab. We can create a linear regression model that includes multiple predictors, such as \\[\\hat{\\mbox{CL}}=\\beta_0+\\beta_1\\mbox{FL}+\\beta_2\\mbox{RW}+\\beta_3\\mbox{CW}+\\beta_4\\mbox{BD}\\] This model can be fitted in R by executing:\n\nfit1 &lt;- lm(CL ~ FL + RW + CW + BD, data = crab_dat)\nfit1 %&gt;% coef\n\n(Intercept)          FL          RW          CW          BD \n  0.3163364   0.2648933  -0.1778948   0.6402190   0.4714152 \n\n\nWe have that \\[\\hat{\\mbox{CL}}=0.32+0.26\\mbox{FL}-0.18\\mbox{RW}+0.64\\mbox{CW}+0.47\\mbox{BD}\\].\nWe can plot the actual observations versus the predicted ones to see how well we did:\n\ncrab_dat &lt;- crab_dat %&gt;% \n              mutate(CL_pred = predict(fit1))\n\nggplot(data = crab_dat,\n       mapping = aes(x = CL, y = CL_pred, colour = sex)) +\n  theme_bw() +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  xlab(\"Observed CL\") + \n  ylab(\"Predicted CL\")\n\n\n\n\n\n\n\n\nHow well would we have done if we only used one predictor variable, say RW?\n\nfit2 &lt;- lm(CL ~ RW, data = crab_dat)\n\ncrab_dat &lt;- crab_dat %&gt;% \n              mutate(CL_pred2 = predict(fit2))\n\nggplot(data = crab_dat,\n       mapping = aes(x = CL, y = CL_pred2, colour = sex)) +\n  theme_bw() +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  xlab(\"Observed CL\") + \n  ylab(\"Predicted CL\")\n\n\n\n\n\n\n\n\nNow what about the categorical variables, namely sex and sp (species colour)? We can also include them in our model. Because they are categorical, they will only influence the overall mean response for each category, and hence we don’t estimate a slope for them, i.e. \\[\\hat{\\mbox{CL}}=\\beta_0+\\beta_1\\mbox{FL}+\\beta_2\\mbox{RW}+\\beta_3\\mbox{CW}+\\beta_4\\mbox{BD}+\\beta_5I(\\mbox{sp}=\\mbox{O})+\\beta_6I(\\mbox{sex}=\\mbox{M})\\] (NB: \\(I(\\mbox{sex}=\\mbox{M})\\) is an indicator function, which is equal to 1 if sex is equal to M and zero otherwise. The same type of interpretation can be drawn from \\(I(\\mbox{sp}=\\mbox{O})\\))\n\nfit3 &lt;- lm(CL ~ FL + RW + CW + BD + sp + sex, data = crab_dat)\nfit3 %&gt;% coef\n\n(Intercept)          FL          RW          CW          BD         spO \n-0.19583237  0.21392757 -0.03750446  0.65118521  0.38956671  0.16389052 \n       sexM \n 0.37020979 \n\ncrab_dat &lt;- crab_dat %&gt;% \n              mutate(CL_pred3 = predict(fit3))\n\nggplot(data = crab_dat,\n       mapping = aes(x = CL, y = CL_pred3, colour = sex)) +\n  theme_bw() +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  xlab(\"Observed CL\") + \n  ylab(\"Predicted CL\")\n\n\n\n\n\n\n\n\nHow do we interpret the coefficients related to sp and sex? They are the increase (or decrease, if negative) in the overall mean if a crab is orange (O) or male (M).\nOne way of comparing how well our modelling strategies did in terms of predictions is to sum the squared discrepancies (residuals) between predicted and observed values, and see which one is smaller.\n\ndiscrepancy_fit1 &lt;- sum(residuals(fit1)^2)\ndiscrepancy_fit2 &lt;- sum(residuals(fit2)^2)\ndiscrepancy_fit3 &lt;- sum(residuals(fit3)^2)\n\ndiscrepancy_fit1\n\n[1] 27.35058\n\ndiscrepancy_fit2\n\n[1] 2047.417\n\ndiscrepancy_fit3\n\n[1] 25.06863\n\n\nTypically we would split the data into training and test set, use only the training set to fit the model, and then perform this computation on both sets. We will see more details on how to compare the models in terms of their predictive power in the module Statistical Machine Learning. We will also see more details on this in the modules Linear Models I and II, how to properly test hypotheses, how to assess goodness-of-fit, what the important assumptions are and how to properly check them.\n\nLogistic Regression Example 2: The Iris Dataset\nWe explored this dataset initially last semester in DS151.\nIris is a genus of about 300 species of flowering plants, taking its name from the Greek word for a rainbow (which is also the name for the Greek goddess of rainbows, Iris). The flowers are very showy, and we would like to know if it is possible to identify some of the species based on measurements of different parts of the flowers.\nLet’s look at a famous example dataset that gives the measurements in centimeters of the variables:\nfor 50 flowers from each of three species if Iris: Iris setosa, Iris versicolor, and Iris virginica.\nThis dataset is available in base R as iris. Let’s have a look:\n\nlibrary(tidyverse)\nglimpse(iris)\n\nRows: 150\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n\nLet’s work, initially, only with species versicolor and virginica. We create a plot that shows in the \\(x\\) axis the petal length, and in the \\(y\\) axis only the values of 0 and 1, representing whether the observation belongs to class virginica or not (0 = the observation belongs to class versicolor; 1 = it belongs to class virginica). We call this a binary (or dummy) variable.\n\niris2 &lt;- iris %&gt;%\n  filter(Species != \"setosa\") %&gt;%\n  mutate(Species.binary = as.numeric(Species == \"virginica\"))\n  \nggplot(iris2, aes(x = Petal.Length, y = Species.binary)) +\n  geom_point() +\n  theme_bw()\n\n\n\n\n\n\n\n\nIt appears that as the petal length increases, so does the likelihood of belonging to the virginica class. We may interpret the numbers 0 and 1 here as the proportion \\(p\\) of plants that belong to class virginica and present a specific measurement of petal length.\nRemember. We transform the scale of the response in such a way that it will be bounded between 0 and 1, and hence our estimates will be sensical. The transformation we use is called the logit, and is the natural logarithm of the odds of belonging to class virginica: \\[\\displaystyle\\log\\left(\\frac{p}{1-p}\\right)=\\beta_0+\\beta_1*\\mbox{Petal.Length},\\] where \\(p\\) is the proportion of plants that belong to class virginica. This way, the estimated proportions will always be bounded between 0 and 1, and are now suitable to our problem. Let’s see how it looks like for our example:\n\nggplot(iris2, aes(x = Petal.Length, y = Species.binary)) +\n  geom_point() +\n  geom_smooth(method = glm, method.args = list(family = \"binomial\"), se = FALSE) +\n  theme_bw()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nIncluding more predictors\nIt is frequently of interest to use multiple covariates to improve our predictive power. Let’s fit a logistic regression model including all four covariates as predictors in our model: \\[\\displaystyle\\log\\left(\\frac{p}{1-p}\\right)=\\beta_0+\\beta_1*\\mbox{Sepal.Length}+\\beta_2*\\mbox{Sepal.Width}+\\beta_3*\\mbox{Petal.Length}+\\beta_4*\\mbox{Petal.Width}\\]\n\nfull_logistic_reg &lt;- glm(Species.binary ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width,\n                         family = binomial, data = iris2)\nfull_logistic_reg %&gt;% coef %&gt;% round(digits = 4)\n\n (Intercept) Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n    -42.6378      -2.4652      -6.6809       9.4294      18.2861 \n\n\nHow well did we do?\n\niris_predict &lt;- iris2 %&gt;% \n                  mutate(p_hat = predict(full_logistic_reg, type = \"response\") %&gt;% round(2),\n                         Species_pred = ifelse(p_hat &gt;= 0.5,\"virginica\",\"versicolor\"))\n\nn_correct_full &lt;- sum(iris_predict$Species_pred == iris_predict$Species)\nn_correct_full\n\n[1] 98\n\n\nOur model now correctly predicts the class of 98 out of 100 observations."
  },
  {
    "objectID": "3-MultipleRegression.html#example-3-the-wine-dataset",
    "href": "3-MultipleRegression.html#example-3-the-wine-dataset",
    "title": "Multiple Regression",
    "section": "Example 3: The Wine Dataset",
    "text": "Example 3: The Wine Dataset"
  },
  {
    "objectID": "3-MultipleRegression.html#logistic-regression-the-wine-dataset",
    "href": "3-MultipleRegression.html#logistic-regression-the-wine-dataset",
    "title": "Multiple Regression",
    "section": "Logistic Regression: The Wine dataset",
    "text": "Logistic Regression: The Wine dataset\nThe wine dataset contains the results of a chemical analysis of wines grown in a specific area of Italy. Three types of wine are represented in the 178 samples, with the results of 13 chemical analyses recorded for each sample. The Type variable has been transformed into a categoric variable.\nThe data contains no missing values and consits of only numeric data, with a three class target variable (Type) for classification.\nLet’s have a glimpse at the dataset:\n\nwine &lt;- read_csv(\"https://www.dropbox.com/s/l5x4ur06gfhpg0h/wine.csv?raw=1\") \n\nRows: 178 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): Type\ndbl (13): Alcohol, Malic, Ash, Alcalinity, Magnesium, Phenols, Flavanoids, N...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(wine)\n\nRows: 178\nColumns: 14\n$ Type                 &lt;chr&gt; \"type1\", \"type1\", \"type1\", \"type1\", \"type1\", \"typ…\n$ Alcohol              &lt;dbl&gt; 14.23, 13.20, 13.16, 14.37, 13.24, 14.20, 14.39, …\n$ Malic                &lt;dbl&gt; 1.71, 1.78, 2.36, 1.95, 2.59, 1.76, 1.87, 2.15, 1…\n$ Ash                  &lt;dbl&gt; 2.43, 2.14, 2.67, 2.50, 2.87, 2.45, 2.45, 2.61, 2…\n$ Alcalinity           &lt;dbl&gt; 15.6, 11.2, 18.6, 16.8, 21.0, 15.2, 14.6, 17.6, 1…\n$ Magnesium            &lt;dbl&gt; 127, 100, 101, 113, 118, 112, 96, 121, 97, 98, 10…\n$ Phenols              &lt;dbl&gt; 2.80, 2.65, 2.80, 3.85, 2.80, 3.27, 2.50, 2.60, 2…\n$ Flavanoids           &lt;dbl&gt; 3.06, 2.76, 3.24, 3.49, 2.69, 3.39, 2.52, 2.51, 2…\n$ Nonflavanoid.phenols &lt;dbl&gt; 0.28, 0.26, 0.30, 0.24, 0.39, 0.34, 0.30, 0.31, 0…\n$ Proanth              &lt;dbl&gt; 2.29, 1.28, 2.81, 2.18, 1.82, 1.97, 1.98, 1.25, 1…\n$ Color                &lt;dbl&gt; 5.64, 4.38, 5.68, 7.80, 4.32, 6.75, 5.25, 5.05, 5…\n$ Hue                  &lt;dbl&gt; 1.04, 1.05, 1.03, 0.86, 1.04, 1.05, 1.02, 1.06, 1…\n$ OD                   &lt;dbl&gt; 3.92, 3.40, 3.17, 3.45, 2.93, 2.85, 3.58, 3.58, 2…\n$ Proline              &lt;dbl&gt; 1065, 1050, 1185, 1480, 735, 1450, 1290, 1295, 10…\n\n\nChange the code below and make different plots with other covariate combinations. What sort of patterns begin to emerge?\n\nggplot(wine, aes(x = Ash, y = Malic, colour = Type)) +\n  geom_point() +\n  theme_bw() +\n  labs(colour = \"Wine Type\")\n\n\n\n\n\n\n\n\nWe will work only with types 1 and 2\n\nwine2 &lt;- wine %&gt;%\n  filter(Type != \"type3\") %&gt;%\n  mutate(Type_binary = as.numeric(Type == \"type1\")) %&gt;% \n  as_tibble\nwine2\n\n# A tibble: 130 × 15\n   Type  Alcohol Malic   Ash Alcalinity Magnesium Phenols Flavanoids\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1 type1    14.2  1.71  2.43       15.6       127    2.8        3.06\n 2 type1    13.2  1.78  2.14       11.2       100    2.65       2.76\n 3 type1    13.2  2.36  2.67       18.6       101    2.8        3.24\n 4 type1    14.4  1.95  2.5        16.8       113    3.85       3.49\n 5 type1    13.2  2.59  2.87       21         118    2.8        2.69\n 6 type1    14.2  1.76  2.45       15.2       112    3.27       3.39\n 7 type1    14.4  1.87  2.45       14.6        96    2.5        2.52\n 8 type1    14.1  2.15  2.61       17.6       121    2.6        2.51\n 9 type1    14.8  1.64  2.17       14          97    2.8        2.98\n10 type1    13.9  1.35  2.27       16          98    2.98       3.15\n# ℹ 120 more rows\n# ℹ 7 more variables: Nonflavanoid.phenols &lt;dbl&gt;, Proanth &lt;dbl&gt;, Color &lt;dbl&gt;,\n#   Hue &lt;dbl&gt;, OD &lt;dbl&gt;, Proline &lt;dbl&gt;, Type_binary &lt;dbl&gt;\n\n\nChange the code below and fit a logistic regression model using the four predictors you believe are the best to classify the two types of wine. Also play around with different thresholds for the classification rule. Compare with your peers. What predictors yielded the best predictive performance?\n\n# include the predictors below\nlogistic_reg &lt;- glm(Type_binary ~ Malic + Ash + Alcalinity + Magnesium,\n                           family = binomial, data = wine2)\n\n# set the threshold for the classification rule below\nthreshold &lt;- 0.50\n\n# computes the percentage of correct predictions\nwine_predict &lt;- wine2 %&gt;% \n                  mutate(p_hat = predict(logistic_reg, type = \"response\") %&gt;% round(2),\n                         Type_pred = ifelse(p_hat &gt;= threshold, \"type1\",\"type2\")) \n\n\nn_correct &lt;- sum(wine_predict$Type_pred == wine_predict$Type)\nn_total &lt;- nrow(wine2)\npercentage_correct &lt;- n_correct/n_total * 100\npercentage_correct\n\n[1] 86.92308"
  },
  {
    "objectID": "3-MultipleRegression.html#modeling-cars",
    "href": "3-MultipleRegression.html#modeling-cars",
    "title": "Multiple Regression",
    "section": "Modeling cars",
    "text": "Modeling cars\n\n\n\n\n\n\n\n\n\n\nDescribe: What is the relationship between cars’ weights and their mileage?\n\n\n\n\n\n\n\n\n\n\n\nPredict: What is your best guess for a car’s MPG that weighs 3,500 pounds?\n\n\n\nWarning in geom_segment(aes(x = 3.5, xend = 3.5, y = -Inf, yend = 18.5), : All aesthetics have length 1, but the data has 32 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = -Inf, xend = 3.5, y = 18.5, yend = 18.5), : All aesthetics have length 1, but the data has 32 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row."
  },
  {
    "objectID": "3-MultipleRegression.html#modelling-cars",
    "href": "3-MultipleRegression.html#modelling-cars",
    "title": "Multiple Regression",
    "section": "Modelling cars",
    "text": "Modelling cars\n\nDescribe: What is the relationship between cars’ weights and their mileage?"
  },
  {
    "objectID": "3-MultipleRegression.html#modelling-cars-1",
    "href": "3-MultipleRegression.html#modelling-cars-1",
    "title": "Multiple Regression",
    "section": "Modelling cars",
    "text": "Modelling cars\n\nPredict: What is your best guess for a car’s MPG that weighs 3,500 pounds?\n\n\n\nWarning in geom_segment(aes(x = 3.5, xend = 3.5, y = -Inf, yend = 18.5), : All aesthetics have length 1, but the data has 32 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = -Inf, xend = 3.5, y = 18.5, yend = 18.5), : All aesthetics have length 1, but the data has 32 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row."
  },
  {
    "objectID": "3-MultipleRegression.html#modelling",
    "href": "3-MultipleRegression.html#modelling",
    "title": "Multiple Regression",
    "section": "Modelling",
    "text": "Modelling\n\nUse models to explain the relationship between variables and to make predictions\nFor now we will focus on linear models (but there are many many other types of models too!)"
  },
  {
    "objectID": "3-MultipleRegression.html#modelling-vocabulary",
    "href": "3-MultipleRegression.html#modelling-vocabulary",
    "title": "Multiple Regression",
    "section": "Modelling vocabulary",
    "text": "Modelling vocabulary\n\nPredictor (explanatory variable)\nOutcome (response variable)\nRegression line\n\nSlope\nIntercept\n\n\n\nPredictor (explanatory variable)\n\n\n\n\n\n\n\n\n\n\nmpg\nwt\n\n\n\n\n21\n2.62\n\n\n21\n2.875\n\n\n22.8\n2.32\n\n\n21.4\n3.215\n\n\n18.7\n3.44\n\n\n18.1\n3.46\n\n\n...\n...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutcome (response variable)\n\n\n\n\n\n\n\n\n\n\nmpg\nwt\n\n\n\n\n21\n2.62\n\n\n21\n2.875\n\n\n22.8\n2.32\n\n\n21.4\n3.215\n\n\n18.7\n3.44\n\n\n18.1\n3.46\n\n\n...\n...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression line\n\n\n\n\n\n\n\n\n\n\n\nRegression line: slope\n\n\n\n\n\n\n\n\n\n\n\nRegression line: intercept"
  },
  {
    "objectID": "3-MultipleRegression.html#predictor-explanatory-variable",
    "href": "3-MultipleRegression.html#predictor-explanatory-variable",
    "title": "Multiple Regression",
    "section": "Predictor (explanatory variable)",
    "text": "Predictor (explanatory variable)\n\n\n\n\n\n\n\n\n\n\nmpg\nwt\n\n\n\n\n21\n2.62\n\n\n21\n2.875\n\n\n22.8\n2.32\n\n\n21.4\n3.215\n\n\n18.7\n3.44\n\n\n18.1\n3.46\n\n\n...\n..."
  },
  {
    "objectID": "3-MultipleRegression.html#outcome-response-variable",
    "href": "3-MultipleRegression.html#outcome-response-variable",
    "title": "Multiple Regression",
    "section": "Outcome (response variable)",
    "text": "Outcome (response variable)\n\n\n\n\n\n\n\n\n\n\nmpg\nwt\n\n\n\n\n21\n2.62\n\n\n21\n2.875\n\n\n22.8\n2.32\n\n\n21.4\n3.215\n\n\n18.7\n3.44\n\n\n18.1\n3.46\n\n\n...\n..."
  },
  {
    "objectID": "3-MultipleRegression.html#correlation-1",
    "href": "3-MultipleRegression.html#correlation-1",
    "title": "Multiple Regression",
    "section": "Correlation",
    "text": "Correlation\n\nRanges between -1 and 1.\nSame sign as the slope.\n\nWe have seen last semester, in DS151, how to study the relationship between two variables using linear regression. See, e.g. the mammals dataset:\n\nlibrary(tidyverse)\nmammals &lt;- read_csv(\"https://www.dropbox.com/s/rb3zd0nk4430tbv/mammals.csv?raw=1\")\nmammals\n\n# A tibble: 62 × 3\n   name               body brain\n   &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt;\n 1 Arctic fox        3.38   44.5\n 2 Owl monkey        0.48   15.5\n 3 Mountain beaver   1.35    8.1\n 4 Cow             465     423  \n 5 Grey wolf        36.3   120. \n 6 Goat             27.7   115  \n 7 Roe deer         14.8    98.2\n 8 Guinea pig        1.04    5.5\n 9 Verbet            4.19   58  \n10 Chinchilla        0.425   6.4\n# ℹ 52 more rows\n\n\n\n\nLet’s have a look at an exploratory plot, using brain weight as the response variable and body weight as the predictor variable:\n\nggplot(data = mammals,\n       mapping = aes(y = brain, x = body)) +\n  theme_bw() +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nLet’s label the points:\n\nggplot(data = mammals,\n       mapping = aes(y = brain, x = body, label = name)) +\n  theme_bw() +\n  geom_point() +\n  geom_label()\n\n\n\n\n\n\n\n\nIt is difficult to see the clump of points at the bottom left part of the plot. Maybe it is better to take the logarithm of the two variables to visualise them –\n\nggplot(data = mammals,\n       mapping = aes(y = log(brain), x = log(body), label = name)) +\n  theme_bw() +\n  geom_label()\n\n\n\n\n\n\n\n\nWe can overlay a linear regression line to the plot:\n\nggplot(data = mammals,\n       mapping = aes(y = log(brain), x = log(body))) +\n  theme_bw() +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe equation that predicts brain weight as a function of body weight is\n\nlm(log(brain) ~ log(body), data = mammals) %&gt;%\n  coef\n\n(Intercept)   log(body) \n  2.1347887   0.7516859 \n\n\n\\[\\log(\\mbox{brain weight}) = 2.13+0.75\\times\\log(\\mbox{body weight})\\]\nHowever, it is rare to find a dataset with only two variables. In many research and applied fields, many different variables are measured at the same time, for various reasons. In many cases, we may actually have too many variables and may end up selecting a subset of them (only the most important) to proceed with the analysis. (NB: there are many ways to define variable importance, and we will look at some of them in depth throughout the Data Science programme)."
  },
  {
    "objectID": "3-MultipleRegression.html#regression-line",
    "href": "3-MultipleRegression.html#regression-line",
    "title": "Multiple Regression",
    "section": "Regression line",
    "text": "Regression line"
  },
  {
    "objectID": "3-MultipleRegression.html#regression-line-slope",
    "href": "3-MultipleRegression.html#regression-line-slope",
    "title": "Multiple Regression",
    "section": "Regression line: slope",
    "text": "Regression line: slope"
  },
  {
    "objectID": "3-MultipleRegression.html#regression-line-intercept",
    "href": "3-MultipleRegression.html#regression-line-intercept",
    "title": "Multiple Regression",
    "section": "Regression line: intercept",
    "text": "Regression line: intercept"
  },
  {
    "objectID": "3-MultipleRegression.html#correlation",
    "href": "3-MultipleRegression.html#correlation",
    "title": "Multiple Regression",
    "section": "Correlation",
    "text": "Correlation"
  },
  {
    "objectID": "3-MultipleRegression.html#example-modeling-cars",
    "href": "3-MultipleRegression.html#example-modeling-cars",
    "title": "Multiple Regression",
    "section": "Example: Modeling cars",
    "text": "Example: Modeling cars\n\n\n\n\n\n\n\n\n\n\nDescribe: What is the relationship between cars’ weights and their mileage?\n\n\n\n\n\n\n\n\n\n\n\nPredict: What is your best guess for a car’s MPG that weighs 3,500 pounds?\n\n\n\n\n\n\n\n\n\n\n\nModelling vocabulary\n\nPredictor (explanatory variable)\nOutcome (response variable)\nRegression line\n\nSlope\nIntercept\n\n\n\nPredictor (explanatory variable)\n\n\n\n\n\n\n\n\n\n\nmpg\nwt\n\n\n\n\n21\n2.62\n\n\n21\n2.875\n\n\n22.8\n2.32\n\n\n21.4\n3.215\n\n\n18.7\n3.44\n\n\n18.1\n3.46\n\n\n...\n...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutcome (response variable)\n\n\n\n\n\n\n\n\n\n\nmpg\nwt\n\n\n\n\n21\n2.62\n\n\n21\n2.875\n\n\n22.8\n2.32\n\n\n21.4\n3.215\n\n\n18.7\n3.44\n\n\n18.1\n3.46\n\n\n...\n...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression line\n\n\n\n\n\n\n\n\n\n\n\nRegression line: slope\n\n\n\n\n\n\n\n\n\n\n\nRegression line: intercept\n\n\n\n\n\n\n\n\n\n\n\n\nR Code: Modeling cars\n\nmtcars_mod &lt;- lm(mpg ~ wt, data = mtcars)\nmtcars_mod\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nCoefficients:\n(Intercept)           wt  \n     37.285       -5.344  \n\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(\n    x = \"Weight (1,000 lbs)\",\n    y = \"Miles per gallon (MPG)\",\n    title = \"MPG vs. weights of cars\"\n  )"
  },
  {
    "objectID": "3-MultipleRegression.html#r-code",
    "href": "3-MultipleRegression.html#r-code",
    "title": "Multiple Regression",
    "section": "R Code",
    "text": "R Code\n\nmtcars_mod &lt;- lm(mpg ~ wt, data = mtcars)\nmtcars_mod\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nCoefficients:\n(Intercept)           wt  \n     37.285       -5.344  \n\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(\n    x = \"Weight (1,000 lbs)\",\n    y = \"Miles per gallon (MPG)\",\n    title = \"MPG vs. weights of cars\"\n  ) \n\n\n\n\n\n\n\n\nHowever, it is rare to find a dataset with only two variables. In many research and applied fields, many different variables are measured at the same time, for various reasons. In many cases, we may actually have too many variables and may end up selecting a subset of them (only the most important) to proceed with the analysis. (NB: there are many ways to define variable importance, and we will look at some of them in depth throughout the Data Science programme)."
  },
  {
    "objectID": "3-MultipleRegression.html#multiple-regression",
    "href": "3-MultipleRegression.html#multiple-regression",
    "title": "Multiple Regression",
    "section": "Multiple Regression",
    "text": "Multiple Regression\nIn many research and applied fields, many different variables are measured at the same time, for various reasons. In many cases, we may actually have too many variables and may end up selecting a subset of them (only the most important) to proceed with the analysis. (NB: there are many ways to define variable importance, and we will look at some of them in depth throughout the Data Science programme)."
  },
  {
    "objectID": "3-MultipleRegression.html#example-1-the-crab-dataset",
    "href": "3-MultipleRegression.html#example-1-the-crab-dataset",
    "title": "Multiple Regression",
    "section": "Example 1: The Crab Dataset",
    "text": "Example 1: The Crab Dataset\nLet’s have a look at an example where we have many predictor variables. The crabs dataset from package MASS has 200 observations on 2 qualitative variables (species colour and sex), and 5 morphological measurements (frontal lobe size, rear width, carapace length and width, and body depth) of crabs of the species Leptograspus variegatus.\n\n\n\nExploratory Analysis\n\ncrab_dat &lt;- MASS::crabs\nglimpse(crab_dat)\n\nRows: 200\nColumns: 8\n$ sp    &lt;fct&gt; B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B…\n$ sex   &lt;fct&gt; M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M…\n$ index &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 1…\n$ FL    &lt;dbl&gt; 8.1, 8.8, 9.2, 9.6, 9.8, 10.8, 11.1, 11.6, 11.8, 11.8, 12.2, 12.…\n$ RW    &lt;dbl&gt; 6.7, 7.7, 7.8, 7.9, 8.0, 9.0, 9.9, 9.1, 9.6, 10.5, 10.8, 11.0, 1…\n$ CL    &lt;dbl&gt; 16.1, 18.1, 19.0, 20.1, 20.3, 23.0, 23.8, 24.5, 24.2, 25.2, 27.3…\n$ CW    &lt;dbl&gt; 19.0, 20.8, 22.4, 23.1, 23.0, 26.5, 27.1, 28.4, 27.8, 29.3, 31.6…\n$ BD    &lt;dbl&gt; 7.0, 7.4, 7.7, 8.2, 8.2, 9.8, 9.8, 10.4, 9.7, 10.3, 10.9, 11.4, …\n\nlibrary(GGally)\nlibrary(ggplot2)\nggpairs(crab_dat, columns = 4:8, aes(colour=sex)) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nDescribe: What are the relationships between the variables?\n\n\n\nExploratory Modelling\nImagine we are interested in predicting the carapace length (CL) of this species of crab. We can create a linear regression model that includes multiple predictors, such as \\[\\hat{\\mbox{CL}}=\\beta_0+\\beta_1\\mbox{FL}+\\beta_2\\mbox{RW}+\\beta_3\\mbox{CW}+\\beta_4\\mbox{BD}\\]\nThis model can be fit in R by executing:\n\ncrab_mod1 &lt;- lm(CL ~ FL + RW + CW + BD, data = crab_dat)\ncrab_mod1\n\n\nCall:\nlm(formula = CL ~ FL + RW + CW + BD, data = crab_dat)\n\nCoefficients:\n(Intercept)           FL           RW           CW           BD  \n     0.3163       0.2649      -0.1779       0.6402       0.4714  \n\n\nWe have that \\[\\hat{\\mbox{CL}}=0.32+0.26\\mbox{FL}-0.18\\mbox{RW}+0.64\\mbox{CW}+0.47\\mbox{BD}\\].\nInterpretation:\n\nAs FL increases by one unit, holding the other predictors constant, then carapace length will increase by 0.26.\nAs RW increases by one unit, holding the other predictors constant, then carapace length will decrease by 0.18.\nAs CW increases by one unit, holding the other predictors constant, then carapace length will increase by 0.64.\nAs BD increases by one unit, holding the other predictors constant, then carapace length will increase by 0.47.\n\n\nQuestion: How well does the model do at predicting carapace length?\n\n\ncrab_dat &lt;- crab_dat %&gt;% \n              mutate(CL_pred = predict(crab_mod1))\n\nggplot(data = crab_dat,\n       mapping = aes(x = CL, y = CL_pred, colour = sex)) +\n  theme_bw() +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  xlab(\"Observed CL\") + \n  ylab(\"Predicted CL\")\n\n\n\n\n\n\n\n\n\nQuestion: How well would we have done if we only used one predictor variable, say RW?\n\n\ncrab_mod2 &lt;- lm(CL ~ RW, data = crab_dat)\ncrab_mod2\n\n\nCall:\nlm(formula = CL ~ RW, data = crab_dat)\n\nCoefficients:\n(Intercept)           RW  \n      0.645        2.470  \n\ncrab_dat &lt;- crab_dat %&gt;% \n              mutate(CL_pred2 = predict(crab_mod2))\n\nggplot(data = crab_dat,\n       mapping = aes(x = CL, y = CL_pred2, colour = sex)) +\n  theme_bw() +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  xlab(\"Observed CL\") + \n  ylab(\"Predicted CL\")\n\n\n\n\n\n\n\n\n\nQuestion: What about including the categorical variables, namely sex or sp (species colour)?\n\nWe can include categorical variables in our model, such that\n\\[\\hat{\\mbox{CL}}=\\beta_0+\\beta_1\\mbox{RW}+\\beta_2I(\\mbox{sex}=\\mbox{M})\\] (NB: \\(I(\\mbox{sex}=\\mbox{M})\\) is an indicator function, which is equal to 1 if sex is equal to M and zero otherwise. The same type of interpretation can be drawn from \\(I(\\mbox{sp}=\\mbox{O})\\))\n\ncrab_mod3 &lt;- lm(CL ~ RW + sex, data = crab_dat)\ncrab_mod3\n\n\nCall:\nlm(formula = CL ~ RW + sex, data = crab_dat)\n\nCoefficients:\n(Intercept)           RW         sexM  \n     -6.293        2.792        5.670  \n\ncrab_dat &lt;- crab_dat %&gt;% \n              mutate(CL_pred3 = predict(crab_mod3))\n\nggplot(data = crab_dat,\n       mapping = aes(x = CL, y = CL_pred3, colour = sex)) +\n  theme_bw() +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  xlab(\"Observed CL\") + \n  ylab(\"Predicted CL\")\n\n\n\n\n\n\n\n\n\nQuestion: How do we interpret the coefficients related to sex?\n\nFor a male crab the expected carapace length increases by 5.67 compared to a female crab.\n\n\nModel Comparison\nOne way of comparing how well our modelling strategies did in terms of predictions is to sum the squared discrepancies (residuals) between predicted and observed values, and see which one is smaller.\n\ndiscrepancy_mod1 &lt;- sum(residuals(crab_mod1)^2)\ndiscrepancy_mod2 &lt;- sum(residuals(crab_mod2)^2)\ndiscrepancy_mod3 &lt;- sum(residuals(crab_mod3)^2)\n\n\ndiscrepancy_mod1\n\n[1] 27.35058\n\ndiscrepancy_mod2\n\n[1] 2047.417\n\ndiscrepancy_mod3\n\n[1] 576.4921\n\n\nTypically we would split the data into training and test set, use only the training set to fit the model, and then perform this computation on both sets. We will see more details on how to compare the models in terms of their predictive power in the module Statistical Machine Learning. We will also see more details on this in the modules Linear Models I and II, how to properly test hypotheses, how to assess goodness-of-fit, what the important assumptions are and how to properly check them."
  },
  {
    "objectID": "3-MultipleRegression.html#logistic-regression",
    "href": "3-MultipleRegression.html#logistic-regression",
    "title": "Multiple Regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nLogistic regression is a generalized regression model where the outcome is a two-level categorical variable. The outcome, takes the value 1 or 0 with probability. Ultimately, it is the probability of the outcome taking the value 1 (i.e., being a “success”) that we model in relation to the predictor variables. For this to work, we transform the expected outcome in such a way that it will be bounded between 0 and 1, and hence our estimates will be sensical. The transformation we use is called the logit, and is the natural logarithm of the odds of success: \\[\\displaystyle\\log\\left(\\frac{p}{1-p}\\right)=\\beta_0+\\beta_1*\\mbox{x},\\] where \\(p\\) is the probability of success."
  },
  {
    "objectID": "3-MultipleRegression.html#example-2-the-iris-dataset",
    "href": "3-MultipleRegression.html#example-2-the-iris-dataset",
    "title": "Multiple Regression",
    "section": "Example 2: The Iris Dataset",
    "text": "Example 2: The Iris Dataset\nYou will have explored the Iris dataset initially last semester in DS151.\nIris is a genus of about 300 species of flowering plants, taking its name from the Greek word for a rainbow (which is also the name for the Greek goddess of rainbows, Iris). The flowers are very showy, and we would like to know if it is possible to identify some of the species based on measurements of different parts of the flowers.\nThe dataset gives the measurements in centimeters of the variables:\n\nsepal length\nsepal width\npetal length\npetal width\n\nfor 50 flowers from each of three species of Iris: Iris setosa, Iris versicolor, and Iris virginica.\nThis dataset is available in base R as iris. Let’s have a look:\n\nlibrary(tidyverse)\nglimpse(iris)\n\nRows: 150\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n\nLet’s work, initially, only with species versicolor and virginica. We create a plot that shows in the \\(x\\) axis the petal length, and in the \\(y\\) axis only the values of 0 and 1, representing whether the observation belongs to class virginica or not (0 = the observation belongs to class versicolor; 1 = it belongs to class virginica). We call this a binary (or dummy) variable.\n\niris2 &lt;- iris %&gt;%\n  filter(Species != \"setosa\") %&gt;%\n  mutate(Species.binary = as.numeric(Species == \"virginica\"))\n  \nggplot(iris2, aes(x = Petal.Length, y = Species.binary)) +\n  geom_point() +\n  theme_bw()\n\n\n\n\n\n\n\n\nWe may interpret the numbers 0 and 1 here as the probability \\(p\\) of belonging to class virginica. It appears that as the petal length increases, so does the likelihood of belonging to the virginica class.\n\nggplot(iris2, aes(x = Petal.Length, y = Species.binary)) +\n  geom_point() +\n  geom_smooth(method = glm, method.args = list(family = \"binomial\"), se = FALSE) +\n  theme_bw()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nIncluding more predictors\nIt is frequently of interest to use multiple covariates to improve our predictive power. Let’s fit a logistic regression model including all four covariates as predictors in our model: \\[\\displaystyle\\log\\left(\\frac{p}{1-p}\\right)=\\beta_0+\\beta_1*\\mbox{Sepal.Length}+\\beta_2*\\mbox{Sepal.Width}+\\beta_3*\\mbox{Petal.Length}+\\beta_4*\\mbox{Petal.Width}\\]\n\nfull_logistic_reg &lt;- glm(Species.binary ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width,\n                         family = binomial, data = iris2)\nfull_logistic_reg %&gt;% coef %&gt;% round(digits = 4)\n\n (Intercept) Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n    -42.6378      -2.4652      -6.6809       9.4294      18.2861 \n\n\nHow well did we do?\n\niris_predict &lt;- iris2 %&gt;% \n                  mutate(p_hat = predict(full_logistic_reg, type = \"response\") %&gt;% round(2),\n                         Species_pred = ifelse(p_hat &gt;= 0.5,\"virginica\",\"versicolor\"))\n\nn_correct_full &lt;- sum(iris_predict$Species_pred == iris_predict$Species)\nn_correct_full\n\n[1] 98\n\n\nOur model now correctly predicts the class of 98 out of 100 observations."
  },
  {
    "objectID": "3-MultipleRegression.html#example-the-iris-dataset",
    "href": "3-MultipleRegression.html#example-the-iris-dataset",
    "title": "Multiple Regression",
    "section": "Example: The Iris Dataset",
    "text": "Example: The Iris Dataset\nYou will have explored the Iris dataset initially last semester in DS151.\nIris is a genus of about 300 species of flowering plants, taking its name from the Greek word for a rainbow (which is also the name for the Greek goddess of rainbows, Iris). The flowers are very showy, and we would like to know if it is possible to identify some of the species based on measurements of different parts of the flowers.\nThe dataset gives the measurements in centimeters of the variables:\n\nsepal length\nsepal width\npetal length\npetal width\n\nfor 50 flowers from each of three species of Iris: Iris setosa, Iris versicolor, and Iris virginica.\nThis dataset is available in base R as iris. Let’s have a look:\n\nlibrary(tidyverse)\nglimpse(iris)\n\nRows: 150\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n\n\nExploratory Analysis\nLet’s work, initially, only with species versicolor and virginica. We create a plot that shows in the \\(x\\) axis the petal length, and in the \\(y\\) axis only the values of 0 and 1, representing whether the observation belongs to class virginica or not (0 = the observation belongs to class versicolor; 1 = it belongs to class virginica). We call this a binary (or dummy) variable.\n\niris2 &lt;- iris %&gt;%\n  filter(Species != \"setosa\") %&gt;%\n  mutate(Species.binary = as.numeric(Species == \"virginica\"))\n  \nggplot(iris2, aes(x = Petal.Length, y = Species.binary)) +\n  geom_point() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\nExploratory Modelling\nA simple logistic regression model can be fit, such that \\[\\displaystyle\\log\\left(\\frac{p}{1-p}\\right)=\\beta_0+\\beta_1*\\mbox{Petal.Length}\\]\nDo this in R by executing:\n\niris_mod1 &lt;- glm(Species.binary ~ Petal.Length,\n                 family = binomial,\n                 data = iris2)\niris_mod1 %&gt;% coef() %&gt;% round(digits = 4)\n\n (Intercept) Petal.Length \n    -43.7809       9.0020 \n\n\n\nggplot(iris2, aes(x = Petal.Length, y = Species.binary)) +\n  geom_point() +\n  geom_smooth(method = glm, method.args = list(family = \"binomial\"), se = FALSE) +\n  ylab(\"p\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nWe may interpret the y-axis numbers here as the probability \\(p\\) of belonging to class virginica. It appears that as the petal length increases, so does the likelihood of belonging to the virginica class.\n\n\nIncluding more predictors\nIt is frequently of interest to use multiple covariates to improve our predictive power. Let’s fit a logistic regression model including all four covariates as predictors in our model: \\[\\displaystyle\\log\\left(\\frac{p}{1-p}\\right)=\\beta_0+\\beta_1*\\mbox{Sepal.Length}+\\beta_2*\\mbox{Sepal.Width}+\\beta_3*\\mbox{Petal.Length}+\\beta_4*\\mbox{Petal.Width}\\]\n\niris_mod2 &lt;- glm(Species.binary ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width,\n                 family = binomial, \n                 data = iris2)\niris_mod2 %&gt;% coef %&gt;% round(digits = 4)\n\n (Intercept) Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n    -42.6378      -2.4652      -6.6809       9.4294      18.2861 \n\n\n\n\nClassification\n\nQuestion: How well did we do at classifying the Iris species?\n\n\niris_predict &lt;- iris2 %&gt;% \n                  mutate(p_hat = predict(iris_mod2, type = \"response\") %&gt;% round(2),\n                         Species_pred = ifelse(p_hat &gt;= 0.5,\"virginica\",\"versicolor\"))\n\nn_correct_full &lt;- sum(iris_predict$Species_pred == iris_predict$Species)\nn_correct_full\n\n[1] 98\n\n\nOur model correctly predicts the species class of 98 out of 100 observations."
  },
  {
    "objectID": "3-MultipleRegression.html#exploratory-analysis-1",
    "href": "3-MultipleRegression.html#exploratory-analysis-1",
    "title": "Multiple Regression",
    "section": "Exploratory Analysis",
    "text": "Exploratory Analysis\nLet’s work, initially, only with species versicolor and virginica. We create a plot that shows in the \\(x\\) axis the petal length, and in the \\(y\\) axis only the values of 0 and 1, representing whether the observation belongs to class virginica or not (0 = the observation belongs to class versicolor; 1 = it belongs to class virginica). We call this a binary (or dummy) variable.\n\niris2 &lt;- iris %&gt;%\n  filter(Species != \"setosa\") %&gt;%\n  mutate(Species.binary = as.numeric(Species == \"virginica\"))\n  \nggplot(iris2, aes(x = Petal.Length, y = Species.binary)) +\n  geom_point() +\n  theme_bw()"
  },
  {
    "objectID": "3-MultipleRegression.html#exploratory-modelling-1",
    "href": "3-MultipleRegression.html#exploratory-modelling-1",
    "title": "Multiple Regression",
    "section": "Exploratory Modelling",
    "text": "Exploratory Modelling\nA simple logistic regression model can be fit, such that \\[\\displaystyle\\log\\left(\\frac{p}{1-p}\\right)=\\beta_0+\\beta_1*\\mbox{Petal.Length}\\]\nDo this in R by executing:\n\niris_mod1 &lt;- glm(Species.binary ~ Petal.Length,\n                 family = binomial,\n                 data = iris2)\niris_mod1 %&gt;% coef() %&gt;% round(digits = 4)\n\n (Intercept) Petal.Length \n    -43.7809       9.0020 \n\n\n\nggplot(iris2, aes(x = Petal.Length, y = Species.binary)) +\n  geom_point() +\n  geom_smooth(method = glm, method.args = list(family = \"binomial\"), se = FALSE) +\n  ylab(\"p\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nWe may interpret the y-axis numbers here as the probability \\(p\\) of belonging to class virginica. It appears that as the petal length increases, so does the likelihood of belonging to the virginica class.\n\nIncluding more predictors\nIt is frequently of interest to use multiple covariates to improve our predictive power. Let’s fit a logistic regression model including all four covariates as predictors in our model: \\[\\displaystyle\\log\\left(\\frac{p}{1-p}\\right)=\\beta_0+\\beta_1*\\mbox{Sepal.Length}+\\beta_2*\\mbox{Sepal.Width}+\\beta_3*\\mbox{Petal.Length}+\\beta_4*\\mbox{Petal.Width}\\]\n\niris_mod2 &lt;- glm(Species.binary ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width,\n                 family = binomial, \n                 data = iris2)\niris_mod2 %&gt;% coef %&gt;% round(digits = 4)\n\n (Intercept) Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n    -42.6378      -2.4652      -6.6809       9.4294      18.2861 \n\n\n\n\nClassification\n\nQuestion: How well did we do at classifying the Iris species?\n\n\niris_predict &lt;- iris2 %&gt;% \n                  mutate(p_hat = predict(iris_mod2, type = \"response\") %&gt;% round(2),\n                         Species_pred = ifelse(p_hat &gt;= 0.5,\"virginica\",\"versicolor\"))\n\nn_correct_full &lt;- sum(iris_predict$Species_pred == iris_predict$Species)\nn_correct_full\n\n[1] 98\n\n\nOur model correctly predicts the species class of 98 out of 100 observations."
  },
  {
    "objectID": "3-MultipleRegression.html#example-the-wine-dataset",
    "href": "3-MultipleRegression.html#example-the-wine-dataset",
    "title": "Multiple Regression",
    "section": "Example: The Wine Dataset",
    "text": "Example: The Wine Dataset\nThe wine dataset contains the results of a chemical analysis of wines grown in a specific area of Italy. Three types of wine are represented in the 178 samples, with the results of 13 chemical analyses recorded for each sample. The Type variable has been transformed into a categoric variable.\nThe data contains no missing values and consits of only numeric data, with a three class target variable (Type) for classification.\nLet’s have a glimpse at the dataset:\n\nwine &lt;- read_csv(\"https://www.dropbox.com/s/l5x4ur06gfhpg0h/wine.csv?raw=1\") \n\nRows: 178 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): Type\ndbl (13): Alcohol, Malic, Ash, Alcalinity, Magnesium, Phenols, Flavanoids, N...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(wine)\n\nRows: 178\nColumns: 14\n$ Type                 &lt;chr&gt; \"type1\", \"type1\", \"type1\", \"type1\", \"type1\", \"typ…\n$ Alcohol              &lt;dbl&gt; 14.23, 13.20, 13.16, 14.37, 13.24, 14.20, 14.39, …\n$ Malic                &lt;dbl&gt; 1.71, 1.78, 2.36, 1.95, 2.59, 1.76, 1.87, 2.15, 1…\n$ Ash                  &lt;dbl&gt; 2.43, 2.14, 2.67, 2.50, 2.87, 2.45, 2.45, 2.61, 2…\n$ Alcalinity           &lt;dbl&gt; 15.6, 11.2, 18.6, 16.8, 21.0, 15.2, 14.6, 17.6, 1…\n$ Magnesium            &lt;dbl&gt; 127, 100, 101, 113, 118, 112, 96, 121, 97, 98, 10…\n$ Phenols              &lt;dbl&gt; 2.80, 2.65, 2.80, 3.85, 2.80, 3.27, 2.50, 2.60, 2…\n$ Flavanoids           &lt;dbl&gt; 3.06, 2.76, 3.24, 3.49, 2.69, 3.39, 2.52, 2.51, 2…\n$ Nonflavanoid.phenols &lt;dbl&gt; 0.28, 0.26, 0.30, 0.24, 0.39, 0.34, 0.30, 0.31, 0…\n$ Proanth              &lt;dbl&gt; 2.29, 1.28, 2.81, 2.18, 1.82, 1.97, 1.98, 1.25, 1…\n$ Color                &lt;dbl&gt; 5.64, 4.38, 5.68, 7.80, 4.32, 6.75, 5.25, 5.05, 5…\n$ Hue                  &lt;dbl&gt; 1.04, 1.05, 1.03, 0.86, 1.04, 1.05, 1.02, 1.06, 1…\n$ OD                   &lt;dbl&gt; 3.92, 3.40, 3.17, 3.45, 2.93, 2.85, 3.58, 3.58, 2…\n$ Proline              &lt;dbl&gt; 1065, 1050, 1185, 1480, 735, 1450, 1290, 1295, 10…\n\n\nChange the code below and make different plots with other covariate combinations. What sort of patterns begin to emerge?\n\nggplot(wine, aes(x = Ash, y = Malic, colour = Type)) +\n  geom_point() +\n  theme_bw() +\n  labs(colour = \"Wine Type\")\n\n\n\n\n\n\n\n\nWe will work only with types 1 and 2\n\nwine2 &lt;- wine %&gt;%\n  filter(Type != \"type3\") %&gt;%\n  mutate(Type_binary = as.numeric(Type == \"type1\")) %&gt;% \n  as_tibble\nwine2\n\n# A tibble: 130 × 15\n   Type  Alcohol Malic   Ash Alcalinity Magnesium Phenols Flavanoids\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1 type1    14.2  1.71  2.43       15.6       127    2.8        3.06\n 2 type1    13.2  1.78  2.14       11.2       100    2.65       2.76\n 3 type1    13.2  2.36  2.67       18.6       101    2.8        3.24\n 4 type1    14.4  1.95  2.5        16.8       113    3.85       3.49\n 5 type1    13.2  2.59  2.87       21         118    2.8        2.69\n 6 type1    14.2  1.76  2.45       15.2       112    3.27       3.39\n 7 type1    14.4  1.87  2.45       14.6        96    2.5        2.52\n 8 type1    14.1  2.15  2.61       17.6       121    2.6        2.51\n 9 type1    14.8  1.64  2.17       14          97    2.8        2.98\n10 type1    13.9  1.35  2.27       16          98    2.98       3.15\n# ℹ 120 more rows\n# ℹ 7 more variables: Nonflavanoid.phenols &lt;dbl&gt;, Proanth &lt;dbl&gt;, Color &lt;dbl&gt;,\n#   Hue &lt;dbl&gt;, OD &lt;dbl&gt;, Proline &lt;dbl&gt;, Type_binary &lt;dbl&gt;\n\n\nChange the code below and fit a logistic regression model using the four predictors you believe are the best to classify the two types of wine. Also play around with different thresholds for the classification rule. Compare with your peers. What predictors yielded the best predictive performance?\n\n# include the predictors below\nlogistic_reg &lt;- glm(Type_binary ~ Malic + Ash + Alcalinity + Magnesium,\n                           family = binomial, data = wine2)\n\n# set the threshold for the classification rule below\nthreshold &lt;- 0.50\n\n# computes the percentage of correct predictions\nwine_predict &lt;- wine2 %&gt;% \n                  mutate(p_hat = predict(logistic_reg, type = \"response\") %&gt;% round(2),\n                         Type_pred = ifelse(p_hat &gt;= threshold, \"type1\",\"type2\")) \n\n\nn_correct &lt;- sum(wine_predict$Type_pred == wine_predict$Type)\nn_total &lt;- nrow(wine2)\npercentage_correct &lt;- n_correct/n_total * 100\npercentage_correct\n\n[1] 86.92308"
  },
  {
    "objectID": "3-MultipleRegression.html#example-the-crab-dataset",
    "href": "3-MultipleRegression.html#example-the-crab-dataset",
    "title": "Multiple Regression",
    "section": "Example: The Crab Dataset",
    "text": "Example: The Crab Dataset\nLet’s have a look at an example where we have many predictor variables. The crabs dataset from package MASS has 200 observations on 2 qualitative variables (species colour and sex), and 5 morphological measurements (frontal lobe size, rear width, carapace length and width, and body depth) of crabs of the species Leptograspus variegatus.\n\n\n\nExploratory Analysis\n\ncrab_dat &lt;- MASS::crabs\nglimpse(crab_dat)\n\nRows: 200\nColumns: 8\n$ sp    &lt;fct&gt; B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B…\n$ sex   &lt;fct&gt; M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M…\n$ index &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 1…\n$ FL    &lt;dbl&gt; 8.1, 8.8, 9.2, 9.6, 9.8, 10.8, 11.1, 11.6, 11.8, 11.8, 12.2, 12.…\n$ RW    &lt;dbl&gt; 6.7, 7.7, 7.8, 7.9, 8.0, 9.0, 9.9, 9.1, 9.6, 10.5, 10.8, 11.0, 1…\n$ CL    &lt;dbl&gt; 16.1, 18.1, 19.0, 20.1, 20.3, 23.0, 23.8, 24.5, 24.2, 25.2, 27.3…\n$ CW    &lt;dbl&gt; 19.0, 20.8, 22.4, 23.1, 23.0, 26.5, 27.1, 28.4, 27.8, 29.3, 31.6…\n$ BD    &lt;dbl&gt; 7.0, 7.4, 7.7, 8.2, 8.2, 9.8, 9.8, 10.4, 9.7, 10.3, 10.9, 11.4, …\n\nlibrary(GGally)\nlibrary(ggplot2)\nggpairs(crab_dat, columns = 4:8, aes(colour=sex)) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nDescribe: What are the relationships between the variables?\n\n\n\nExploratory Modelling\nImagine we are interested in predicting the carapace length (CL) of this species of crab. We can create a linear regression model that includes multiple predictors, such as \\[\\hat{\\mbox{CL}}=\\beta_0+\\beta_1\\mbox{FL}+\\beta_2\\mbox{RW}+\\beta_3\\mbox{CW}+\\beta_4\\mbox{BD}\\]\nThis model can be fit in R by executing:\n\ncrab_mod1 &lt;- lm(CL ~ FL + RW + CW + BD, data = crab_dat)\ncrab_mod1\n\n\nCall:\nlm(formula = CL ~ FL + RW + CW + BD, data = crab_dat)\n\nCoefficients:\n(Intercept)           FL           RW           CW           BD  \n     0.3163       0.2649      -0.1779       0.6402       0.4714  \n\n\nWe have that \\[\\hat{\\mbox{CL}}=0.32+0.26\\mbox{FL}-0.18\\mbox{RW}+0.64\\mbox{CW}+0.47\\mbox{BD}\\].\nInterpretation:\n\nAs FL increases by one unit, holding the other predictors constant, then carapace length will increase by 0.26.\nAs RW increases by one unit, holding the other predictors constant, then carapace length will decrease by 0.18.\nAs CW increases by one unit, holding the other predictors constant, then carapace length will increase by 0.64.\nAs BD increases by one unit, holding the other predictors constant, then carapace length will increase by 0.47.\n\n\nQuestion: How well does the model do at predicting carapace length?\n\n\ncrab_dat &lt;- crab_dat %&gt;% \n              mutate(CL_pred = predict(crab_mod1))\n\nggplot(data = crab_dat,\n       mapping = aes(x = CL, y = CL_pred, colour = sex)) +\n  theme_bw() +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  xlab(\"Observed CL\") + \n  ylab(\"Predicted CL\")\n\n\n\n\n\n\n\n\n\nQuestion: How well would we have done if we only used one predictor variable, say RW?\n\n\ncrab_mod2 &lt;- lm(CL ~ RW, data = crab_dat)\ncrab_mod2\n\n\nCall:\nlm(formula = CL ~ RW, data = crab_dat)\n\nCoefficients:\n(Intercept)           RW  \n      0.645        2.470  \n\ncrab_dat &lt;- crab_dat %&gt;% \n              mutate(CL_pred2 = predict(crab_mod2))\n\nggplot(data = crab_dat,\n       mapping = aes(x = CL, y = CL_pred2, colour = sex)) +\n  theme_bw() +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  xlab(\"Observed CL\") + \n  ylab(\"Predicted CL\")\n\n\n\n\n\n\n\n\n\nQuestion: What about including the categorical variables, namely sex or sp (species colour)?\n\nWe can include categorical variables in our model, such that\n\\[\\hat{\\mbox{CL}}=\\beta_0+\\beta_1\\mbox{RW}+\\beta_2I(\\mbox{sex}=\\mbox{M})\\] (NB: \\(I(\\mbox{sex}=\\mbox{M})\\) is an indicator function, which is equal to 1 if sex is equal to M and zero otherwise. The same type of interpretation can be drawn from \\(I(\\mbox{sp}=\\mbox{O})\\))\n\ncrab_mod3 &lt;- lm(CL ~ RW + sex, data = crab_dat)\ncrab_mod3\n\n\nCall:\nlm(formula = CL ~ RW + sex, data = crab_dat)\n\nCoefficients:\n(Intercept)           RW         sexM  \n     -6.293        2.792        5.670  \n\ncrab_dat &lt;- crab_dat %&gt;% \n              mutate(CL_pred3 = predict(crab_mod3))\n\nggplot(data = crab_dat,\n       mapping = aes(x = CL, y = CL_pred3, colour = sex)) +\n  theme_bw() +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  xlab(\"Observed CL\") + \n  ylab(\"Predicted CL\")\n\n\n\n\n\n\n\n\n\nQuestion: How do we interpret the coefficients related to sex?\n\nFor a male crab the expected carapace length increases by 5.67 compared to a female crab.\n\n\nModel Comparison\nOne way of comparing how well our modelling strategies did in terms of predictions is to sum the squared discrepancies (residuals) between predicted and observed values, and see which one is smaller.\n\ndiscrepancy_mod1 &lt;- sum(residuals(crab_mod1)^2)\ndiscrepancy_mod2 &lt;- sum(residuals(crab_mod2)^2)\ndiscrepancy_mod3 &lt;- sum(residuals(crab_mod3)^2)\n\n\ndiscrepancy_mod1\n\n[1] 27.35058\n\ndiscrepancy_mod2\n\n[1] 2047.417\n\ndiscrepancy_mod3\n\n[1] 576.4921\n\n\nTypically we would split the data into training and test set, use only the training set to fit the model, and then perform this computation on both sets. We will see more details on how to compare the models in terms of their predictive power in the module Statistical Machine Learning. We will also see more details on this in the modules Linear Models I and II, how to properly test hypotheses, how to assess goodness-of-fit, what the important assumptions are and how to properly check them."
  }
]