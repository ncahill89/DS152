[
  {
    "objectID": "1-TidyverseRecap.html",
    "href": "1-TidyverseRecap.html",
    "title": "Tidyverse Recap",
    "section": "",
    "text": "The tidyverse is a collection of R packages designed for data science.\nAll packages share an underlying design philosophy, grammar, and data structures.\nIt emphasizes tidy data in data frames, performs operations one step at a time, connects with pipes and makes code human readable."
  },
  {
    "objectID": "1-TidyverseRecap.html#tidyr-pivot",
    "href": "1-TidyverseRecap.html#tidyr-pivot",
    "title": "Tidyverse Recap",
    "section": "tidyr: pivot",
    "text": "tidyr: pivot\n\n# Load the tidyr package\nlibrary(tidyr)\n\n\n# Assume we have a dataset 'data' with 'ID1', 'ID2', 'x', and 'y' columns\ndata_ex1 &lt;- tibble(ID1 = rep(LETTERS[1:4],times = 3), \n                   ID2 = rep(letters[1:3], each = 4), \n                   x = 1:12, \n                   y = 21:32)\n\nprint(data_ex1)\n\n# A tibble: 12 × 4\n   ID1   ID2       x     y\n   &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt;\n 1 A     a         1    21\n 2 B     a         2    22\n 3 C     a         3    23\n 4 D     a         4    24\n 5 A     b         5    25\n 6 B     b         6    26\n 7 C     b         7    27\n 8 D     b         8    28\n 9 A     c         9    29\n10 B     c        10    30\n11 C     c        11    31\n12 D     c        12    32\n\n# Use pivot_longer() to convert wide data to long format\ndata_long &lt;- data_ex1 %&gt;% pivot_longer(cols = c(\"x\", \"y\"), \n                                       names_to = \"Variable\", \n                                       values_to = \"Value\")\n\n# Print the long format data\nprint(data_long)\n\n# A tibble: 24 × 4\n   ID1   ID2   Variable Value\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;    &lt;int&gt;\n 1 A     a     x            1\n 2 A     a     y           21\n 3 B     a     x            2\n 4 B     a     y           22\n 5 C     a     x            3\n 6 C     a     y           23\n 7 D     a     x            4\n 8 D     a     y           24\n 9 A     b     x            5\n10 A     b     y           25\n# ℹ 14 more rows\n\n# Use pivot_wider() to convert long data back to wide format\ndata_wide &lt;- data_long %&gt;% pivot_wider(names_from = Variable,\n                                       values_from = Value)\n\n# Print the wide format data\nprint(data_wide)\n\n# A tibble: 12 × 4\n   ID1   ID2       x     y\n   &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt;\n 1 A     a         1    21\n 2 B     a         2    22\n 3 C     a         3    23\n 4 D     a         4    24\n 5 A     b         5    25\n 6 B     b         6    26\n 7 C     b         7    27\n 8 D     b         8    28\n 9 A     c         9    29\n10 B     c        10    30\n11 C     c        11    31\n12 D     c        12    32\n\n\nIn this example, pivot_longer is used to convert the wide format data to long format, where each row is a single observation associated with the variables ID1, ID2, Variable (containing the original column names ‘x’ and ‘y’), and Value (containing the values from ‘x’ and ‘y’ columns). We can then also convert back to wide format using pivot_wider."
  },
  {
    "objectID": "1-TidyverseRecap.html#tidyr-separate",
    "href": "1-TidyverseRecap.html#tidyr-separate",
    "title": "Tidyverse Recap",
    "section": "tidyr: separate",
    "text": "tidyr: separate\n\n# Load the tidyr package\nlibrary(tidyr)\n\n# Assume we have a dataset 'dataNew' with a 'datetime' column\ndata_ex2 &lt;- tibble(datetime = \n                    c(\"2016-01-01 07:30:29\", \"2016-01-02 09:43:36\", \"2016-01-03 13:59:00\"), \n                   event = c(\"u\", \"a\", \"l\"))\n\n# Use the separate() function from tidyr to separate the 'datetime' column into \n# 'date' and 'time'\n# Then separate 'time' into 'hour', 'min', 'second'\ndata_sep &lt;- data_ex2 %&gt;% \n              separate(datetime, c('date', 'time'), sep = ' ') %&gt;% \n              separate(time, c('hour', 'min', 'second'), sep = ':')\n\n# Print the new dataset\nprint(data_sep)\n\n# A tibble: 3 × 5\n  date       hour  min   second event\n  &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;\n1 2016-01-01 07    30    29     u    \n2 2016-01-02 09    43    36     a    \n3 2016-01-03 13    59    00     l    \n\n# change hour, min, second to numeric values\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ndata_sep %&gt;% mutate_at(vars(hour, min, second), as.numeric)\n\n# A tibble: 3 × 5\n  date        hour   min second event\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;\n1 2016-01-01     7    30     29 u    \n2 2016-01-02     9    43     36 a    \n3 2016-01-03    13    59      0 l"
  },
  {
    "objectID": "1-TidyverseRecap.html#example-dplyr",
    "href": "1-TidyverseRecap.html#example-dplyr",
    "title": "Tidyverse Recap",
    "section": "Example: dplyr",
    "text": "Example: dplyr\n\n# Load the dplyr package\nlibrary(dplyr)\n\n# Assume we have a dataset 'data' with 'ID', 'Age', 'Gender', and 'Income' columns\ndata_ex3 &lt;- tibble(ID = 1:4, \n                   Age = c(21, 35, 58, 40), \n                   Gender = c(\"Male\", \"Female\", \"Male\", \"Female\"), \n                   Income = c(50000, 80000, 120000, 75000))\n\n# Use select() to choose the 'ID' and 'Age' columns\nselected_data &lt;- data_ex3 %&gt;% select(ID, Age)\nselected_data\n\n# A tibble: 4 × 2\n     ID   Age\n  &lt;int&gt; &lt;dbl&gt;\n1     1    21\n2     2    35\n3     3    58\n4     4    40\n\n# Use filter() to get rows where 'Age' is greater than 30\nfiltered_data &lt;- data_ex3 %&gt;% filter(Age &gt; 30)\nfiltered_data\n\n# A tibble: 3 × 4\n     ID   Age Gender Income\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1     2    35 Female  80000\n2     3    58 Male   120000\n3     4    40 Female  75000\n\n# Use mutate() to create a new column 'IncomeInThousands'\nmutated_data &lt;- data_ex3 %&gt;% mutate(IncomeInThousands = Income / 1000)\nmutated_data\n\n# A tibble: 4 × 5\n     ID   Age Gender Income IncomeInThousands\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;             &lt;dbl&gt;\n1     1    21 Male    50000                50\n2     2    35 Female  80000                80\n3     3    58 Male   120000               120\n4     4    40 Female  75000                75\n\n# Use arrange() to sort data by 'Income'\narranged_data &lt;- data_ex3 %&gt;% arrange(Income)\narranged_data\n\n# A tibble: 4 × 4\n     ID   Age Gender Income\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1     1    21 Male    50000\n2     4    40 Female  75000\n3     2    35 Female  80000\n4     3    58 Male   120000\n\n# Use summarise() to get the mean 'Income'\nsummary_data &lt;- data_ex3 %&gt;% summarise(MeanIncome = mean(Income))\nsummary_data\n\n# A tibble: 1 × 1\n  MeanIncome\n       &lt;dbl&gt;\n1      81250\n\n# Use group_by() and summarise() to get the mean 'Income' for each 'Gender'\ngrouped_data &lt;- data_ex3 %&gt;% \n                  group_by(Gender) %&gt;% \n                  summarise(MeanIncome = mean(Income))\ngrouped_data\n\n# A tibble: 2 × 2\n  Gender MeanIncome\n  &lt;chr&gt;       &lt;dbl&gt;\n1 Female      77500\n2 Male        85000\n\n\nIn these examples, select is used to choose specific columns, filter is used to select rows based on a condition, mutate is used to create a new column, arrange is used to sort data, summarise is used to calculate summary statistics, and group_by is used to perform operations on groups of data."
  },
  {
    "objectID": "1-TidyverseRecap.html#example-ggplot",
    "href": "1-TidyverseRecap.html#example-ggplot",
    "title": "Tidyverse Recap",
    "section": "Example: ggplot",
    "text": "Example: ggplot"
  },
  {
    "objectID": "1-TidyverseRecap.html#question",
    "href": "1-TidyverseRecap.html#question",
    "title": "Tidyverse Recap",
    "section": "Question",
    "text": "Question\nSuppose we have a dataset called penguins and suppose we would like to study how the ratio of penguin body mass to flipper size differs across the species in the dataset. Rearrange the following steps in the pipeline into an order that accomplishes this goal.\n\n# a\narrange(avg_mass_flipper_ratio)\n\n# b\ngroup_by(species)\n\n# c\npenguins \n  \n# d\nsummarise(\n  avg_mass_flipper_ratioo = median(mass_flipper_ratio)\n)\n  \n# e\nmutate(\n  mass_flipper_ratio = body_mass_g/flipper_length_mm\n)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Introduction to Data Science (2)!",
    "section": "",
    "text": "Welcome to the course website for DS152 Introduction to Data Science (2).\nModule information\nLecture material (slides, notes, videos) are licensed under CC-BY-NC 4.0.\nContact: Niamh Cahill (niamh.cahill@mu.ie)"
  },
  {
    "objectID": "index.html#lecture-slides",
    "href": "index.html#lecture-slides",
    "title": "Welcome to Bayesian Data Analysis!",
    "section": "Lecture Slides",
    "text": "Lecture Slides\n\nWeek 1\n1a: Bayes Rule\n1b: Inferring a Binomial Probability\n\n\nWeek 2\n2a: Beta Binomial\n2b: Bayesian Inference\n\n\nWeek 3\n3: Single Parameter Normal\n\n\nWeek 4\n4: MCMC Sampling\n\n\nWeek 5\n5a: MCMC Diagnostics\n5b: Just Another Gibbs Sampler\n\n\nWeek 6\n6: Bayesian Linear Regression\n\n\nWeek 7\n7: Model Checking\n\n\nWeek 8\n8: Introducing Bayesian Hierarchical Modelling\n\n\nWeek 9\n9: Bayesian Hierarchical Regression Modelling\n\n\nWeek 10\n10: Bayesian Generalised Linear Models (GLMs)\n\n\nWeek 11\n11: Bayesian Hierarchical Modelling - GLM"
  },
  {
    "objectID": "index.html#tutorials",
    "href": "index.html#tutorials",
    "title": "Welcome to Bayesian Data Analysis!",
    "section": "Tutorials",
    "text": "Tutorials\nTutorial Sheet 1: Bayesian inference using binomial and Poisson models\n\nTutorial Sheet 2: Bayesian Model for Multiple Proportions - Email Campaign Click-Through Rates\nTutorial Sheet 3: Bayesian Regression Model - Fisherys Data\nTutorial Sheet 4: Bayesian Hierarchical Regression Modelling - Simulation"
  },
  {
    "objectID": "index.html#assignments",
    "href": "index.html#assignments",
    "title": "Welcome to Bayesian Data Analysis!",
    "section": "Assignments",
    "text": "Assignments\nAssignment 1: Bayesian Inference on Particle Emission Energy\nAssignment 2: Comparing Video Game Playing Among Students in 5 Different Countries\nAssignment 3: Football outcomes vs point spread\nAssignment 4: Radon Analysis"
  },
  {
    "objectID": "index.html#lecture-notes",
    "href": "index.html#lecture-notes",
    "title": "Welcome to Introduction to Data Science (2)!",
    "section": "Lecture Notes",
    "text": "Lecture Notes\n1: Tidyverse Recap\n2: Correlation (and Causation)\n3: Multiple Regression Models\n4: Sampling Principles and Strategies"
  },
  {
    "objectID": "0-Information.html#course-organisation",
    "href": "0-Information.html#course-organisation",
    "title": "Module Information",
    "section": "",
    "text": "Lecture and Lab Timetable\n\nMonday 11am (Lecture, CB8), Wednesday 2pm (Lecture, CH)\nTuesday 3pm (Lab, TSI239), Friday 9am (Lab, TSI239)\n\nLabs will start in week 2\nPlease confirm your choice on Moodle.\n\n\nTutorials\n\nTuesdays @ 9am, 10am, 2pm, 3pm, 5pm, Wednesdays @ 12pm, 1pm\nTutorials – starting week 4 (24/02/2025)\n\nOffice hours\n\nBy appointment"
  },
  {
    "objectID": "0-Information.html#course-organisation-1",
    "href": "0-Information.html#course-organisation-1",
    "title": "Module Information",
    "section": "Course organisation",
    "text": "Course organisation\nMarks\n\n10% – compulsory assignments\n10% – quizzes\n15% – mid-term exam (March 28th, MCQ)\n65% – final exam (date TBC)\n\nNotes\n\nNotes will be a combination of handouts posted on Moodle and notes taken down in class\n\nAssignments\n\nWill be uploaded PDFs\n\nRecommended reading\n\nR for Data Science (https://r4ds.had.co.nz)"
  },
  {
    "objectID": "0-Information.html#diversity-inclusion",
    "href": "0-Information.html#diversity-inclusion",
    "title": "Module Information",
    "section": "Diversity & inclusion",
    "text": "Diversity & inclusion\nIt is my intent to present materials and activities that are respectful of diversity: gender identity, sexuality, disability, age, socioeconomic status, ethnicity, race, nationality, religion, and culture. I may not always get this right so please let me know ways to improve the effectiveness of the course for you personally, or for other students or student groups.\nTo help with this:\n\nIf you have a name that differs from those that appear in your official University records, please let me know!\nPlease let me know your preferred pronouns if you wish to do so.\nI (like many people) am still in the process of learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please talk to me about it."
  },
  {
    "objectID": "0-Information.html",
    "href": "0-Information.html",
    "title": "Module Information",
    "section": "",
    "text": "Lecture and Lab Timetable\n\nMonday 11am (Lecture, CB8), Wednesday 2pm (Lecture, CH)\nTuesday 3pm (Lab, TSI239), Friday 9am (Lab, TSI239)\n\nLabs will start in week 2\nPlease confirm your choice on Moodle.\n\n\nTutorials\n\nTuesdays @ 9am, 10am, 2pm, 3pm, 5pm, Wednesdays @ 12pm, 1pm\nTutorials – starting week 4 (24/02/2025)\n\nOffice hours\n\nBy appointment"
  },
  {
    "objectID": "1-TidyverseRecap.html#what-is-tidyverse",
    "href": "1-TidyverseRecap.html#what-is-tidyverse",
    "title": "Tidyverse Recap",
    "section": "",
    "text": "The tidyverse is a collection of R packages designed for data science.\nAll packages share an underlying design philosophy, grammar, and data structures.\nIt emphasizes tidy data in data frames, performs operations one step at a time, connects with pipes and makes code human readable."
  },
  {
    "objectID": "1-TidyverseRecap.html#key-packages-in-tidyverse",
    "href": "1-TidyverseRecap.html#key-packages-in-tidyverse",
    "title": "Tidyverse Recap",
    "section": "Key Packages in tidyverse",
    "text": "Key Packages in tidyverse\n\nreadr: Used for importing data.\ntidyr: Used for tidying and reshaping data.\ndplyr: Used for data transformation.\nggplot2: Used for data visualization.\nmagrittr: Provides the pipe operator (%&gt;%) or (|&gt;) which is used to chain together sequences of operations."
  },
  {
    "objectID": "1-TidyverseRecap.html#importing-data-with-readr",
    "href": "1-TidyverseRecap.html#importing-data-with-readr",
    "title": "Tidyverse Recap",
    "section": "Importing Data with readr",
    "text": "Importing Data with readr\n\nreadr provides faster and consistent replacements for data import functions in base R.\nIt fits into the tidyverse naturally and extends neatly into other data types.\nExample: read_csv(file, show_col_types = FALSE)."
  },
  {
    "objectID": "1-TidyverseRecap.html#tidying-data-with-tidyr",
    "href": "1-TidyverseRecap.html#tidying-data-with-tidyr",
    "title": "Tidyverse Recap",
    "section": "Tidying Data with tidyr",
    "text": "Tidying Data with tidyr\n\ntidyr provides a set of functions that help to tidy data.\nTidy data is data where every column is a variable, every row is an observation, and every cell is a single value.\n\n\ntidyr: pivot\n\n# Load the tidyr package\nlibrary(tidyr)\n\n\n# Assume we have a dataset 'data' with 'ID1', 'ID2', 'x', and 'y' columns\ndata_ex1 &lt;- tibble(ID1 = rep(LETTERS[1:4],times = 3), \n                   ID2 = rep(letters[1:3], each = 4), \n                   x = 1:12, \n                   y = 21:32)\n\nprint(data_ex1)\n\n# A tibble: 12 × 4\n   ID1   ID2       x     y\n   &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt;\n 1 A     a         1    21\n 2 B     a         2    22\n 3 C     a         3    23\n 4 D     a         4    24\n 5 A     b         5    25\n 6 B     b         6    26\n 7 C     b         7    27\n 8 D     b         8    28\n 9 A     c         9    29\n10 B     c        10    30\n11 C     c        11    31\n12 D     c        12    32\n\n# Use pivot_longer() to convert wide data to long format\ndata_long &lt;- data_ex1 %&gt;% pivot_longer(cols = c(\"x\", \"y\"), \n                                       names_to = \"Variable\", \n                                       values_to = \"Value\")\n\n# Print the long format data\nprint(data_long)\n\n# A tibble: 24 × 4\n   ID1   ID2   Variable Value\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;    &lt;int&gt;\n 1 A     a     x            1\n 2 A     a     y           21\n 3 B     a     x            2\n 4 B     a     y           22\n 5 C     a     x            3\n 6 C     a     y           23\n 7 D     a     x            4\n 8 D     a     y           24\n 9 A     b     x            5\n10 A     b     y           25\n# ℹ 14 more rows\n\n# Use pivot_wider() to convert long data back to wide format\ndata_wide &lt;- data_long %&gt;% pivot_wider(names_from = Variable,\n                                       values_from = Value)\n\n# Print the wide format data\nprint(data_wide)\n\n# A tibble: 12 × 4\n   ID1   ID2       x     y\n   &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt;\n 1 A     a         1    21\n 2 B     a         2    22\n 3 C     a         3    23\n 4 D     a         4    24\n 5 A     b         5    25\n 6 B     b         6    26\n 7 C     b         7    27\n 8 D     b         8    28\n 9 A     c         9    29\n10 B     c        10    30\n11 C     c        11    31\n12 D     c        12    32\n\n\nIn this example, pivot_longer is used to convert the wide format data to long format, where each row is a single observation associated with the variables ID1, ID2, Variable (containing the original column names ‘x’ and ‘y’), and Value (containing the values from ‘x’ and ‘y’ columns). We can then also convert back to wide format using pivot_wider.\n\n\ntidyr: separate\n\n# Load the tidyr package\nlibrary(tidyr)\n\n# Assume we have a dataset 'dataNew' with a 'datetime' column\ndata_ex2 &lt;- tibble(datetime = \n                    c(\"2016-01-01 07:30:29\", \"2016-01-02 09:43:36\", \"2016-01-03 13:59:00\"), \n                   event = c(\"u\", \"a\", \"l\"))\n\n# Use the separate() function from tidyr to separate the 'datetime' column into \n# 'date' and 'time'\n# Then separate 'time' into 'hour', 'min', 'second'\ndata_sep &lt;- data_ex2 %&gt;% \n              separate(datetime, c('date', 'time'), sep = ' ') %&gt;% \n              separate(time, c('hour', 'min', 'second'), sep = ':')\n\n# Print the new dataset\nprint(data_sep)\n\n# A tibble: 3 × 5\n  date       hour  min   second event\n  &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;\n1 2016-01-01 07    30    29     u    \n2 2016-01-02 09    43    36     a    \n3 2016-01-03 13    59    00     l    \n\n# change hour, min, second to numeric values\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ndata_sep %&gt;% mutate_at(vars(hour, min, second), as.numeric)\n\n# A tibble: 3 × 5\n  date        hour   min second event\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;\n1 2016-01-01     7    30     29 u    \n2 2016-01-02     9    43     36 a    \n3 2016-01-03    13    59      0 l"
  },
  {
    "objectID": "1-TidyverseRecap.html#transforming-data-with-dplyr",
    "href": "1-TidyverseRecap.html#transforming-data-with-dplyr",
    "title": "Tidyverse Recap",
    "section": "Transforming Data with dplyr",
    "text": "Transforming Data with dplyr\n\ndplyr is a grammar of data manipulation, providing a consistent set of verbs that solve the most common data manipulation challenges.\nExample: filter(data, condition).\n\n\nExample: dplyr\n\n# Load the dplyr package\nlibrary(dplyr)\n\n# Assume we have a dataset 'data' with 'ID', 'Age', 'Gender', and 'Income' columns\ndata_ex3 &lt;- tibble(ID = 1:4, \n                   Age = c(21, 35, 58, 40), \n                   Gender = c(\"Male\", \"Female\", \"Male\", \"Female\"), \n                   Income = c(50000, 80000, 120000, 75000))\n\n# Use select() to choose the 'ID' and 'Age' columns\nselected_data &lt;- data_ex3 %&gt;% select(ID, Age)\nselected_data\n\n# A tibble: 4 × 2\n     ID   Age\n  &lt;int&gt; &lt;dbl&gt;\n1     1    21\n2     2    35\n3     3    58\n4     4    40\n\n# Use filter() to get rows where 'Age' is greater than 30\nfiltered_data &lt;- data_ex3 %&gt;% filter(Age &gt; 30)\nfiltered_data\n\n# A tibble: 3 × 4\n     ID   Age Gender Income\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1     2    35 Female  80000\n2     3    58 Male   120000\n3     4    40 Female  75000\n\n# Use mutate() to create a new column 'IncomeInThousands'\nmutated_data &lt;- data_ex3 %&gt;% mutate(IncomeInThousands = Income / 1000)\nmutated_data\n\n# A tibble: 4 × 5\n     ID   Age Gender Income IncomeInThousands\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;             &lt;dbl&gt;\n1     1    21 Male    50000                50\n2     2    35 Female  80000                80\n3     3    58 Male   120000               120\n4     4    40 Female  75000                75\n\n# Use arrange() to sort data by 'Income'\narranged_data &lt;- data_ex3 %&gt;% arrange(Income)\narranged_data\n\n# A tibble: 4 × 4\n     ID   Age Gender Income\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1     1    21 Male    50000\n2     4    40 Female  75000\n3     2    35 Female  80000\n4     3    58 Male   120000\n\n# Use summarise() to get the mean 'Income'\nsummary_data &lt;- data_ex3 %&gt;% summarise(MeanIncome = mean(Income))\nsummary_data\n\n# A tibble: 1 × 1\n  MeanIncome\n       &lt;dbl&gt;\n1      81250\n\n# Use group_by() and summarise() to get the mean 'Income' for each 'Gender'\ngrouped_data &lt;- data_ex3 %&gt;% \n                  group_by(Gender) %&gt;% \n                  summarise(MeanIncome = mean(Income))\ngrouped_data\n\n# A tibble: 2 × 2\n  Gender MeanIncome\n  &lt;chr&gt;       &lt;dbl&gt;\n1 Female      77500\n2 Male        85000\n\n\nIn these examples, select is used to choose specific columns, filter is used to select rows based on a condition, mutate is used to create a new column, arrange is used to sort data, summarise is used to calculate summary statistics, and group_by is used to perform operations on groups of data."
  },
  {
    "objectID": "1-TidyverseRecap.html#visualizing-data-with-ggplot2",
    "href": "1-TidyverseRecap.html#visualizing-data-with-ggplot2",
    "title": "Tidyverse Recap",
    "section": "Visualizing Data with ggplot2",
    "text": "Visualizing Data with ggplot2\n\nggplot2 is a system for declaratively creating graphics, based on “The Grammar of Graphics”.\nYou provide the data, tell ggplot2 how to map variables to aesthetics, what graphic to use, and it takes care of the details.\n\n\n\n\nExample: ggplot\nBasic scatter plot with a regression line\n\n# Load the ggplot2 package\nlibrary(ggplot2)\n\np1 &lt;- ggplot(mtcars, aes(x = mpg, y = hp)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\")\np1\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nHistogram\n\n# Load the ggplot2 package\nlibrary(ggplot2)\n\np2 &lt;- ggplot(mtcars, aes(x = mpg)) +\n  geom_histogram(binwidth = 2, fill = \"blue\", color = \"white\")\np2\n\n\n\n\n\n\n\n\nBoxplot\n\n# Load the ggplot2 package\nlibrary(ggplot2)\n\np3 &lt;- ggplot(mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_boxplot(fill = \"orange\", color = \"darkred\")\np3\n\n\n\n\n\n\n\n\nBar chart\n\n# Load the ggplot2 package\nlibrary(ggplot2)\n\np4 &lt;- ggplot(mtcars, aes(x = factor(cyl))) +\n  geom_bar(fill = \"steelblue\") +\n  labs(x = \"Number of cylinders\", y = \"Frequency\")\np4\n\n\n\n\n\n\n\n\nIn these examples, geom_point is used to create a scatter plot, geom_smooth with method = \"lm\" is used to add a linear regression line, geom_histogram is used to create a histogram, geom_boxplot is used to create a boxplot, and geom_bar is used to create a bar chart."
  },
  {
    "objectID": "1-TidyverseRecap.html#class-exercise",
    "href": "1-TidyverseRecap.html#class-exercise",
    "title": "Tidyverse Recap",
    "section": "Class Exercise",
    "text": "Class Exercise\nSuppose we have a dataset called penguins and suppose we would like to study how the ratio of penguin body mass to flipper size differs across the species in the dataset. Rearrange the following steps in the pipeline into an order that accomplishes this goal.\n\n# a\narrange(avg_mass_flipper_ratio)\n\n# b\ngroup_by(species) %&gt;% \n\n# c\npenguins %&gt;% \n  \n# d\nsummarise(\n  avg_mass_flipper_ratio = median(mass_flipper_ratio, na.rm = TRUE)\n) %&gt;% \n  \n# e\nmutate(\n  mass_flipper_ratio = body_mass_g/flipper_length_mm\n) %&gt;%"
  },
  {
    "objectID": "2-Correlation.html",
    "href": "2-Correlation.html",
    "title": "Correlation (and Causation)",
    "section": "",
    "text": "When we see a pattern, we don’t just say “how extraordinary!” and move on; instead, we try and attribute a cause!\nhttps://www.tylervigen.com/spurious-correlations"
  },
  {
    "objectID": "2-Correlation.html#correlation-vs.-causation",
    "href": "2-Correlation.html#correlation-vs.-causation",
    "title": "Causation and Correlation",
    "section": "",
    "text": "When we see a pattern, we don’t just say “how extraordinary!” and move on; instead, we try and attribute a cause!\n\nWe all draw conclusions on the basis of what we see\nBut it is important for us to remember that just because there is a correlation between two facts, there isn’t necessarily a cause/effect relationship between them.\n\nlistening to loud music and acne\nice cream consumption and shark attacks\nhand size and reading ability in children\n…\n\nThese variables are correlated, but one does not cause the other!\n\nhttps://www.tylervigen.com/spurious-correlations"
  },
  {
    "objectID": "2-Correlation.html#correlation",
    "href": "2-Correlation.html#correlation",
    "title": "Correlation (and Causation)",
    "section": "Correlation",
    "text": "Correlation\nCorrelation (r) quantifies the linear association between two quantitative variables.\n\nThe value of \\(r\\) is between -1 and 1.\n\\(r &gt;\\) 0 when \\(x\\) and \\(y\\) have a positive association.\n\\(r &lt;\\) 0 when \\(x\\) and \\(y\\) have a negative association.\n\\(r\\) = 1 means a perfect positive linear association.\n\\(r\\) = -1 means a perfect negative linear association.\n\\(r\\) = 0 indicates no linear association between \\(x\\) and \\(y\\).\nThe value of \\(r\\) is a measure of the extent to which \\(x\\) and \\(y\\) are linearly related."
  },
  {
    "objectID": "2-Correlation.html#what-are-the-correlation-values-here",
    "href": "2-Correlation.html#what-are-the-correlation-values-here",
    "title": "Correlation (and Causation)",
    "section": "What are the correlation values here?",
    "text": "What are the correlation values here?\nTask: Match the plot panel number to the letter with the correct correlation value.\n\n\n\n\n\n\n\n\n\n          A           B           C           D           E           F \n-0.02770462  0.96643642 -0.69813746  0.04969697 -0.96735724  0.11948059 \n          G           H \n-0.39421900  0.67692544"
  },
  {
    "objectID": "2-Correlation.html#what-are-the-correlation-values-here-1",
    "href": "2-Correlation.html#what-are-the-correlation-values-here-1",
    "title": "Causation and Correlation",
    "section": "What are the correlation values here?",
    "text": "What are the correlation values here?"
  },
  {
    "objectID": "2-Correlation.html#what-are-the-correlation-values-here-2",
    "href": "2-Correlation.html#what-are-the-correlation-values-here-2",
    "title": "Causation and Correlation",
    "section": "What are the correlation values here?",
    "text": "What are the correlation values here?"
  },
  {
    "objectID": "2-Correlation.html#what-are-the-correlation-values-here-3",
    "href": "2-Correlation.html#what-are-the-correlation-values-here-3",
    "title": "Causation and Correlation",
    "section": "What are the correlation values here?",
    "text": "What are the correlation values here?"
  },
  {
    "objectID": "2-Correlation.html#what-are-the-correlation-values-here-4",
    "href": "2-Correlation.html#what-are-the-correlation-values-here-4",
    "title": "Causation and Correlation",
    "section": "What are the correlation values here?",
    "text": "What are the correlation values here?"
  },
  {
    "objectID": "2-Correlation.html#what-are-the-correlation-values-here-5",
    "href": "2-Correlation.html#what-are-the-correlation-values-here-5",
    "title": "Causation and Correlation",
    "section": "What are the correlation values here?",
    "text": "What are the correlation values here?"
  },
  {
    "objectID": "2-Correlation.html#what-are-the-correlation-values-here-6",
    "href": "2-Correlation.html#what-are-the-correlation-values-here-6",
    "title": "Causation and Correlation",
    "section": "What are the correlation values here?",
    "text": "What are the correlation values here?"
  },
  {
    "objectID": "2-Correlation.html#what-are-the-correlation-values-here-7",
    "href": "2-Correlation.html#what-are-the-correlation-values-here-7",
    "title": "Causation and Correlation",
    "section": "What are the correlation values here?",
    "text": "What are the correlation values here?"
  },
  {
    "objectID": "2-Correlation.html#calculating-the-correlation-coefficient",
    "href": "2-Correlation.html#calculating-the-correlation-coefficient",
    "title": "Correlation (and Causation)",
    "section": "Calculating the correlation coefficient",
    "text": "Calculating the correlation coefficient\nWe denote \\[\\begin{eqnarray*}\nS_{xx} &=& \\sum_{i=1}^{n}(x_i - \\bar{x})^2 = \\sum_{i=1}^{n}x_i^2 - n\\bar{x}^2 \\\\\nS_{yy} &=& \\sum_{i=1}^{n}(y_i - \\bar{y})^2 = \\sum_{i=1}^{n}y_i^2 - n\\bar{y}^2 \\\\\nS_{xy} &=& \\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y}) = \\sum_{i=1}^{n}x_iy_i - n\\bar{x}\\bar{y}\n\\end{eqnarray*}\\]\nThen \\[\\begin{eqnarray*}\nr &=& \\frac{Sxy}{\\sqrt{SxxSyy}}     \n\\end{eqnarray*}\\]\n\nExample: Calculate the correlation between \\(x\\) and \\(y\\)\n\n\n\n\n\nx\n2\n4\n1\n6\n7\n\n\ny\n3\n4\n0\n8\n8\n\n\n\n\n\n\n\\[\\begin{align*}\n&\\sum_{i=1}^{n}x_i^2=106, \\sum_{i=1}^{n}y_i^2=153, \\sum_{i=1}^{n}x_iy_i=126, \\bar{x}=4,\\bar{y}=4.6\\\\\n&S_{xx} = \\sum_{i=1}^{n}x_i^2 - n\\bar{x}^2 = 26, S_{yy} = \\sum_{i=1}^{n}y_i^2 - n\\bar{y}^2 = 47.2\\\\\n&S_{xy} = \\sum_{i=1}^{n}x_iy_i - n\\bar{x}\\bar{y} = 34\\\\\n\\\\\n&r = \\frac{Sxy}{\\sqrt{SxxSyy}} = \\frac{34}{\\sqrt{26 \\times 47.2}} = 0.97        \n\\end{align*}\\]\n\n\nExample: Change of scale and the correlation coefficient\nThe distance of the race and the time it took to complete was recorded for five races in kilometres and seconds respectively. The correlation was calculated between the two variables. The data set was also converted into miles (\\(\\times\\) 0.621371192) and minutes (/60) and the correlation was re-calculated.\n\n\n\n\n\nKilometres\nSeconds\nMiles\nMinutes\n\n\n\n\n0.1\n10\n0.0621371\n0.1666667\n\n\n0.4\n120\n0.2485485\n2.0000000\n\n\n0.8\n300\n0.4970970\n5.0000000\n\n\n1.6\n535\n0.9941939\n8.9166667\n\n\n3.0\n950\n1.8641136\n15.8333333\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nIf \\(x\\) and \\(y\\) measurement units are changed, correlation does not change.\nIf \\(x\\) and \\(y\\) are reversed, i.e. correlation of \\(y\\) and \\(x\\), the correlation does not change.\nCorrelation is a measure of linear association. It does not establish causation.\nTwo variables, x and y, could be highly correlated because there is another variable, z, having an impact on both x and y."
  },
  {
    "objectID": "2-Correlation.html#example-calculate-the-correlation-between-x-and-y",
    "href": "2-Correlation.html#example-calculate-the-correlation-between-x-and-y",
    "title": "Causation and Correlation",
    "section": "Example: Calculate the correlation between \\(x\\) and \\(y\\)",
    "text": "Example: Calculate the correlation between \\(x\\) and \\(y\\)\n\n\n\n\n\nx\n2\n4\n1\n6\n7\n\n\ny\n3\n4\n0\n8\n8\n\n\n\n\n\n\n\\[\\begin{align*}\n&\\sum_{i=1}^{n}x_i^2=106, \\sum_{i=1}^{n}y_i^2=153, \\sum_{i=1}^{n}x_iy_i=126, \\bar{x}=4,\\bar{y}=4.6\\\\\n&S_{xx} = \\sum_{i=1}^{n}x_i^2 - n\\bar{x}^2 = 26, S_{yy} = \\sum_{i=1}^{n}y_i^2 - n\\bar{y}^2 = 47.2\\\\\n&S_{xy} = \\sum_{i=1}^{n}x_iy_i - n\\bar{x}\\bar{y} = 34\\\\\n\\\\\n&r = \\frac{Sxy}{\\sqrt{SxxSyy}} = \\frac{34}{\\sqrt{26 \\times 47.2}} = 0.97        \n\\end{align*}\\]"
  },
  {
    "objectID": "2-Correlation.html#notes-on-correlation",
    "href": "2-Correlation.html#notes-on-correlation",
    "title": "Causation and Correlation",
    "section": "Notes on correlation",
    "text": "Notes on correlation\n\nIf \\(x\\) and \\(y\\) measurement units are changed, correlation does not change.\nIf \\(x\\) and \\(y\\) are reversed, i.e. correlation of \\(y\\) and \\(x\\), the correlation does not change.\nCorrelation is a measure of linear association. It does not establish causation.\nTwo variables, x and y, could be highly correlated because there is another variable, z, having an impact on both x and y.\n\nExample, we have not established that extra height causes bigger feet. In fact genetic factors cause both."
  },
  {
    "objectID": "2-Correlation.html#example-change-of-scale-and-the-correlation-coefficient",
    "href": "2-Correlation.html#example-change-of-scale-and-the-correlation-coefficient",
    "title": "Causation and Correlation",
    "section": "Example: Change of scale and the correlation coefficient",
    "text": "Example: Change of scale and the correlation coefficient\nThe distance of the race and the time it took to complete was recorded for five races in kilometres and seconds respectively. The correlation was calculated between the two variables. The data set was also converted into miles (\\(\\times\\) 0.621371192) and minutes (/60) and the correlation was re-calculated.\n\n\n\n\n\nKilometres\nSeconds\nMiles\nMinutes\n\n\n\n\n0.1\n10\n0.0621371\n0.1666667\n\n\n0.4\n120\n0.2485485\n2.0000000\n\n\n0.8\n300\n0.4970970\n5.0000000\n\n\n1.6\n535\n0.9941939\n8.9166667\n\n\n3.0\n950\n1.8641136\n15.8333333"
  },
  {
    "objectID": "2-Correlation.html#explore-spurious-correlations",
    "href": "2-Correlation.html#explore-spurious-correlations",
    "title": "Causation and Correlation",
    "section": "Explore Spurious Correlations",
    "text": "Explore Spurious Correlations\nFind two correlated variables from: https://www.tylervigen.com/spurious-correlations.\nCreate a scatter plot and find the correlation."
  },
  {
    "objectID": "2-Correlation.html#association-between-pga-golfers-accuracy-and-driving-distance",
    "href": "2-Correlation.html#association-between-pga-golfers-accuracy-and-driving-distance",
    "title": "Causation and Correlation",
    "section": "Association between PGA golfer’s accuracy and driving distance",
    "text": "Association between PGA golfer’s accuracy and driving distance\nDescription:\nThe data set `golf’ was taken from PGA Tour Recordsof 195 golf rounds by PGA players in an attempt to explain what golf attributes contribute the most to low scores."
  },
  {
    "objectID": "2-Correlation.html#spurious-correlations",
    "href": "2-Correlation.html#spurious-correlations",
    "title": "Correlation (and Causation)",
    "section": "Spurious Correlations",
    "text": "Spurious Correlations\nA spurious correlation is a statistical relationship between two variables that appears to be meaningful but is actually caused by coincidence or the influence of a third (confounding) variable. This misleading association can arise due to random chance, indirect causation, or omitted variables.\nFor example, there may be a strong correlation between ice cream sales and drowning incidents, but this does not mean one causes the other. Instead, a third factor—hot weather—increases both ice cream sales and swimming activity, which in turn raises the risk of drowning.\nSpurious correlations can often be identified through deeper statistical analysis, such as controlling for confounding variables or using causal inference techniques.\nTask: Find two correlated variables from: https://www.tylervigen.com/spurious-correlations. Create a scatter plot and find the correlation."
  },
  {
    "objectID": "2-Correlation.html#what-is-the-capital-of-france",
    "href": "2-Correlation.html#what-is-the-capital-of-france",
    "title": "Causation and Correlation",
    "section": "What is the capital of France?",
    "text": "What is the capital of France?\n\nLondon\nParis\nBerlin\nMadrid"
  },
  {
    "objectID": "2-Correlation.html#example-data-1-association-between-pga-golfers-accuracy-and-driving-distance",
    "href": "2-Correlation.html#example-data-1-association-between-pga-golfers-accuracy-and-driving-distance",
    "title": "Causation and Correlation",
    "section": "Example Data (1): Association between PGA golfer’s accuracy and driving distance",
    "text": "Example Data (1): Association between PGA golfer’s accuracy and driving distance\nDescription:\nThe data set `golf’ was taken from PGA Tour Recordsof 195 golf rounds by PGA players in an attempt to explain what golf attributes contribute the most to low scores."
  },
  {
    "objectID": "2-Correlation.html#example-data-2-association-between-pga-golfers-accuracy-and-driving-distance",
    "href": "2-Correlation.html#example-data-2-association-between-pga-golfers-accuracy-and-driving-distance",
    "title": "Causation and Correlation",
    "section": "Example Data (2): Association between PGA golfer’s accuracy and driving distance",
    "text": "Example Data (2): Association between PGA golfer’s accuracy and driving distance"
  },
  {
    "objectID": "2-Correlation.html#example-data-1-what-is-the-association-between-pga-golfers-accuracy-and-driving-distance",
    "href": "2-Correlation.html#example-data-1-what-is-the-association-between-pga-golfers-accuracy-and-driving-distance",
    "title": "Correlation (and Causation)",
    "section": "Example Data (1): What is the association between PGA golfer’s accuracy and driving distance?",
    "text": "Example Data (1): What is the association between PGA golfer’s accuracy and driving distance?\nThe data set `golf’ was taken from PGA Tour Recordsof 195 golf rounds by PGA players in an attempt to explain what golf attributes contribute the most to low scores."
  },
  {
    "objectID": "2-Correlation.html#example-data-2-what-is-the-relationship-between-cars-weights-and-their-mileage",
    "href": "2-Correlation.html#example-data-2-what-is-the-relationship-between-cars-weights-and-their-mileage",
    "title": "Correlation (and Causation)",
    "section": "Example Data (2): What is the relationship between cars’ weights and their mileage?",
    "text": "Example Data (2): What is the relationship between cars’ weights and their mileage?\nThe data mtcars was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973–74 models)."
  },
  {
    "objectID": "0-Information.html#assessment-and-materials",
    "href": "0-Information.html#assessment-and-materials",
    "title": "Module Information",
    "section": "Assessment and Materials",
    "text": "Assessment and Materials\nNotes\n\nNotes will be a combination of links posted on Moodle and notes taken down in class\n\nAssignments\n\nWill be uploaded PDFs.\n\nLab quizzes\n\nWill be Moodle Quiz format with multiple attempts allowed until the end of the semester.\n\nMidterm Exam\n\nWill be a Moodle Quiz MCQ.\n\nMarks\n\n10% – compulsory assignments\n10% – quizzes\n15% – mid-term exam (March 28th, MCQ)\n65% – final exam (date TBC)\n\nRecommended reading\n\nR for Data Science (https://r4ds.had.co.nz)"
  },
  {
    "objectID": "0-Information.html#topics-covered",
    "href": "0-Information.html#topics-covered",
    "title": "Module Information",
    "section": "Topics Covered",
    "text": "Topics Covered\n\nTidyverse Recap\nCorrelation (and Causation)\nObservational Studies\nControlled Experiments\nSurveys\nData Privacy and Anonymisation\nSupervised and Unsupervised Learning\nPredictive Analytics\nImage and Text Analysis"
  },
  {
    "objectID": "0-Information.html#materials-and-assessments",
    "href": "0-Information.html#materials-and-assessments",
    "title": "Module Information",
    "section": "Materials and Assessments",
    "text": "Materials and Assessments\nNotes\n\nNotes will be a combination of links posted on Moodle and notes taken down in class\n\nTutorial Questions\n\nPractice tutorial questions will be posted to Moodle.\n\nAssignments\n\nAssignment questions will be posted to Moodle and answers should be uploaded as PDFs.\n\nLab Quizzes\n\nEach lab will have an associated quiz that will be a Moodle Quiz format with multiple attempts allowed until the end of the semester.\n\nMidterm Exam\n\nThe midterm exam will be a Moodle Quiz MCQ.\n\nMarks\n\n10% – compulsory assignments\n10% – quizzes\n15% – mid-term exam (March 28th, MCQ)\n65% – final exam (date TBC)\n\nRecommended reading\n\nR for Data Science (https://r4ds.had.co.nz)"
  },
  {
    "objectID": "3-MultipleRegression.html",
    "href": "3-MultipleRegression.html",
    "title": "Multiple Regression",
    "section": "",
    "text": "We have seen last semester, in DS151, how to study the relationship between two variables using linear regression.\nWe can create a linear regression model that includes a predictor, such that \\[\\hat{\\mbox{y}}=\\beta_0+\\beta_1\\mbox{x}\\]"
  },
  {
    "objectID": "3-MultipleRegression.html#recap-simple-regression",
    "href": "3-MultipleRegression.html#recap-simple-regression",
    "title": "Multiple Regression",
    "section": "",
    "text": "We have seen last semester, in DS151, how to study the relationship between two variables using linear regression.\nWe can create a linear regression model that includes a predictor, such that \\[\\hat{\\mbox{y}}=\\beta_0+\\beta_1\\mbox{x}\\]"
  },
  {
    "objectID": "3-MultipleRegression.html#multiple-regression-example-1-the-crab-dataset",
    "href": "3-MultipleRegression.html#multiple-regression-example-1-the-crab-dataset",
    "title": "Multiple Regression",
    "section": "Multiple Regression Example 1: The Crab Dataset",
    "text": "Multiple Regression Example 1: The Crab Dataset\nIn many research and applied fields, many different variables are measured at the same time, for various reasons. In many cases, we may actually have too many variables and may end up selecting a subset of them (only the most important) to proceed with the analysis. (NB: there are many ways to define variable importance, and we will look at some of them in depth throughout the Data Science programme).\nLet’s have a look at an example where we have many predictor variables. The crabs dataset from package MASS has 200 observations on 2 qualitative variables (species colour and sex), and 5 morphological measurements (frontal lobe size, rear width, carapace length and width, and body depth) of crabs of the species Leptograspus variegatus.\n\n\n\ncrab_dat &lt;- MASS::crabs\nglimpse(crab_dat)\n\nRows: 200\nColumns: 8\n$ sp    &lt;fct&gt; B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B…\n$ sex   &lt;fct&gt; M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M…\n$ index &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 1…\n$ FL    &lt;dbl&gt; 8.1, 8.8, 9.2, 9.6, 9.8, 10.8, 11.1, 11.6, 11.8, 11.8, 12.2, 12.…\n$ RW    &lt;dbl&gt; 6.7, 7.7, 7.8, 7.9, 8.0, 9.0, 9.9, 9.1, 9.6, 10.5, 10.8, 11.0, 1…\n$ CL    &lt;dbl&gt; 16.1, 18.1, 19.0, 20.1, 20.3, 23.0, 23.8, 24.5, 24.2, 25.2, 27.3…\n$ CW    &lt;dbl&gt; 19.0, 20.8, 22.4, 23.1, 23.0, 26.5, 27.1, 28.4, 27.8, 29.3, 31.6…\n$ BD    &lt;dbl&gt; 7.0, 7.4, 7.7, 8.2, 8.2, 9.8, 9.8, 10.4, 9.7, 10.3, 10.9, 11.4, …\n\nhead(crab_dat)\n\n  sp sex index   FL  RW   CL   CW  BD\n1  B   M     1  8.1 6.7 16.1 19.0 7.0\n2  B   M     2  8.8 7.7 18.1 20.8 7.4\n3  B   M     3  9.2 7.8 19.0 22.4 7.7\n4  B   M     4  9.6 7.9 20.1 23.1 8.2\n5  B   M     5  9.8 8.0 20.3 23.0 8.2\n6  B   M     6 10.8 9.0 23.0 26.5 9.8\n\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\nlibrary(ggplot2)\nggpairs(crab_dat, columns = 4:8, ggplot2::aes(colour=sex)) +theme_bw()\n\n\n\n\n\n\n\n\nImagine we are interested in predicting the carapace length (CL) of this species of crab. We can create a linear regression model that includes multiple predictors, such as \\[\\hat{\\mbox{CL}}=\\beta_0+\\beta_1\\mbox{FL}+\\beta_2\\mbox{RW}+\\beta_3\\mbox{CW}+\\beta_4\\mbox{BD}\\] This model can be fitted in R by executing:\n\nfit1 &lt;- lm(CL ~ FL + RW + CW + BD, data = crab_dat)\nfit1 %&gt;% coef\n\n(Intercept)          FL          RW          CW          BD \n  0.3163364   0.2648933  -0.1778948   0.6402190   0.4714152 \n\n\nWe have that \\[\\hat{\\mbox{CL}}=0.32+0.26\\mbox{FL}-0.18\\mbox{RW}+0.64\\mbox{CW}+0.47\\mbox{BD}\\].\nWe can plot the actual observations versus the predicted ones to see how well we did:\n\ncrab_dat &lt;- crab_dat %&gt;% \n              mutate(CL_pred = predict(fit1))\n\nggplot(data = crab_dat,\n       mapping = aes(x = CL, y = CL_pred, colour = sex)) +\n  theme_bw() +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  xlab(\"Observed CL\") + \n  ylab(\"Predicted CL\")\n\n\n\n\n\n\n\n\nHow well would we have done if we only used one predictor variable, say RW?\n\nfit2 &lt;- lm(CL ~ RW, data = crab_dat)\n\ncrab_dat &lt;- crab_dat %&gt;% \n              mutate(CL_pred2 = predict(fit2))\n\nggplot(data = crab_dat,\n       mapping = aes(x = CL, y = CL_pred2, colour = sex)) +\n  theme_bw() +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  xlab(\"Observed CL\") + \n  ylab(\"Predicted CL\")\n\n\n\n\n\n\n\n\nNow what about the categorical variables, namely sex and sp (species colour)? We can also include them in our model. Because they are categorical, they will only influence the overall mean response for each category, and hence we don’t estimate a slope for them, i.e. \\[\\hat{\\mbox{CL}}=\\beta_0+\\beta_1\\mbox{FL}+\\beta_2\\mbox{RW}+\\beta_3\\mbox{CW}+\\beta_4\\mbox{BD}+\\beta_5I(\\mbox{sp}=\\mbox{O})+\\beta_6I(\\mbox{sex}=\\mbox{M})\\] (NB: \\(I(\\mbox{sex}=\\mbox{M})\\) is an indicator function, which is equal to 1 if sex is equal to M and zero otherwise. The same type of interpretation can be drawn from \\(I(\\mbox{sp}=\\mbox{O})\\))\n\nfit3 &lt;- lm(CL ~ FL + RW + CW + BD + sp + sex, data = crab_dat)\nfit3 %&gt;% coef\n\n(Intercept)          FL          RW          CW          BD         spO \n-0.19583237  0.21392757 -0.03750446  0.65118521  0.38956671  0.16389052 \n       sexM \n 0.37020979 \n\ncrab_dat &lt;- crab_dat %&gt;% \n              mutate(CL_pred3 = predict(fit3))\n\nggplot(data = crab_dat,\n       mapping = aes(x = CL, y = CL_pred3, colour = sex)) +\n  theme_bw() +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  xlab(\"Observed CL\") + \n  ylab(\"Predicted CL\")\n\n\n\n\n\n\n\n\nHow do we interpret the coefficients related to sp and sex? They are the increase (or decrease, if negative) in the overall mean if a crab is orange (O) or male (M).\nOne way of comparing how well our modelling strategies did in terms of predictions is to sum the squared discrepancies (residuals) between predicted and observed values, and see which one is smaller.\n\ndiscrepancy_fit1 &lt;- sum(residuals(fit1)^2)\ndiscrepancy_fit2 &lt;- sum(residuals(fit2)^2)\ndiscrepancy_fit3 &lt;- sum(residuals(fit3)^2)\n\ndiscrepancy_fit1\n\n[1] 27.35058\n\ndiscrepancy_fit2\n\n[1] 2047.417\n\ndiscrepancy_fit3\n\n[1] 25.06863\n\n\nTypically we would split the data into training and test set, use only the training set to fit the model, and then perform this computation on both sets. We will see more details on how to compare the models in terms of their predictive power in the module Statistical Machine Learning. We will also see more details on this in the modules Linear Models I and II, how to properly test hypotheses, how to assess goodness-of-fit, what the important assumptions are and how to properly check them.\n\nLogistic Regression Example 2: The Iris Dataset\nWe explored this dataset initially last semester in DS151.\nIris is a genus of about 300 species of flowering plants, taking its name from the Greek word for a rainbow (which is also the name for the Greek goddess of rainbows, Iris). The flowers are very showy, and we would like to know if it is possible to identify some of the species based on measurements of different parts of the flowers.\nLet’s look at a famous example dataset that gives the measurements in centimeters of the variables:\nfor 50 flowers from each of three species if Iris: Iris setosa, Iris versicolor, and Iris virginica.\nThis dataset is available in base R as iris. Let’s have a look:\n\nlibrary(tidyverse)\nglimpse(iris)\n\nRows: 150\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n\nLet’s work, initially, only with species versicolor and virginica. We create a plot that shows in the \\(x\\) axis the petal length, and in the \\(y\\) axis only the values of 0 and 1, representing whether the observation belongs to class virginica or not (0 = the observation belongs to class versicolor; 1 = it belongs to class virginica). We call this a binary (or dummy) variable.\n\niris2 &lt;- iris %&gt;%\n  filter(Species != \"setosa\") %&gt;%\n  mutate(Species.binary = as.numeric(Species == \"virginica\"))\n  \nggplot(iris2, aes(x = Petal.Length, y = Species.binary)) +\n  geom_point() +\n  theme_bw()\n\n\n\n\n\n\n\n\nIt appears that as the petal length increases, so does the likelihood of belonging to the virginica class. We may interpret the numbers 0 and 1 here as the proportion \\(p\\) of plants that belong to class virginica and present a specific measurement of petal length.\nRemember. We transform the scale of the response in such a way that it will be bounded between 0 and 1, and hence our estimates will be sensical. The transformation we use is called the logit, and is the natural logarithm of the odds of belonging to class virginica: \\[\\displaystyle\\log\\left(\\frac{p}{1-p}\\right)=\\beta_0+\\beta_1*\\mbox{Petal.Length},\\] where \\(p\\) is the proportion of plants that belong to class virginica. This way, the estimated proportions will always be bounded between 0 and 1, and are now suitable to our problem. Let’s see how it looks like for our example:\n\nggplot(iris2, aes(x = Petal.Length, y = Species.binary)) +\n  geom_point() +\n  geom_smooth(method = glm, method.args = list(family = \"binomial\"), se = FALSE) +\n  theme_bw()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nIncluding more predictors\nIt is frequently of interest to use multiple covariates to improve our predictive power. Let’s fit a logistic regression model including all four covariates as predictors in our model: \\[\\displaystyle\\log\\left(\\frac{p}{1-p}\\right)=\\beta_0+\\beta_1*\\mbox{Sepal.Length}+\\beta_2*\\mbox{Sepal.Width}+\\beta_3*\\mbox{Petal.Length}+\\beta_4*\\mbox{Petal.Width}\\]\n\nfull_logistic_reg &lt;- glm(Species.binary ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width,\n                         family = binomial, data = iris2)\nfull_logistic_reg %&gt;% coef %&gt;% round(digits = 4)\n\n (Intercept) Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n    -42.6378      -2.4652      -6.6809       9.4294      18.2861 \n\n\nHow well did we do?\n\niris_predict &lt;- iris2 %&gt;% \n                  mutate(p_hat = predict(full_logistic_reg, type = \"response\") %&gt;% round(2),\n                         Species_pred = ifelse(p_hat &gt;= 0.5,\"virginica\",\"versicolor\"))\n\nn_correct_full &lt;- sum(iris_predict$Species_pred == iris_predict$Species)\nn_correct_full\n\n[1] 98\n\n\nOur model now correctly predicts the class of 98 out of 100 observations."
  },
  {
    "objectID": "3-MultipleRegression.html#example-3-the-wine-dataset",
    "href": "3-MultipleRegression.html#example-3-the-wine-dataset",
    "title": "Multiple Regression",
    "section": "Example 3: The Wine Dataset",
    "text": "Example 3: The Wine Dataset"
  },
  {
    "objectID": "3-MultipleRegression.html#logistic-regression-the-wine-dataset",
    "href": "3-MultipleRegression.html#logistic-regression-the-wine-dataset",
    "title": "Multiple Regression",
    "section": "Logistic Regression: The Wine dataset",
    "text": "Logistic Regression: The Wine dataset\nThe wine dataset contains the results of a chemical analysis of wines grown in a specific area of Italy. Three types of wine are represented in the 178 samples, with the results of 13 chemical analyses recorded for each sample. The Type variable has been transformed into a categoric variable.\nThe data contains no missing values and consits of only numeric data, with a three class target variable (Type) for classification.\nLet’s have a glimpse at the dataset:\n\nwine &lt;- read_csv(\"https://www.dropbox.com/s/l5x4ur06gfhpg0h/wine.csv?raw=1\") \n\nRows: 178 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): Type\ndbl (13): Alcohol, Malic, Ash, Alcalinity, Magnesium, Phenols, Flavanoids, N...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(wine)\n\nRows: 178\nColumns: 14\n$ Type                 &lt;chr&gt; \"type1\", \"type1\", \"type1\", \"type1\", \"type1\", \"typ…\n$ Alcohol              &lt;dbl&gt; 14.23, 13.20, 13.16, 14.37, 13.24, 14.20, 14.39, …\n$ Malic                &lt;dbl&gt; 1.71, 1.78, 2.36, 1.95, 2.59, 1.76, 1.87, 2.15, 1…\n$ Ash                  &lt;dbl&gt; 2.43, 2.14, 2.67, 2.50, 2.87, 2.45, 2.45, 2.61, 2…\n$ Alcalinity           &lt;dbl&gt; 15.6, 11.2, 18.6, 16.8, 21.0, 15.2, 14.6, 17.6, 1…\n$ Magnesium            &lt;dbl&gt; 127, 100, 101, 113, 118, 112, 96, 121, 97, 98, 10…\n$ Phenols              &lt;dbl&gt; 2.80, 2.65, 2.80, 3.85, 2.80, 3.27, 2.50, 2.60, 2…\n$ Flavanoids           &lt;dbl&gt; 3.06, 2.76, 3.24, 3.49, 2.69, 3.39, 2.52, 2.51, 2…\n$ Nonflavanoid.phenols &lt;dbl&gt; 0.28, 0.26, 0.30, 0.24, 0.39, 0.34, 0.30, 0.31, 0…\n$ Proanth              &lt;dbl&gt; 2.29, 1.28, 2.81, 2.18, 1.82, 1.97, 1.98, 1.25, 1…\n$ Color                &lt;dbl&gt; 5.64, 4.38, 5.68, 7.80, 4.32, 6.75, 5.25, 5.05, 5…\n$ Hue                  &lt;dbl&gt; 1.04, 1.05, 1.03, 0.86, 1.04, 1.05, 1.02, 1.06, 1…\n$ OD                   &lt;dbl&gt; 3.92, 3.40, 3.17, 3.45, 2.93, 2.85, 3.58, 3.58, 2…\n$ Proline              &lt;dbl&gt; 1065, 1050, 1185, 1480, 735, 1450, 1290, 1295, 10…\n\n\nChange the code below and make different plots with other covariate combinations. What sort of patterns begin to emerge?\n\nggplot(wine, aes(x = Ash, y = Malic, colour = Type)) +\n  geom_point() +\n  theme_bw() +\n  labs(colour = \"Wine Type\")\n\n\n\n\n\n\n\n\nWe will work only with types 1 and 2\n\nwine2 &lt;- wine %&gt;%\n  filter(Type != \"type3\") %&gt;%\n  mutate(Type_binary = as.numeric(Type == \"type1\")) %&gt;% \n  as_tibble\nwine2\n\n# A tibble: 130 × 15\n   Type  Alcohol Malic   Ash Alcalinity Magnesium Phenols Flavanoids\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1 type1    14.2  1.71  2.43       15.6       127    2.8        3.06\n 2 type1    13.2  1.78  2.14       11.2       100    2.65       2.76\n 3 type1    13.2  2.36  2.67       18.6       101    2.8        3.24\n 4 type1    14.4  1.95  2.5        16.8       113    3.85       3.49\n 5 type1    13.2  2.59  2.87       21         118    2.8        2.69\n 6 type1    14.2  1.76  2.45       15.2       112    3.27       3.39\n 7 type1    14.4  1.87  2.45       14.6        96    2.5        2.52\n 8 type1    14.1  2.15  2.61       17.6       121    2.6        2.51\n 9 type1    14.8  1.64  2.17       14          97    2.8        2.98\n10 type1    13.9  1.35  2.27       16          98    2.98       3.15\n# ℹ 120 more rows\n# ℹ 7 more variables: Nonflavanoid.phenols &lt;dbl&gt;, Proanth &lt;dbl&gt;, Color &lt;dbl&gt;,\n#   Hue &lt;dbl&gt;, OD &lt;dbl&gt;, Proline &lt;dbl&gt;, Type_binary &lt;dbl&gt;\n\n\nChange the code below and fit a logistic regression model using the four predictors you believe are the best to classify the two types of wine. Also play around with different thresholds for the classification rule. Compare with your peers. What predictors yielded the best predictive performance?\n\n# include the predictors below\nlogistic_reg &lt;- glm(Type_binary ~ Malic + Ash + Alcalinity + Magnesium,\n                           family = binomial, data = wine2)\n\n# set the threshold for the classification rule below\nthreshold &lt;- 0.50\n\n# computes the percentage of correct predictions\nwine_predict &lt;- wine2 %&gt;% \n                  mutate(p_hat = predict(logistic_reg, type = \"response\") %&gt;% round(2),\n                         Type_pred = ifelse(p_hat &gt;= threshold, \"type1\",\"type2\")) \n\n\nn_correct &lt;- sum(wine_predict$Type_pred == wine_predict$Type)\nn_total &lt;- nrow(wine2)\npercentage_correct &lt;- n_correct/n_total * 100\npercentage_correct\n\n[1] 86.92308"
  },
  {
    "objectID": "3-MultipleRegression.html#modeling-cars",
    "href": "3-MultipleRegression.html#modeling-cars",
    "title": "Multiple Regression",
    "section": "Modeling cars",
    "text": "Modeling cars\n\n\n\n\n\n\n\n\n\n\nDescribe: What is the relationship between cars’ weights and their mileage?\n\n\n\n\n\n\n\n\n\n\n\nPredict: What is your best guess for a car’s MPG that weighs 3,500 pounds?\n\n\n\nWarning in geom_segment(aes(x = 3.5, xend = 3.5, y = -Inf, yend = 18.5), : All aesthetics have length 1, but the data has 32 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = -Inf, xend = 3.5, y = 18.5, yend = 18.5), : All aesthetics have length 1, but the data has 32 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row."
  },
  {
    "objectID": "3-MultipleRegression.html#modelling-cars",
    "href": "3-MultipleRegression.html#modelling-cars",
    "title": "Multiple Regression",
    "section": "Modelling cars",
    "text": "Modelling cars\n\nDescribe: What is the relationship between cars’ weights and their mileage?"
  },
  {
    "objectID": "3-MultipleRegression.html#modelling-cars-1",
    "href": "3-MultipleRegression.html#modelling-cars-1",
    "title": "Multiple Regression",
    "section": "Modelling cars",
    "text": "Modelling cars\n\nPredict: What is your best guess for a car’s MPG that weighs 3,500 pounds?\n\n\n\nWarning in geom_segment(aes(x = 3.5, xend = 3.5, y = -Inf, yend = 18.5), : All aesthetics have length 1, but the data has 32 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = -Inf, xend = 3.5, y = 18.5, yend = 18.5), : All aesthetics have length 1, but the data has 32 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row."
  },
  {
    "objectID": "3-MultipleRegression.html#modelling",
    "href": "3-MultipleRegression.html#modelling",
    "title": "Multiple Regression",
    "section": "Modelling",
    "text": "Modelling\n\nUse models to explain the relationship between variables and to make predictions\nFor now we will focus on linear models (but there are many many other types of models too!)"
  },
  {
    "objectID": "3-MultipleRegression.html#modelling-vocabulary",
    "href": "3-MultipleRegression.html#modelling-vocabulary",
    "title": "Multiple Regression",
    "section": "Modelling vocabulary",
    "text": "Modelling vocabulary\n\nPredictor (explanatory variable)\nOutcome (response variable)\nRegression line\n\nSlope\nIntercept\n\n\n\nPredictor (explanatory variable)\n\n\n\n\n\n\n\n\n\n\nmpg\nwt\n\n\n\n\n21\n2.62\n\n\n21\n2.875\n\n\n22.8\n2.32\n\n\n21.4\n3.215\n\n\n18.7\n3.44\n\n\n18.1\n3.46\n\n\n...\n...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutcome (response variable)\n\n\n\n\n\n\n\n\n\n\nmpg\nwt\n\n\n\n\n21\n2.62\n\n\n21\n2.875\n\n\n22.8\n2.32\n\n\n21.4\n3.215\n\n\n18.7\n3.44\n\n\n18.1\n3.46\n\n\n...\n...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression line\n\n\n\n\n\n\n\n\n\n\n\nRegression line: slope\n\n\n\n\n\n\n\n\n\n\n\nRegression line: intercept"
  },
  {
    "objectID": "3-MultipleRegression.html#predictor-explanatory-variable",
    "href": "3-MultipleRegression.html#predictor-explanatory-variable",
    "title": "Multiple Regression",
    "section": "Predictor (explanatory variable)",
    "text": "Predictor (explanatory variable)\n\n\n\n\n\n\n\n\n\n\nmpg\nwt\n\n\n\n\n21\n2.62\n\n\n21\n2.875\n\n\n22.8\n2.32\n\n\n21.4\n3.215\n\n\n18.7\n3.44\n\n\n18.1\n3.46\n\n\n...\n..."
  },
  {
    "objectID": "3-MultipleRegression.html#outcome-response-variable",
    "href": "3-MultipleRegression.html#outcome-response-variable",
    "title": "Multiple Regression",
    "section": "Outcome (response variable)",
    "text": "Outcome (response variable)\n\n\n\n\n\n\n\n\n\n\nmpg\nwt\n\n\n\n\n21\n2.62\n\n\n21\n2.875\n\n\n22.8\n2.32\n\n\n21.4\n3.215\n\n\n18.7\n3.44\n\n\n18.1\n3.46\n\n\n...\n..."
  },
  {
    "objectID": "3-MultipleRegression.html#correlation-1",
    "href": "3-MultipleRegression.html#correlation-1",
    "title": "Multiple Regression",
    "section": "Correlation",
    "text": "Correlation\n\nRanges between -1 and 1.\nSame sign as the slope.\n\nWe have seen last semester, in DS151, how to study the relationship between two variables using linear regression. See, e.g. the mammals dataset:\n\nlibrary(tidyverse)\nmammals &lt;- read_csv(\"https://www.dropbox.com/s/rb3zd0nk4430tbv/mammals.csv?raw=1\")\nmammals\n\n# A tibble: 62 × 3\n   name               body brain\n   &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt;\n 1 Arctic fox        3.38   44.5\n 2 Owl monkey        0.48   15.5\n 3 Mountain beaver   1.35    8.1\n 4 Cow             465     423  \n 5 Grey wolf        36.3   120. \n 6 Goat             27.7   115  \n 7 Roe deer         14.8    98.2\n 8 Guinea pig        1.04    5.5\n 9 Verbet            4.19   58  \n10 Chinchilla        0.425   6.4\n# ℹ 52 more rows\n\n\n\n\nLet’s have a look at an exploratory plot, using brain weight as the response variable and body weight as the predictor variable:\n\nggplot(data = mammals,\n       mapping = aes(y = brain, x = body)) +\n  theme_bw() +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nLet’s label the points:\n\nggplot(data = mammals,\n       mapping = aes(y = brain, x = body, label = name)) +\n  theme_bw() +\n  geom_point() +\n  geom_label()\n\n\n\n\n\n\n\n\nIt is difficult to see the clump of points at the bottom left part of the plot. Maybe it is better to take the logarithm of the two variables to visualise them –\n\nggplot(data = mammals,\n       mapping = aes(y = log(brain), x = log(body), label = name)) +\n  theme_bw() +\n  geom_label()\n\n\n\n\n\n\n\n\nWe can overlay a linear regression line to the plot:\n\nggplot(data = mammals,\n       mapping = aes(y = log(brain), x = log(body))) +\n  theme_bw() +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe equation that predicts brain weight as a function of body weight is\n\nlm(log(brain) ~ log(body), data = mammals) %&gt;%\n  coef\n\n(Intercept)   log(body) \n  2.1347887   0.7516859 \n\n\n\\[\\log(\\mbox{brain weight}) = 2.13+0.75\\times\\log(\\mbox{body weight})\\]\nHowever, it is rare to find a dataset with only two variables. In many research and applied fields, many different variables are measured at the same time, for various reasons. In many cases, we may actually have too many variables and may end up selecting a subset of them (only the most important) to proceed with the analysis. (NB: there are many ways to define variable importance, and we will look at some of them in depth throughout the Data Science programme)."
  },
  {
    "objectID": "3-MultipleRegression.html#regression-line",
    "href": "3-MultipleRegression.html#regression-line",
    "title": "Multiple Regression",
    "section": "Regression line",
    "text": "Regression line"
  },
  {
    "objectID": "3-MultipleRegression.html#regression-line-slope",
    "href": "3-MultipleRegression.html#regression-line-slope",
    "title": "Multiple Regression",
    "section": "Regression line: slope",
    "text": "Regression line: slope"
  },
  {
    "objectID": "3-MultipleRegression.html#regression-line-intercept",
    "href": "3-MultipleRegression.html#regression-line-intercept",
    "title": "Multiple Regression",
    "section": "Regression line: intercept",
    "text": "Regression line: intercept"
  },
  {
    "objectID": "3-MultipleRegression.html#correlation",
    "href": "3-MultipleRegression.html#correlation",
    "title": "Multiple Regression",
    "section": "Correlation",
    "text": "Correlation"
  },
  {
    "objectID": "3-MultipleRegression.html#example-modeling-cars",
    "href": "3-MultipleRegression.html#example-modeling-cars",
    "title": "Multiple Regression",
    "section": "Example: Modeling cars",
    "text": "Example: Modeling cars\n\n\n\n\n\n\n\n\n\n\nDescribe: What is the relationship between cars’ weights and their mileage?\n\n\n\n\n\n\n\n\n\n\n\nPredict: What is your best guess for a car’s MPG that weighs 3,500 pounds?\n\n\n\n\n\n\n\n\n\n\n\nModelling vocabulary\n\nPredictor (explanatory variable)\nOutcome (response variable)\nRegression line\n\nSlope\nIntercept\n\n\n\nPredictor (explanatory variable)\n\n\n\n\n\n\n\n\n\n\nmpg\nwt\n\n\n\n\n21\n2.62\n\n\n21\n2.875\n\n\n22.8\n2.32\n\n\n21.4\n3.215\n\n\n18.7\n3.44\n\n\n18.1\n3.46\n\n\n...\n...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutcome (response variable)\n\n\n\n\n\n\n\n\n\n\nmpg\nwt\n\n\n\n\n21\n2.62\n\n\n21\n2.875\n\n\n22.8\n2.32\n\n\n21.4\n3.215\n\n\n18.7\n3.44\n\n\n18.1\n3.46\n\n\n...\n...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression line\n\n\n\n\n\n\n\n\n\n\n\nRegression line: slope\n\n\n\n\n\n\n\n\n\n\n\nRegression line: intercept\n\n\n\n\n\n\n\n\n\n\n\n\nR Code: Modeling cars\n\nmtcars_mod &lt;- lm(mpg ~ wt, data = mtcars)\nmtcars_mod\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nCoefficients:\n(Intercept)           wt  \n     37.285       -5.344  \n\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(\n    x = \"Weight (1,000 lbs)\",\n    y = \"Miles per gallon (MPG)\",\n    title = \"MPG vs. weights of cars\"\n  )"
  },
  {
    "objectID": "3-MultipleRegression.html#r-code",
    "href": "3-MultipleRegression.html#r-code",
    "title": "Multiple Regression",
    "section": "R Code",
    "text": "R Code\n\nmtcars_mod &lt;- lm(mpg ~ wt, data = mtcars)\nmtcars_mod\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nCoefficients:\n(Intercept)           wt  \n     37.285       -5.344  \n\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(\n    x = \"Weight (1,000 lbs)\",\n    y = \"Miles per gallon (MPG)\",\n    title = \"MPG vs. weights of cars\"\n  ) \n\n\n\n\n\n\n\n\nHowever, it is rare to find a dataset with only two variables. In many research and applied fields, many different variables are measured at the same time, for various reasons. In many cases, we may actually have too many variables and may end up selecting a subset of them (only the most important) to proceed with the analysis. (NB: there are many ways to define variable importance, and we will look at some of them in depth throughout the Data Science programme)."
  },
  {
    "objectID": "3-MultipleRegression.html#multiple-regression",
    "href": "3-MultipleRegression.html#multiple-regression",
    "title": "Multiple Regression",
    "section": "Multiple Regression",
    "text": "Multiple Regression\nMultiple linear regression is used to model the relationship between a continuous outcome variable and two or more predictor variables. It extends simple linear regression by incorporating multiple predictors to explain variations in the outcome variable. The model estimates coefficients for each predictor, allowing researchers to assess their individual contributions while controlling for others, making it useful for predicting outcomes and identifying key influencing factors."
  },
  {
    "objectID": "3-MultipleRegression.html#example-1-the-crab-dataset",
    "href": "3-MultipleRegression.html#example-1-the-crab-dataset",
    "title": "Multiple Regression",
    "section": "Example 1: The Crab Dataset",
    "text": "Example 1: The Crab Dataset\nLet’s have a look at an example where we have many predictor variables. The crabs dataset from package MASS has 200 observations on 2 qualitative variables (species colour and sex), and 5 morphological measurements (frontal lobe size, rear width, carapace length and width, and body depth) of crabs of the species Leptograspus variegatus.\n\n\n\nExploratory Analysis\n\ncrab_dat &lt;- MASS::crabs\nglimpse(crab_dat)\n\nRows: 200\nColumns: 8\n$ sp    &lt;fct&gt; B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B…\n$ sex   &lt;fct&gt; M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M…\n$ index &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 1…\n$ FL    &lt;dbl&gt; 8.1, 8.8, 9.2, 9.6, 9.8, 10.8, 11.1, 11.6, 11.8, 11.8, 12.2, 12.…\n$ RW    &lt;dbl&gt; 6.7, 7.7, 7.8, 7.9, 8.0, 9.0, 9.9, 9.1, 9.6, 10.5, 10.8, 11.0, 1…\n$ CL    &lt;dbl&gt; 16.1, 18.1, 19.0, 20.1, 20.3, 23.0, 23.8, 24.5, 24.2, 25.2, 27.3…\n$ CW    &lt;dbl&gt; 19.0, 20.8, 22.4, 23.1, 23.0, 26.5, 27.1, 28.4, 27.8, 29.3, 31.6…\n$ BD    &lt;dbl&gt; 7.0, 7.4, 7.7, 8.2, 8.2, 9.8, 9.8, 10.4, 9.7, 10.3, 10.9, 11.4, …\n\nlibrary(GGally)\nlibrary(ggplot2)\nggpairs(crab_dat, columns = 4:8, aes(colour=sex)) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nDescribe: What are the relationships between the variables?\n\n\n\nExploratory Modelling\nImagine we are interested in predicting the carapace length (CL) of this species of crab. We can create a linear regression model that includes multiple predictors, such as \\[\\hat{\\mbox{CL}}=\\beta_0+\\beta_1\\mbox{FL}+\\beta_2\\mbox{RW}+\\beta_3\\mbox{CW}+\\beta_4\\mbox{BD}\\]\nThis model can be fit in R by executing:\n\ncrab_mod1 &lt;- lm(CL ~ FL + RW + CW + BD, data = crab_dat)\ncrab_mod1\n\n\nCall:\nlm(formula = CL ~ FL + RW + CW + BD, data = crab_dat)\n\nCoefficients:\n(Intercept)           FL           RW           CW           BD  \n     0.3163       0.2649      -0.1779       0.6402       0.4714  \n\n\nWe have that \\[\\hat{\\mbox{CL}}=0.32+0.26\\mbox{FL}-0.18\\mbox{RW}+0.64\\mbox{CW}+0.47\\mbox{BD}\\].\nInterpretation:\n\nAs FL increases by one unit, holding the other predictors constant, then carapace length will increase by 0.26.\nAs RW increases by one unit, holding the other predictors constant, then carapace length will decrease by 0.18.\nAs CW increases by one unit, holding the other predictors constant, then carapace length will increase by 0.64.\nAs BD increases by one unit, holding the other predictors constant, then carapace length will increase by 0.47.\n\n\nQuestion: How well does the model do at predicting carapace length?\n\n\ncrab_dat &lt;- crab_dat %&gt;% \n              mutate(CL_pred = predict(crab_mod1))\n\nggplot(data = crab_dat,\n       mapping = aes(x = CL, y = CL_pred, colour = sex)) +\n  theme_bw() +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  xlab(\"Observed CL\") + \n  ylab(\"Predicted CL\")\n\n\n\n\n\n\n\n\n\nQuestion: How well would we have done if we only used one predictor variable, say RW?\n\n\ncrab_mod2 &lt;- lm(CL ~ RW, data = crab_dat)\ncrab_mod2\n\n\nCall:\nlm(formula = CL ~ RW, data = crab_dat)\n\nCoefficients:\n(Intercept)           RW  \n      0.645        2.470  \n\ncrab_dat &lt;- crab_dat %&gt;% \n              mutate(CL_pred2 = predict(crab_mod2))\n\nggplot(data = crab_dat,\n       mapping = aes(x = CL, y = CL_pred2, colour = sex)) +\n  theme_bw() +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  xlab(\"Observed CL\") + \n  ylab(\"Predicted CL\")\n\n\n\n\n\n\n\n\n\nQuestion: What about including the categorical variables, namely sex or sp (species colour)?\n\nWe can include categorical variables in our model, such that\n\\[\\hat{\\mbox{CL}}=\\beta_0+\\beta_1\\mbox{RW}+\\beta_2I(\\mbox{sex}=\\mbox{M})\\] (NB: \\(I(\\mbox{sex}=\\mbox{M})\\) is an indicator function, which is equal to 1 if sex is equal to M and zero otherwise. The same type of interpretation can be drawn from \\(I(\\mbox{sp}=\\mbox{O})\\))\n\ncrab_mod3 &lt;- lm(CL ~ RW + sex, data = crab_dat)\ncrab_mod3\n\n\nCall:\nlm(formula = CL ~ RW + sex, data = crab_dat)\n\nCoefficients:\n(Intercept)           RW         sexM  \n     -6.293        2.792        5.670  \n\ncrab_dat &lt;- crab_dat %&gt;% \n              mutate(CL_pred3 = predict(crab_mod3))\n\nggplot(data = crab_dat,\n       mapping = aes(x = CL, y = CL_pred3, colour = sex)) +\n  theme_bw() +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  xlab(\"Observed CL\") + \n  ylab(\"Predicted CL\")\n\n\n\n\n\n\n\n\n\nQuestion: How do we interpret the coefficients related to sex?\n\nFor a male crab the expected carapace length increases by 5.67 compared to a female crab.\n\n\nModel Comparison\nOne way of comparing how well our modelling strategies did in terms of predictions is to sum the squared discrepancies (residuals) between predicted and observed values, and see which one is smaller.\n\ndiscrepancy_mod1 &lt;- sum(residuals(crab_mod1)^2)\ndiscrepancy_mod2 &lt;- sum(residuals(crab_mod2)^2)\ndiscrepancy_mod3 &lt;- sum(residuals(crab_mod3)^2)\n\n\ndiscrepancy_mod1\n\n[1] 27.35058\n\ndiscrepancy_mod2\n\n[1] 2047.417\n\ndiscrepancy_mod3\n\n[1] 576.4921\n\n\nTypically we would split the data into training and test set, use only the training set to fit the model, and then perform this computation on both sets. We will see more details on how to compare the models in terms of their predictive power in the module Statistical Machine Learning. We will also see more details on this in the modules Linear Models I and II, how to properly test hypotheses, how to assess goodness-of-fit, what the important assumptions are and how to properly check them."
  },
  {
    "objectID": "3-MultipleRegression.html#logistic-regression",
    "href": "3-MultipleRegression.html#logistic-regression",
    "title": "Multiple Regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nLogistic regression is a generalized regression model where the outcome is a two-level categorical variable. The outcome, takes the value 1 or 0 with probability. Ultimately, it is the probability of the outcome taking the value 1 (i.e., being a “success”) that we model in relation to the predictor variables. For this to work, we transform the expected outcome in such a way that it will be bounded between 0 and 1, and hence our estimates will be sensical. The transformation we use is called the logit, and is the natural logarithm of the odds of success.\n\nModelling binary outcomes\n\n\\(y\\) takes on values 0 (failure) or 1 (success)\n\\(p\\): probability of success\n\\(1-p\\): probability of failure\nWe can’t model \\(y\\) directly, so instead we model \\(p\\)\n\nLinear model\n\\[\n\\hat{p}_i = \\beta_o + \\beta_1 \\times x\n\\]\n\nBut remember that \\(p\\) must be between 0 and 1\nWe need a link function that transforms the linear model to have an appropriate range\n\nLogit link function\nThe logit function takes values between 0 and 1 (probabilities) and maps them to values in the range negative infinity to positive infinity:\n\\[\nlogit(p) = log \\bigg( \\frac{p}{1 - p} \\bigg)\n\\]\nGeneralized linear model\n\nWe model the logit (log-odds) of \\(p\\) :\n\n\\[\nlogit(\\hat{p}) = log \\bigg( \\frac{\\hat{p}}{1 - \\hat{p}} \\bigg) = \\beta_o + \\beta_1 \\times x\n\\]\n\nThen take the inverse to obtain the predicted \\(p\\):\n\n\\[\n\\hat{p} = \\frac{e^{\\beta_o + \\beta_1 \\times x }}{1 + e^{\\beta_o + \\beta_1 \\times x }}\n\\]\nA logistic model visualized"
  },
  {
    "objectID": "3-MultipleRegression.html#example-2-the-iris-dataset",
    "href": "3-MultipleRegression.html#example-2-the-iris-dataset",
    "title": "Multiple Regression",
    "section": "Example 2: The Iris Dataset",
    "text": "Example 2: The Iris Dataset\nYou will have explored the Iris dataset initially last semester in DS151.\nIris is a genus of about 300 species of flowering plants, taking its name from the Greek word for a rainbow (which is also the name for the Greek goddess of rainbows, Iris). The flowers are very showy, and we would like to know if it is possible to identify some of the species based on measurements of different parts of the flowers.\nThe dataset gives the measurements in centimeters of the variables:\n\nsepal length\nsepal width\npetal length\npetal width\n\nfor 50 flowers from each of three species of Iris: Iris setosa, Iris versicolor, and Iris virginica.\nThis dataset is available in base R as iris. Let’s have a look:\n\nlibrary(tidyverse)\nglimpse(iris)\n\nRows: 150\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n\nLet’s work, initially, only with species versicolor and virginica. We create a plot that shows in the \\(x\\) axis the petal length, and in the \\(y\\) axis only the values of 0 and 1, representing whether the observation belongs to class virginica or not (0 = the observation belongs to class versicolor; 1 = it belongs to class virginica). We call this a binary (or dummy) variable.\n\niris2 &lt;- iris %&gt;%\n  filter(Species != \"setosa\") %&gt;%\n  mutate(Species.binary = as.numeric(Species == \"virginica\"))\n  \nggplot(iris2, aes(x = Petal.Length, y = Species.binary)) +\n  geom_point() +\n  theme_bw()\n\n\n\n\n\n\n\n\nWe may interpret the numbers 0 and 1 here as the probability \\(p\\) of belonging to class virginica. It appears that as the petal length increases, so does the likelihood of belonging to the virginica class.\n\nggplot(iris2, aes(x = Petal.Length, y = Species.binary)) +\n  geom_point() +\n  geom_smooth(method = glm, method.args = list(family = \"binomial\"), se = FALSE) +\n  theme_bw()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nIncluding more predictors\nIt is frequently of interest to use multiple covariates to improve our predictive power. Let’s fit a logistic regression model including all four covariates as predictors in our model: \\[\\displaystyle\\log\\left(\\frac{p}{1-p}\\right)=\\beta_0+\\beta_1*\\mbox{Sepal.Length}+\\beta_2*\\mbox{Sepal.Width}+\\beta_3*\\mbox{Petal.Length}+\\beta_4*\\mbox{Petal.Width}\\]\n\nfull_logistic_reg &lt;- glm(Species.binary ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width,\n                         family = binomial, data = iris2)\nfull_logistic_reg %&gt;% coef %&gt;% round(digits = 4)\n\n (Intercept) Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n    -42.6378      -2.4652      -6.6809       9.4294      18.2861 \n\n\nHow well did we do?\n\niris_predict &lt;- iris2 %&gt;% \n                  mutate(p_hat = predict(full_logistic_reg, type = \"response\") %&gt;% round(2),\n                         Species_pred = ifelse(p_hat &gt;= 0.5,\"virginica\",\"versicolor\"))\n\nn_correct_full &lt;- sum(iris_predict$Species_pred == iris_predict$Species)\nn_correct_full\n\n[1] 98\n\n\nOur model now correctly predicts the class of 98 out of 100 observations."
  },
  {
    "objectID": "3-MultipleRegression.html#example-the-iris-dataset",
    "href": "3-MultipleRegression.html#example-the-iris-dataset",
    "title": "Multiple Regression",
    "section": "Example: The Iris Dataset",
    "text": "Example: The Iris Dataset\nYou will have explored the Iris dataset initially last semester in DS151.\nIris is a genus of about 300 species of flowering plants, taking its name from the Greek word for a rainbow (which is also the name for the Greek goddess of rainbows, Iris). The flowers are very showy, and we would like to know if it is possible to identify some of the species based on measurements of different parts of the flowers.\nThe dataset gives the measurements in centimeters of the variables:\n\nsepal length\nsepal width\npetal length\npetal width\n\nfor 50 flowers from each of three species of Iris: Iris setosa, Iris versicolor, and Iris virginica.\nThis dataset is available in base R as iris. Let’s have a look:\n\nlibrary(tidyverse)\nglimpse(iris)\n\nRows: 150\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n\n\nExploratory Analysis\nLet’s work, initially, only with species versicolor and virginica. We create a plot that shows in the \\(x\\) axis the petal length, and in the \\(y\\) axis only the values of 0 and 1, representing whether the observation belongs to class virginica or not (0 = the observation belongs to class versicolor; 1 = it belongs to class virginica). We call this a binary (or dummy) variable.\n\niris2 &lt;- iris %&gt;%\n  filter(Species != \"setosa\") %&gt;%\n  mutate(Species.binary = as.numeric(Species == \"virginica\"))\n  \nggplot(iris2, aes(x = Petal.Length, y = Species.binary)) +\n  geom_point() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\nExploratory Modelling\nA simple logistic regression model can be fit, such that \\[\\displaystyle\\log\\left(\\frac{p}{1-p}\\right)=\\beta_0+\\beta_1*\\mbox{Petal.Length}\\]\nDo this in R by executing:\n\niris_mod1 &lt;- glm(Species.binary ~ Petal.Length,\n                 family = binomial,\n                 data = iris2)\niris_mod1 %&gt;% coef() %&gt;% round(digits = 4)\n\n (Intercept) Petal.Length \n    -43.7809       9.0020 \n\n\n\nggplot(iris2, aes(x = Petal.Length, y = Species.binary)) +\n  geom_point() +\n  geom_smooth(method = glm, method.args = list(family = \"binomial\"), se = FALSE) +\n  ylab(\"p\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nWe may interpret the y-axis numbers here as the probability \\(p\\) of belonging to class virginica. It appears that as the petal length increases, so does the likelihood of belonging to the virginica class."
  },
  {
    "objectID": "3-MultipleRegression.html#exploratory-analysis-1",
    "href": "3-MultipleRegression.html#exploratory-analysis-1",
    "title": "Multiple Regression",
    "section": "Exploratory Analysis",
    "text": "Exploratory Analysis\nLet’s work, initially, only with species versicolor and virginica. We create a plot that shows in the \\(x\\) axis the petal length, and in the \\(y\\) axis only the values of 0 and 1, representing whether the observation belongs to class virginica or not (0 = the observation belongs to class versicolor; 1 = it belongs to class virginica). We call this a binary (or dummy) variable.\n\niris2 &lt;- iris %&gt;%\n  filter(Species != \"setosa\") %&gt;%\n  mutate(Species.binary = as.numeric(Species == \"virginica\"))\n  \nggplot(iris2, aes(x = Petal.Length, y = Species.binary)) +\n  geom_point() +\n  theme_bw()"
  },
  {
    "objectID": "3-MultipleRegression.html#exploratory-modelling-1",
    "href": "3-MultipleRegression.html#exploratory-modelling-1",
    "title": "Multiple Regression",
    "section": "Exploratory Modelling",
    "text": "Exploratory Modelling\nA simple logistic regression model can be fit, such that \\[\\displaystyle\\log\\left(\\frac{p}{1-p}\\right)=\\beta_0+\\beta_1*\\mbox{Petal.Length}\\]\nDo this in R by executing:\n\niris_mod1 &lt;- glm(Species.binary ~ Petal.Length,\n                 family = binomial,\n                 data = iris2)\niris_mod1 %&gt;% coef() %&gt;% round(digits = 4)\n\n (Intercept) Petal.Length \n    -43.7809       9.0020 \n\n\n\nggplot(iris2, aes(x = Petal.Length, y = Species.binary)) +\n  geom_point() +\n  geom_smooth(method = glm, method.args = list(family = \"binomial\"), se = FALSE) +\n  ylab(\"p\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nWe may interpret the y-axis numbers here as the probability \\(p\\) of belonging to class virginica. It appears that as the petal length increases, so does the likelihood of belonging to the virginica class.\n\nIncluding more predictors\nIt is frequently of interest to use multiple covariates to improve our predictive power. Let’s fit a logistic regression model including all four covariates as predictors in our model: \\[\\displaystyle\\log\\left(\\frac{p}{1-p}\\right)=\\beta_0+\\beta_1*\\mbox{Sepal.Length}+\\beta_2*\\mbox{Sepal.Width}+\\beta_3*\\mbox{Petal.Length}+\\beta_4*\\mbox{Petal.Width}\\]\n\niris_mod2 &lt;- glm(Species.binary ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width,\n                 family = binomial, \n                 data = iris2)\niris_mod2 %&gt;% coef %&gt;% round(digits = 4)\n\n (Intercept) Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n    -42.6378      -2.4652      -6.6809       9.4294      18.2861 \n\n\n\n\nClassification\n\nQuestion: How well did we do at classifying the Iris species?\n\n\niris_predict &lt;- iris2 %&gt;% \n                  mutate(p_hat = predict(iris_mod2, type = \"response\") %&gt;% round(2),\n                         Species_pred = ifelse(p_hat &gt;= 0.5,\"virginica\",\"versicolor\"))\n\nn_correct_full &lt;- sum(iris_predict$Species_pred == iris_predict$Species)\nn_correct_full\n\n[1] 98\n\n\nOur model correctly predicts the species class of 98 out of 100 observations."
  },
  {
    "objectID": "3-MultipleRegression.html#example-the-wine-dataset",
    "href": "3-MultipleRegression.html#example-the-wine-dataset",
    "title": "Multiple Regression",
    "section": "Example: The Wine Dataset",
    "text": "Example: The Wine Dataset\nThe wine dataset contains the results of a chemical analysis of wines grown in a specific area of Italy. Three types of wine are represented in the 178 samples, with the results of 13 chemical analyses recorded for each sample. The Type variable has been transformed into a categoric variable.\nThe data contains no missing values and consits of only numeric data, with a three class target variable (Type) for classification.\nLet’s have a glimpse at the dataset:\n\nwine &lt;- read_csv(\"https://www.dropbox.com/s/l5x4ur06gfhpg0h/wine.csv?raw=1\") \n\nRows: 178 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): Type\ndbl (13): Alcohol, Malic, Ash, Alcalinity, Magnesium, Phenols, Flavanoids, N...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(wine)\n\nRows: 178\nColumns: 14\n$ Type                 &lt;chr&gt; \"type1\", \"type1\", \"type1\", \"type1\", \"type1\", \"typ…\n$ Alcohol              &lt;dbl&gt; 14.23, 13.20, 13.16, 14.37, 13.24, 14.20, 14.39, …\n$ Malic                &lt;dbl&gt; 1.71, 1.78, 2.36, 1.95, 2.59, 1.76, 1.87, 2.15, 1…\n$ Ash                  &lt;dbl&gt; 2.43, 2.14, 2.67, 2.50, 2.87, 2.45, 2.45, 2.61, 2…\n$ Alcalinity           &lt;dbl&gt; 15.6, 11.2, 18.6, 16.8, 21.0, 15.2, 14.6, 17.6, 1…\n$ Magnesium            &lt;dbl&gt; 127, 100, 101, 113, 118, 112, 96, 121, 97, 98, 10…\n$ Phenols              &lt;dbl&gt; 2.80, 2.65, 2.80, 3.85, 2.80, 3.27, 2.50, 2.60, 2…\n$ Flavanoids           &lt;dbl&gt; 3.06, 2.76, 3.24, 3.49, 2.69, 3.39, 2.52, 2.51, 2…\n$ Nonflavanoid.phenols &lt;dbl&gt; 0.28, 0.26, 0.30, 0.24, 0.39, 0.34, 0.30, 0.31, 0…\n$ Proanth              &lt;dbl&gt; 2.29, 1.28, 2.81, 2.18, 1.82, 1.97, 1.98, 1.25, 1…\n$ Color                &lt;dbl&gt; 5.64, 4.38, 5.68, 7.80, 4.32, 6.75, 5.25, 5.05, 5…\n$ Hue                  &lt;dbl&gt; 1.04, 1.05, 1.03, 0.86, 1.04, 1.05, 1.02, 1.06, 1…\n$ OD                   &lt;dbl&gt; 3.92, 3.40, 3.17, 3.45, 2.93, 2.85, 3.58, 3.58, 2…\n$ Proline              &lt;dbl&gt; 1065, 1050, 1185, 1480, 735, 1450, 1290, 1295, 10…\n\n\nChange the code below and make different plots with other covariate combinations. What sort of patterns begin to emerge?\n\nggplot(wine, aes(x = Ash, y = Malic, colour = Type)) +\n  geom_point() +\n  theme_bw() +\n  labs(colour = \"Wine Type\")\n\n\n\n\n\n\n\n\nWe will work only with types 1 and 2\n\nwine2 &lt;- wine %&gt;%\n  filter(Type != \"type3\") %&gt;%\n  mutate(Type_binary = as.numeric(Type == \"type1\")) %&gt;% \n  as_tibble\nwine2\n\n# A tibble: 130 × 15\n   Type  Alcohol Malic   Ash Alcalinity Magnesium Phenols Flavanoids\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1 type1    14.2  1.71  2.43       15.6       127    2.8        3.06\n 2 type1    13.2  1.78  2.14       11.2       100    2.65       2.76\n 3 type1    13.2  2.36  2.67       18.6       101    2.8        3.24\n 4 type1    14.4  1.95  2.5        16.8       113    3.85       3.49\n 5 type1    13.2  2.59  2.87       21         118    2.8        2.69\n 6 type1    14.2  1.76  2.45       15.2       112    3.27       3.39\n 7 type1    14.4  1.87  2.45       14.6        96    2.5        2.52\n 8 type1    14.1  2.15  2.61       17.6       121    2.6        2.51\n 9 type1    14.8  1.64  2.17       14          97    2.8        2.98\n10 type1    13.9  1.35  2.27       16          98    2.98       3.15\n# ℹ 120 more rows\n# ℹ 7 more variables: Nonflavanoid.phenols &lt;dbl&gt;, Proanth &lt;dbl&gt;, Color &lt;dbl&gt;,\n#   Hue &lt;dbl&gt;, OD &lt;dbl&gt;, Proline &lt;dbl&gt;, Type_binary &lt;dbl&gt;\n\n\nChange the code below and fit a logistic regression model using the four predictors you believe are the best to classify the two types of wine. Also play around with different thresholds for the classification rule. Compare with your peers. What predictors yielded the best predictive performance?\n\n# include the predictors below\nlogistic_reg &lt;- glm(Type_binary ~ Malic + Ash + Alcalinity + Magnesium,\n                           family = binomial, data = wine2)\n\n# set the threshold for the classification rule below\nthreshold &lt;- 0.50\n\n# computes the percentage of correct predictions\nwine_predict &lt;- wine2 %&gt;% \n                  mutate(p_hat = predict(logistic_reg, type = \"response\") %&gt;% round(2),\n                         Type_pred = ifelse(p_hat &gt;= threshold, \"type1\",\"type2\")) \n\n\nn_correct &lt;- sum(wine_predict$Type_pred == wine_predict$Type)\nn_total &lt;- nrow(wine2)\npercentage_correct &lt;- n_correct/n_total * 100\npercentage_correct\n\n[1] 86.92308"
  },
  {
    "objectID": "3-MultipleRegression.html#example-the-crab-dataset",
    "href": "3-MultipleRegression.html#example-the-crab-dataset",
    "title": "Multiple Regression",
    "section": "Example: The Crab Dataset",
    "text": "Example: The Crab Dataset\nLet’s have a look at an example where we have many predictor variables. The crabs dataset from package MASS has 200 observations on 2 qualitative variables (species colour and sex), and 5 morphological measurements (frontal lobe size, rear width, carapace length and width, and body depth) of crabs of the species Leptograspus variegatus.\n\n\n\nExploratory Analysis\n\ncrab_dat &lt;- MASS::crabs\nglimpse(crab_dat)\n\nRows: 200\nColumns: 8\n$ sp    &lt;fct&gt; B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B…\n$ sex   &lt;fct&gt; M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M…\n$ index &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 1…\n$ FL    &lt;dbl&gt; 8.1, 8.8, 9.2, 9.6, 9.8, 10.8, 11.1, 11.6, 11.8, 11.8, 12.2, 12.…\n$ RW    &lt;dbl&gt; 6.7, 7.7, 7.8, 7.9, 8.0, 9.0, 9.9, 9.1, 9.6, 10.5, 10.8, 11.0, 1…\n$ CL    &lt;dbl&gt; 16.1, 18.1, 19.0, 20.1, 20.3, 23.0, 23.8, 24.5, 24.2, 25.2, 27.3…\n$ CW    &lt;dbl&gt; 19.0, 20.8, 22.4, 23.1, 23.0, 26.5, 27.1, 28.4, 27.8, 29.3, 31.6…\n$ BD    &lt;dbl&gt; 7.0, 7.4, 7.7, 8.2, 8.2, 9.8, 9.8, 10.4, 9.7, 10.3, 10.9, 11.4, …\n\nlibrary(GGally)\nlibrary(ggplot2)\nggpairs(crab_dat, columns = 4:8, aes(colour=sex)) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nDescribe: What are the relationships between the variables?\n\n\n\nExploratory Modelling\nImagine we are interested in predicting the carapace length (CL) of this species of crab. We can create a linear regression model that includes multiple predictors, such as \\[\\hat{\\mbox{CL}}=\\beta_0+\\beta_1\\mbox{FL}+\\beta_2\\mbox{RW}+\\beta_3\\mbox{CW}+\\beta_4\\mbox{BD}\\]\nThis model can be fit in R by executing:\n\ncrab_mod1 &lt;- lm(CL ~ FL + RW + CW + BD, data = crab_dat)\ncrab_mod1\n\n\nCall:\nlm(formula = CL ~ FL + RW + CW + BD, data = crab_dat)\n\nCoefficients:\n(Intercept)           FL           RW           CW           BD  \n     0.3163       0.2649      -0.1779       0.6402       0.4714  \n\n\nWe have that \\[\\hat{\\mbox{CL}}=0.32+0.26\\mbox{FL}-0.18\\mbox{RW}+0.64\\mbox{CW}+0.47\\mbox{BD}\\].\nInterpretation:\n\nAs FL increases by one unit, holding the other predictors constant, then carapace length will increase by 0.26.\nAs RW increases by one unit, holding the other predictors constant, then carapace length will decrease by 0.18.\nAs CW increases by one unit, holding the other predictors constant, then carapace length will increase by 0.64.\nAs BD increases by one unit, holding the other predictors constant, then carapace length will increase by 0.47.\n\n\nQuestion: How well does the model do at predicting carapace length?\n\n\ncrab_dat &lt;- crab_dat %&gt;% \n              mutate(CL_pred = predict(crab_mod1))\n\nggplot(data = crab_dat,\n       mapping = aes(x = CL, y = CL_pred, colour = sex)) +\n  theme_bw() +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  xlab(\"Observed CL\") + \n  ylab(\"Predicted CL\")\n\n\n\n\n\n\n\n\n\nQuestion: How well would we have done if we only used one predictor variable, say RW?\n\n\ncrab_mod2 &lt;- lm(CL ~ RW, data = crab_dat)\ncrab_mod2\n\n\nCall:\nlm(formula = CL ~ RW, data = crab_dat)\n\nCoefficients:\n(Intercept)           RW  \n      0.645        2.470  \n\ncrab_dat &lt;- crab_dat %&gt;% \n              mutate(CL_pred2 = predict(crab_mod2))\n\n\nggplot(data = crab_dat,\n      aes(x = RW, y = CL)) +\n  theme_bw() +\n  geom_point() +\n  geom_line(aes(x = RW, y = CL_pred2)) \n\n\n\n\n\n\n\nggplot(data = crab_dat,\n       aes(x = CL, y = CL_pred2)) +\n  theme_bw() +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  xlab(\"Observed CL\") + \n  ylab(\"Predicted CL\")\n\n\n\n\n\n\n\n\n\nQuestion: What about including the categorical variables, namely sex or sp (species colour)?\n\nWe can include categorical variables in our model, such that\n\\[\\hat{\\mbox{CL}}=\\beta_0+\\beta_1\\mbox{RW}+\\beta_2I(\\mbox{sex}=\\mbox{M})\\] (NB: \\(I(\\mbox{sex}=\\mbox{M})\\) is an indicator function, which is equal to 1 if sex is equal to M and zero otherwise. The same type of interpretation can be drawn from \\(I(\\mbox{sp}=\\mbox{O})\\))\n\ncrab_mod3 &lt;- lm(CL ~ RW + sex, data = crab_dat)\ncrab_mod3\n\n\nCall:\nlm(formula = CL ~ RW + sex, data = crab_dat)\n\nCoefficients:\n(Intercept)           RW         sexM  \n     -6.293        2.792        5.670  \n\ncrab_dat &lt;- crab_dat %&gt;% \n              mutate(CL_pred3 = predict(crab_mod3))\n\nggplot(data = crab_dat,\n      aes(x = RW, y = CL, colour = sex)) +\n  theme_bw() +\n  geom_point() +\n  geom_line(aes(x = RW, y = CL_pred3, colour = sex)) \n\n\n\n\n\n\n\nggplot(data = crab_dat,\n       aes(x = CL, y = CL_pred3, colour = sex)) +\n  theme_bw() +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  xlab(\"Observed CL\") + \n  ylab(\"Predicted CL\")\n\n\n\n\n\n\n\n\n\nQuestion: How do we interpret the coefficients related to sex?\n\nFor a male crab the expected carapace length increases by 5.67 compared to a female crab.\n\n\nModel Comparison\nOne way of comparing how well our modelling strategies did in terms of predictions is to sum the squared discrepancies (residuals) between predicted and observed values, and see which one is smaller.\n\ndiscrepancy_mod1 &lt;- sum(residuals(crab_mod1)^2)\ndiscrepancy_mod2 &lt;- sum(residuals(crab_mod2)^2)\ndiscrepancy_mod3 &lt;- sum(residuals(crab_mod3)^2)\n\n\ndiscrepancy_mod1\n\n[1] 27.35058\n\ndiscrepancy_mod2\n\n[1] 2047.417\n\ndiscrepancy_mod3\n\n[1] 576.4921\n\n\nTypically we would split the data into training and test set, use only the training set to fit the model, and then perform this computation on both sets. We will see more details on how to compare the models in terms of their predictive power in the module Statistical Machine Learning. We will also see more details on this in the modules Linear Models I and II, how to properly test hypotheses, how to assess goodness-of-fit, what the important assumptions are and how to properly check them."
  },
  {
    "objectID": "ObservationalStudies.html",
    "href": "ObservationalStudies.html",
    "title": "Observational Studies",
    "section": "",
    "text": "Observational\n\nCollect data in a way that does not directly interfere with how the data arise, i.e. merely “observe”;\nBased on an observational study, we can only establish an association, in other words correlation, between the explanatory and response variables;\nWith an observational study you are just observing the data and collecting it after or as it occurs.\n\nExperimental\n\nRandomly assign subjects to treatments\nEstablish causal connections"
  },
  {
    "objectID": "ObservationalStudies.html#scientific-studies",
    "href": "ObservationalStudies.html#scientific-studies",
    "title": "Observational Studies",
    "section": "",
    "text": "Observational\n\nCollect data in a way that does not directly interfere with how the data arise, i.e. merely “observe”;\nBased on an observational study, we can only establish an association, in other words correlation, between the explanatory and response variables;\nWith an observational study you are just observing the data and collecting it after or as it occurs.\n\nExperimental\n\nRandomly assign subjects to treatments\nEstablish causal connections"
  },
  {
    "objectID": "ObservationalStudies.html#studies-and-conclusions",
    "href": "ObservationalStudies.html#studies-and-conclusions",
    "title": "Observational Studies",
    "section": "Studies and Conclusions",
    "text": "Studies and Conclusions"
  },
  {
    "objectID": "ObservationalStudies.html#why-use-observational-studies",
    "href": "ObservationalStudies.html#why-use-observational-studies",
    "title": "Observational Studies",
    "section": "Why use observational studies?",
    "text": "Why use observational studies?\nReasons why we must sometimes use an observational study instead of an experiment …\n\nIt is unethical or impossible to assign people to receive a specific treatment.\n\nFor example, if you want to know the impact of smoking on cancer you cannot design a study and assign some people to be smokers and other to be non-smokers. That is not ethical.\n\nCertain exposure variables are inherent traits and cannot be randomly assigned."
  },
  {
    "objectID": "ObservationalStudies.html#observational-studies",
    "href": "ObservationalStudies.html#observational-studies",
    "title": "Observational Studies",
    "section": "Observational studies",
    "text": "Observational studies\n\nObservational studies are very useful when it is not possible to design a study\nObervational studies often have large sample sizes\nThey are cheaper than designing a study (for example, clinical trials in medicine are very expensive)\nYou have to get to know the data a lot better with observational studies. You haven’t set up to study or controlled for influences other than the thing you are studying."
  },
  {
    "objectID": "ObservationalStudies.html#retrospective-vs-prospective-studies",
    "href": "ObservationalStudies.html#retrospective-vs-prospective-studies",
    "title": "Observational Studies",
    "section": "Retrospective Vs Prospective Studies",
    "text": "Retrospective Vs Prospective Studies\nIf an observational study uses data from the past, it is called retrospective study, whereas if data are collected throughout the study, it is called prospective;"
  },
  {
    "objectID": "ObservationalStudies.html#example-does-working-out-increase-energy-levels",
    "href": "ObservationalStudies.html#example-does-working-out-increase-energy-levels",
    "title": "Observational Studies",
    "section": "Example: Does working out increase energy levels?",
    "text": "Example: Does working out increase energy levels?\nWe want to evaluate if regularly working out has any impact on energy levels.\n\nIn an observational study, we sample two types of people from the population, those who choose to work out regularly and those who don’t.\nWe ask the people in each group to rate their enery levels from 1-10.\nThen, we find the average “energy level” for the two groups of people and compare."
  },
  {
    "objectID": "ObservationalStudies.html#example-does-working-out-increase-energy-levels-1",
    "href": "ObservationalStudies.html#example-does-working-out-increase-energy-levels-1",
    "title": "Observational Studies",
    "section": "Example: Does working out increase energy levels?",
    "text": "Example: Does working out increase energy levels?\n\nCan we conclude from this that working out is the cause of increased energy levels?"
  },
  {
    "objectID": "ObservationalStudies.html#example-does-working-out-increase-energy-levels-2",
    "href": "ObservationalStudies.html#example-does-working-out-increase-energy-levels-2",
    "title": "Observational Studies",
    "section": "Example: Does working out increase energy levels?",
    "text": "Example: Does working out increase energy levels?\n\nThere may be other variables that we didn’t control for in this study that contribute to the observed difference.\nFor example, people who have young children might have less time to work out and also have lower energy levels.\nThis is known as confounding.\nThis study allows us to make correlation statements. But, we cannot make a causal statement attributing increased energy levels to working out!"
  },
  {
    "objectID": "ObservationalStudies.html#confounding-variables",
    "href": "ObservationalStudies.html#confounding-variables",
    "title": "Observational Studies",
    "section": "Confounding variables",
    "text": "Confounding variables\nConfounding variables: Extraneous variables that affect both the exposure (e.g., working out) and the outcome variables (e.g., increased energy), and that make it seem like there is a relationship between them are called confounding variables."
  },
  {
    "objectID": "ObservationalStudies.html#example",
    "href": "ObservationalStudies.html#example",
    "title": "Observational Studies",
    "section": "Example",
    "text": "Example\nMany years ago, investigators reported an association between coffee drinking and pancreatic cancer in an observational study (MacMahon B, Yen S, Trichopoulos D, Warren K, Nardi G. Coffee and cancer of the pancreas. N Eng J Med 1981; 304: 630-3)."
  },
  {
    "objectID": "ObservationalStudies.html#example-1",
    "href": "ObservationalStudies.html#example-1",
    "title": "Observational Studies",
    "section": "Example",
    "text": "Example\nIf we take coffee as our exposure of interest and correlate it with an increased development of pancreatic cancer there is the potential, as was the case with these investigators, to be misled if there is a third causal factor, such as cigarette smoking, that was more common among those who reported drinking coffee.\n\nOnce the confounding variable, smoking is taken into account the correlation between coffee and pancreatic cancer disappears."
  },
  {
    "objectID": "ObservationalStudies.html#reducing-confounding-matching",
    "href": "ObservationalStudies.html#reducing-confounding-matching",
    "title": "Observational Studies",
    "section": "Reducing confounding: Matching",
    "text": "Reducing confounding: Matching\nMatching is a technique that involves selecting study participants with similar characteristics outside the outcome or exposure variables.\n\nRather than using random assignment to equalize the experimental groups, the experimenters do it by matching observable characteristics.\nFor every participant in the exposed group, the researchers find a participant with comparable traits to include in the control group.\nMatching subjects facilitates valid comparisons between those groups.\nThe researchers use subject-area knowledge to identify characteristics that are critical to match."
  },
  {
    "objectID": "ObservationalStudies.html#reducing-confounding-multiple-regression",
    "href": "ObservationalStudies.html#reducing-confounding-multiple-regression",
    "title": "Observational Studies",
    "section": "Reducing confounding: Multiple Regression",
    "text": "Reducing confounding: Multiple Regression\nMultiple regression models specify the way in which different characteristics/variables (exposure and confounders) affects the outcome, thereby isolating the effect of each variable.\nchance of cancer = a x (coffee) + b x (smoking) + c x (gender) + d x (age)\n\nthis allows us to make a statement about what would happen if one variable (i.e., the exposure) were to change while all the others (i.e., the confounders) remained the same.\nObtaining isolated exposure effects conditional on the other variables remaining constant is said to adjust for (or control for) the effect of these confounders"
  },
  {
    "objectID": "ObservationalStudies.html#conditional-probability",
    "href": "ObservationalStudies.html#conditional-probability",
    "title": "Observational Studies",
    "section": "Conditional Probability",
    "text": "Conditional Probability\nYou and your friend are trying to find the perfect restaurant for dinner. You can’t decide so you want to instead rely on some observational data (i.e., restaurant reviews).\nYou find two worthy restaurants, Carla’s and Sophia’s each with 400 reviews and an indicator of whether the restaurant is recommended or not recommended.\nYou find that\n\nrecommended for Sophia’s = 250/400\nrecommended for Carla’s = 216/400\n\nSo what we have is a conditional probability:\np(recommended|Sophia’s) = 62.5%\np(recommended|Carla’s) = 54%"
  },
  {
    "objectID": "ObservationalStudies.html#conditional-probability-1",
    "href": "ObservationalStudies.html#conditional-probability-1",
    "title": "Observational Studies",
    "section": "Conditional Probability",
    "text": "Conditional Probability\nWhat if we consider age as a factor here?\n\nrecommended for 18-35 yr old diners at Sophia’s = 50/150\nrecommended for 35+ diners at Sophia’s = 200/250\nrecommended for 18-35 yr old diners at Carla’s = 180/360\nrecommended for 35+ diners at Carla’s = 36/40\n\nSo what we have is:\np(recommended|Sophia’s, younger) = 30%\np(recommended|Sophia’s, older) = 80%\np(recommended|Carla’s, younger) = 50%\np(recommended|Carla’s, older) = 90%"
  },
  {
    "objectID": "ObservationalStudies.html#whats-going-on",
    "href": "ObservationalStudies.html#whats-going-on",
    "title": "Observational Studies",
    "section": "What’s going on?",
    "text": "What’s going on?\nYou have unknowingly entered the world of Simpson’s Paradox, where a restaurant can be both better and worse than its competitor, exercise can lower and increase the risk of disease, and the same dataset can be used to prove two opposing arguments. Instead of going out to dinner, perhaps you and your friend should spend the evening discussing this fascinating statistical phenomenon.\nSimpson’s Paradox occurs when trends that appear when a dataset is separated into groups reverse when the data are aggregated."
  },
  {
    "objectID": "ObservationalStudies.html#simpsons-paradox-correlation-reversal",
    "href": "ObservationalStudies.html#simpsons-paradox-correlation-reversal",
    "title": "Observational Studies",
    "section": "Simpsons’ Paradox: Correlation Reversal",
    "text": "Simpsons’ Paradox: Correlation Reversal\nSay we have data on the number of hours of exercise per week versus the risk of developing a disease for two sets of patients, those below the age of 50 and those over the age of 50. Here are individual plots showing the relationship between exercise and risk of disease."
  },
  {
    "objectID": "ObservationalStudies.html#example-continued",
    "href": "ObservationalStudies.html#example-continued",
    "title": "Observational Studies",
    "section": "Example Continued",
    "text": "Example Continued\nWe clearly saw a negative correlation, indicating that increased levels of exercise per week are correlated with a lower risk of developing the disease for both groups. Now, let’s combine the data together on a single plot:"
  },
  {
    "objectID": "ObservationalStudies.html#resolving-the-paradox",
    "href": "ObservationalStudies.html#resolving-the-paradox",
    "title": "Observational Studies",
    "section": "Resolving the Paradox",
    "text": "Resolving the Paradox\nTo avoid Simpson’s Paradox leading us to two opposite conclusions, we need to choose to segregate the data in groups or aggregate it together. That seems simple enough, but how do we decide which to do?\nThe answer is to think causally: how was the data generated and based on this, what factors influence the results that we are not shown?\nIn the exercise vs disease example, we intuitively know that exercise is not the only factor affecting the risk of developing a disease. In the data, there are two different causes of disease yet by aggregating the data and looking at only risk vs exercise, we ignore the second cause - age - completely."
  },
  {
    "objectID": "ObservationalStudies.html#resolving-the-paradox-1",
    "href": "ObservationalStudies.html#resolving-the-paradox-1",
    "title": "Observational Studies",
    "section": "Resolving the Paradox",
    "text": "Resolving the Paradox\nIf we go ahead and plot risk vs age, we can see that the age of the patient is strongly positively correlated with disease risk.\n\n\n\n\n\n\n\n\n\nAs the patient increases in age, their risk of the disease increases which means older patients are more likely to develop the disease than younger patients even with the same amount of exercise. Therefore, to assess the effect of just exercise on disease, we would want to hold the age constant and change the amount of weekly exercise."
  },
  {
    "objectID": "3-MultipleRegression.html#takeaways",
    "href": "3-MultipleRegression.html#takeaways",
    "title": "Multiple Regression",
    "section": "Takeaways",
    "text": "Takeaways\n\nGeneralized linear models allow us to fit models to predict non-continuous outcomes\nPredicting binary outcomes requires modeling the log-odds of success, where p = probability of success"
  },
  {
    "objectID": "3-MultipleRegression.html#recap-logistic-regression",
    "href": "3-MultipleRegression.html#recap-logistic-regression",
    "title": "Multiple Regression",
    "section": "Recap: Logistic Regression",
    "text": "Recap: Logistic Regression\nLogistic regression is a generalized regression model where the outcome is a two-level categorical variable. The outcome, takes the value 1 or 0 with probability. Ultimately, it is the probability of the outcome taking the value 1 (i.e., being a “success”) that we model in relation to the predictor variables. For this to work, we transform the expected outcome in such a way that it will be bounded between 0 and 1, and hence our estimates will be sensical. The transformation we use is called the logit, and is the natural logarithm of the odds of success.\n\nModelling binary outcomes\n\n\\(y\\) takes on values 0 (failure) or 1 (success)\n\\(p\\): probability of success\n\\(1-p\\): probability of failure\nWe can’t model \\(y\\) directly, so instead we model \\(p\\)\n\nLinear model\n\\[\n\\hat{p}_i = \\beta_o + \\beta_1 \\times x\n\\]\n\nBut remember that \\(p\\) must be between 0 and 1\nWe need a link function that transforms the linear model to have an appropriate range\n\nLogit link function\nThe logit function takes values between 0 and 1 (probabilities) and maps them to values in the range negative infinity to positive infinity:\n\\[\nlogit(p) = log \\bigg( \\frac{p}{1 - p} \\bigg)\n\\]\nGeneralized linear model\n\nWe model the logit (log-odds) of \\(p\\) :\n\n\\[\nlogit(\\hat{p}) = log \\bigg( \\frac{\\hat{p}}{1 - \\hat{p}} \\bigg) = \\beta_o + \\beta_1 \\times x\n\\]\n\nThen take the inverse to obtain the predicted \\(p\\):\n\n\\[\n\\hat{p} = \\frac{e^{\\beta_o + \\beta_1 \\times x }}{1 + e^{\\beta_o + \\beta_1 \\times x }}\n\\]\nA logistic model visualized"
  },
  {
    "objectID": "3-MultipleRegression.html#multiple-logistic-regression",
    "href": "3-MultipleRegression.html#multiple-logistic-regression",
    "title": "Multiple Regression",
    "section": "Multiple Logistic Regression",
    "text": "Multiple Logistic Regression\nMultiple logistic regression is used to model the relationship between a binary outcome variable and two or more predictor variables. It extends simple logistic regression by including multiple independent variables, which can be continuous or categorical, to assess their combined effect on the probability of an event occurring. The model estimates odds ratios for each predictor while controlling for the effects of others, making it useful for understanding complex associations."
  },
  {
    "objectID": "3-MultipleRegression.html#example-irish-continued---including-more-predictors",
    "href": "3-MultipleRegression.html#example-irish-continued---including-more-predictors",
    "title": "Multiple Regression",
    "section": "Example: Irish continued - including more predictors",
    "text": "Example: Irish continued - including more predictors\nIt is frequently of interest to use multiple covariates to improve our predictive power. Let’s fit a logistic regression model including all four covariates as predictors in our model: \\[\\displaystyle\\log\\left(\\frac{p}{1-p}\\right)=\\beta_0+\\beta_1*\\mbox{Sepal.Length}+\\beta_2*\\mbox{Sepal.Width}+\\beta_3*\\mbox{Petal.Length}+\\beta_4*\\mbox{Petal.Width}\\]\n\niris_mod2 &lt;- glm(Species.binary ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width,\n                 family = binomial, \n                 data = iris2)\niris_mod2 %&gt;% coef %&gt;% round(digits = 4)\n\n (Intercept) Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n    -42.6378      -2.4652      -6.6809       9.4294      18.2861 \n\n\n\nClassification\n\nQuestion: How well did we do at classifying the Iris species?\n\n\niris_predict &lt;- iris2 %&gt;% \n                  mutate(p_hat = predict(iris_mod2, type = \"response\") %&gt;% round(2),\n                         Species_pred = ifelse(p_hat &gt;= 0.5,\"virginica\",\"versicolor\"))\n\nn_correct_full &lt;- sum(iris_predict$Species_pred == iris_predict$Species)\nn_correct_full\n\n[1] 98\n\n\nOur model correctly predicts the species class of 98 out of 100 observations."
  },
  {
    "objectID": "sampling.html",
    "href": "sampling.html",
    "title": "Sampling Principles and Strategies",
    "section": "",
    "text": "The first step in conducting research is to identify topics or questions that are to be investigated. A clearly laid out research question is helpful in identifying what subjects or cases should be studied and what variables are important. It is also important to consider how data are collected so that the data are reliable and help achieve the research goals."
  },
  {
    "objectID": "sampling.html#research-questions",
    "href": "sampling.html#research-questions",
    "title": "Untitled",
    "section": "",
    "text": "Most research questions actually break down into 2 parts:\n\nDescriptive Statistics: What relationship can we observe between the variables, in the sample?\nInferential Statistics: Supposing we see a relationship in the sample data, how much evidence is provided for a relationship in the population? Does the data provide lots of evidence for a relationship in the population, or could the relationship we see in the sample be due just to chance variation in the sampling process that gave us the data?\n\nResearch Question: In the Ireland, what is the mean height of adult males (18 years +)?\nPopulation: all Irish adult males.\nQ. Can we survey the entire population?\nA. This is nearly impossible! It would be much quicker and easier to measure only a subset of the population, a sample.\nQ. How can we ensure the sample is an accurate reflection of the population?"
  },
  {
    "objectID": "sampling.html#sampling",
    "href": "sampling.html#sampling",
    "title": "Sampling",
    "section": "Sampling",
    "text": "Sampling\n\nSuppose that we were able to choose an appropriate sample that provides an accurate representation of the Irish population of men.\nThen, we have two different means to consider:\nMean Height of the Sample\n\nStatistic - describes the sample\nCan be known, but it changes depending on the sample\nSymbol - \\(\\bar{x}\\) (pronounced “x bar”)\n\nMean Height of the Population\n\nParameter - describes the population\nUsually unknown - but we wish we knew it!\nSymbol - \\(\\mu\\) (pronounced “mu”)"
  },
  {
    "objectID": "sampling.html#sample-vs-population",
    "href": "sampling.html#sample-vs-population",
    "title": "Sampling",
    "section": "Sample vs Population",
    "text": "Sample vs Population\nA reminder of an important distinction that we want to make before discussing surveys is the distinction between a sample and a population.\nA population is the set of subjects of interest.\nA sample is the subset of the population for which we have data.\n\n\nOur goal is to use the information we’ve gathered from the sample to infer, or predict, something about the population.\nFor our example, we want to predict the population mean, using our knowledge of the sample.\nThe accuracy of our sample mean relies heavily upon how well our sample represents the population at large.\nIf our sample does a poor job at representing the population, then any inferences that we make about the population are also going to be poor.\n\nThus, it is very important to select a good sample!"
  },
  {
    "objectID": "sampling.html#random-sampling",
    "href": "sampling.html#random-sampling",
    "title": "Sampling",
    "section": "Random Sampling",
    "text": "Random Sampling\nThere are four different methods of random sampling that we will introduce:\n\nSimple Random Sampling (SRS)\nSystematic Sampling\nStratified Sampling\nCluster Sampling"
  },
  {
    "objectID": "sampling.html#example-data-fakeschool",
    "href": "sampling.html#example-data-fakeschool",
    "title": "Sampling Principles and Strategies",
    "section": "Example Data: FakeSchool",
    "text": "Example Data: FakeSchool\n\nWe will use the FakeSchool data to compare the sampling methods.\nThe dataset contains information on 28 students from FakeSchool. We will assume that this is the population from which we will sample.\n\nHere is a snippet of the data:\n\n\n# A tibble: 4 × 5\n  Students Sex   class   GPA Honors\n  &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; \n1 Alice    F     Fr     3.8  Yes   \n2 Brad     M     Fr     2.6  Yes   \n3 Caleb    M     Fr     2.25 No    \n4 Daisy    F     Fr     2.1  No    \n\n\n\nLet’s say that we are interested in mean GPA\nWe can compute the true mean GPA\n\n\nFakeSchool %&gt;% \n  pull(GPA) %&gt;% \n  mean\n\n[1] 2.766429\n\n\n\nRemember this value is not typically known!"
  },
  {
    "objectID": "sampling.html#simple-random-sampling",
    "href": "sampling.html#simple-random-sampling",
    "title": "Sampling Principles and Strategies",
    "section": "Simple Random Sampling",
    "text": "Simple Random Sampling\nIn simple random sampling (SRS), for a given sample size, n, each member of the population has an equal chance of being selected.\nLet’s select a simple random sample of 7 elements without replacement. We can accomplish this easily with the dplyr function sample_n() in R. This function requires two pieces of information:\n\nthe size of the sample\nthe dataset from which to draw the sample\n\n\nSimple Random Sampling in R\n\n## create a simple random sample (n = 7)\nsrs &lt;- FakeSchool %&gt;% \n          sample_n(7)\nsrs\n\n# A tibble: 7 × 5\n  Students Sex   class   GPA Honors\n  &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; \n1 Daisy    F     Fr      2.1 No    \n2 Bob      M     Sr      3.8 Yes   \n3 Frank    M     Sr      2   No    \n4 Garth    M     Jr      1.1 No    \n5 Eliott   M     Jr      1.9 No    \n6 Angela   F     Sr      4   Yes   \n7 Chris    M     So      4   Yes   \n\n## calculate the mean of the srs\nsrs %&gt;% \n  pull(GPA) %&gt;% \n  mean\n\n[1] 2.7\n\n\n\n\nSRS Strength vs Weaknesses\nStrengths\n\nThe selection of one element does not affect the selection of others.\nEach possible sample, of a given size, has an equal chance of being selected.\nSimple random samples tend to be good representations of the population.\nRequires little knowledge of the population.\n\nWeaknesses\n\nIf there are small subgroups within the population, a SRS may not give an accurate representation of that subgroup. In fact, it may not include it at all! This is especially true if the sample size is small.\nIf the population is large and widely dispersed, it can be costly (both in time and money) to collect the data."
  },
  {
    "objectID": "sampling.html#systematic-sampling",
    "href": "sampling.html#systematic-sampling",
    "title": "Sampling Principles and Strategies",
    "section": "Systematic Sampling",
    "text": "Systematic Sampling\nIn a systematic sample, the sample members from a larger population are selected according to a random starting point but with a fixed, periodic interval, i.\nThese are the two main steps required to implement systematic sampling:\n\nDivide the size of the target population “N” by sample size “n” to calculate the sampling interval “i”. If this value is in decimals, it must be rounded to the nearest whole number/integer.\nThen, a random starting point, “r”, may be chosen from where the sampling interval “i” is used in order to choose respondents from the target population.\n\nBefore selecting the sample group, researchers must ensure that the list of the sample frame is not organized in a cyclical or periodic way in order to avoid selecting a biased sample group.\n\n\nTo illustrate the idea, let’s take a 1-in-4 systematic sample from our FakeSchool population.\n\nSystematic Sampling in R\n\n# randomly selecting our starting element.\nstart=sample(1:4,1)\nstart\n\n[1] 3\n\n# Now find every 4th row index starting with start\nsample_rows &lt;- seq(start, nrow(FakeSchool), by = 4)\nsample_rows\n\n[1]  3  7 11 15 19 23 27\n\n# Now choose the data corresponding to the row indexes\nsys_samp &lt;- FakeSchool %&gt;% \n        filter(row_number() %in% sample_rows)\n\n\n# print the systematic sample\nsys_samp\n\n# A tibble: 7 × 5\n  Students Sex   class   GPA Honors\n  &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; \n1 Caleb    M     Fr     2.25 No    \n2 Georg    M     Fr     1.4  No    \n3 Dylan    M     So     3.5  Yes   \n4 Adam     M     Jr     3.98 Yes   \n5 Faith    F     Jr     2.5  Yes   \n6 Bob      M     Sr     3.8  Yes   \n7 Ed       M     Sr     1.5  No    \n\n# find the mean GPA from the sys_samp\nsys_samp %&gt;% \n  pull(GPA) %&gt;% \n  mean\n\n[1] 2.704286\n\n\n\n\nStrength vs Weaknesses\nStrengths\n\nAssures an even, random sampling of the population.\nIt is especially useful when the population that you are studying is arranged in time. For example, suppose you are interested in the average amount of money that people spend at the grocery store on a Wednesday evening. A systematic sample could be used by selecting every 10th person that walks into the store.\n\nWeaknesses\n\nNot every combination has an equal chance of being selected. Many combinations will never be selected using a systematic sample!\nBeware of periodicity in the population! If, the selections match some pattern then the sample may not be representative of the population.\n\n\n\nNoticing patterns in data\n\nThe FakeSchool data is ordered according to the student’s year in school (freshmen, sophomore, junior, senior) and then by GPA (highest - lowest)\nTaking a systematic sample ensures that we have a person from each class represented in our sample.\nBut, what would happen if we took a systematic sample where k = 7 and the sample started at 1?"
  },
  {
    "objectID": "sampling.html#stratified-sampling",
    "href": "sampling.html#stratified-sampling",
    "title": "Sampling Principles and Strategies",
    "section": "Stratified Sampling",
    "text": "Stratified Sampling\nIn a stratified sample, the population must first be separated into homogeneous groups, or strata. Each element only belongs to one stratum and the stratum consist of elements that are alike in some way. A simple random sample is then drawn from each stratum, which is combined to make the stratified sample.\nLet’s take a stratified sample of 7 elements from FakeSchool using the following strata: Honors, Not Honors.\n\nStratified Sampling in R\n\n# determine how many elements belong to each strata\nFakeSchool %&gt;% \n  count(Honors) \n\n# A tibble: 2 × 2\n  Honors     n\n  &lt;fct&gt;  &lt;int&gt;\n1 No        16\n2 Yes       12\n\n# get the data for each strata\n# honors\nhon_strata &lt;- FakeSchool %&gt;% \n                filter(Honors == \"Yes\")\n\n# not honors\nnonhon_strata &lt;- FakeSchool %&gt;% \n                filter(Honors == \"No\")\n\nTry to divide the sampling evenly, the sample size is odd so use the bigger number for the larger strata\n\nhon_samp &lt;- hon_strata %&gt;% \n              sample_n(3)\n\nnonhon_samp &lt;- nonhon_strata %&gt;% \n                sample_n(4)\n\nstrat_samp &lt;- full_join(hon_samp, nonhon_samp)\n\nJoining with `by = join_by(Students, Sex, class, GPA, Honors)`\n\n\n\n# print the stratified sample\nstrat_samp\n\n# A tibble: 7 × 5\n  Students Sex   class   GPA Honors\n  &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; \n1 Angela   F     Sr     4    Yes   \n2 Adam     M     Jr     3.98 Yes   \n3 Cassie   F     Jr     3.75 Yes   \n4 Garth    M     Jr     1.1  No    \n5 Brittany F     Jr     3.9  No    \n6 Frank    M     Sr     2    No    \n7 Eva      F     Fr     1.8  No    \n\n# get the mean GPA from the stratified sample\nstrat_samp %&gt;% \n  pull(GPA) %&gt;% \n  mean\n\n[1] 2.932857\n\n\n\n\nStrength vs Weaknesses\nStrengths\n\nRepresentative of the population, because elements from all strata are included in the sample.\nEnsures that specific groups are represented, sometimes even proportionally, in the sample.\nAllows comparisons to be made between strata, if necessary. For example, a stratified sample allows you to easily compare the mean GPA of Honors students to the mean GPA of non-Honors students.\n\nWeaknesses\n\nRequires prior knowledge of the population. You have to know something about the population to be able to split into strata!"
  },
  {
    "objectID": "sampling.html#cluster-sampling",
    "href": "sampling.html#cluster-sampling",
    "title": "Sampling Principles and Strategies",
    "section": "Cluster Sampling",
    "text": "Cluster Sampling\nCluster sampling is a sampling method used when natural groups are evident in the population. The clusters should all be similar each other: each cluster should be a small scale representation of the population. To take a cluster sample, a random sample of the clusters is chosen. The elements of the randomly chosen clusters make up the sample.\nLet’s take a cluster sample using the grade level (freshmen, sophomore, junior, senior) of FakeSchool as the clusters. Let’s take a random sample of 2 of them.\n\nCluster Sampling in R\n\ncluster_samp &lt;- FakeSchool %&gt;% \n                  group_nest(class) %&gt;% \n                  sample_n(size = 2) %&gt;% \n                  unnest(data)\n\n## what class groups were used \ncluster_samp$class %&gt;% unique()\n\n[1] Jr So\nLevels: Fr Jr So Sr\n\n# calculate the mean GPA from the cluster sample\ncluster_samp %&gt;% \n  pull(GPA) %&gt;% \n  mean\n\n[1] 3.057857\n\n\n\n\nStrength vs Weaknesses\nStrengths\nMakes it possible to sample if there is no list of the entire population, but there is a list of subpopulations. For example, there is not a list of all school members in the United States. However, there is a list of schools that you could sample and then acquire the members list from each of the selected schools.\nWeaknesses\nNot always representative of the population. Elements within clusters tend to be similar to one another based on some characteristic(s). This can lead to over-representation or under-representation of those characteristics in the sample."
  },
  {
    "objectID": "sampling.html#sampling-principles-and-strategies",
    "href": "sampling.html#sampling-principles-and-strategies",
    "title": "Sampling",
    "section": "Sampling Principles and Strategies",
    "text": "Sampling Principles and Strategies\n\nResearch Question(s)\nResearch Question: Over the last 5 years, how many MU Data Science or Statistics graduates have gone on to get a job in a field directly related to their degree.\nPopulation: All DS or Statistics graduates from MU from the last 5 years.\nQ. Can we survey the entire population?\nA. This would likely be very difficult. It is more realistic to assume that we can work with a fraction of the population.\nQ. How can we ensure the sample is an accurate reflection of the population?\nA. Appropriate sampling.\nMost research questions actually break down into 2 parts:\n\nDescriptive Statistics: What relationship can we observe between the variables in the sample?\nInferential Statistics: Supposing we see a relationship in the sample data, how much evidence is provided for a relationship in the population? Does the data provide lots of evidence for a relationship in the population, or could the relationship we see in the sample be due just to chance variation in the sampling process that gave us the data?\n\n\n\nAnecdotal Evidence\n“I met two students who did a Data Science degree in Maynooth but they are not working as data scientists. The degree must not get you a Data Science job.”\nThere are two problems here. First, the data only represent two cases. Second, and more importantly, it is unclear whether these cases are actually representative of the population. Data collected in this haphazard fashion are called anecdotal evidence."
  },
  {
    "objectID": "sampling.html#sampling-from-the-population",
    "href": "sampling.html#sampling-from-the-population",
    "title": "Sampling Principles and Strategies",
    "section": "Sampling from the population",
    "text": "Sampling from the population\nA population is the set of subjects of interest.\nA sample is the subset of the population for which we have data.\nSuppose that we were able to choose an appropriate sample that provides an accurate representation of the DS and Statistics Graduates:\n\nNow we have two different summaries to consider for our research question:\nProportion of the sample that went into a directly related field\n\nStatistic - describes the sample\nCan be known, but it changes depending on the sample\nSymbol - \\(\\hat{p}\\)\n\nProportion of the population that went into a directly related field*\n\nParameter - describes the population\nUsually unknown - but we wish we knew it!\nSymbol - \\(\\pi\\)\n\n\n\n\nOur goal is to use the information we’ve gathered from the sample to infer, or predict, something about the population.\nFor our example, we want to predict the population proportion, using our knowledge of the sample.\nThe accuracy of our sample proportion relies heavily upon how well our sample represents the population at large.\nIf our sample does a poor job at representing the population, then any inferences that we make about the population are also going to be poor."
  },
  {
    "objectID": "sampling.html#sampling-procedures",
    "href": "sampling.html#sampling-procedures",
    "title": "Sampling Principles and Strategies",
    "section": "Sampling Procedures",
    "text": "Sampling Procedures\nAlmost all statistical methods are based on the notion of implied randomness. If data are not collected in a random framework from a population, these statistical methods – the estimates and errors associated with the estimates – are not reliable.\nThere are four different methods of random sampling that we will introduce:\n\nSimple Random Sampling (SRS)\nSystematic Sampling\nStratified Sampling\nCluster Sampling"
  },
  {
    "objectID": "4-sampling.html",
    "href": "4-sampling.html",
    "title": "Sampling Principles and Strategies",
    "section": "",
    "text": "The first step in conducting research is to identify topics or questions that are to be investigated. A clearly laid out research question is helpful in identifying what subjects or cases should be studied and what variables are important. It is also important to consider how data are collected so that the data are reliable and help achieve the research goals."
  },
  {
    "objectID": "4-sampling.html#sampling-from-the-population",
    "href": "4-sampling.html#sampling-from-the-population",
    "title": "Sampling Principles and Strategies",
    "section": "Sampling from the population",
    "text": "Sampling from the population\nA population is the set of subjects of interest.\nA sample is the subset of the population for which we have data.\nSuppose that we were able to choose an appropriate sample that provides an accurate representation of the DS and Statistics Graduates:\n\nNow we have two different summaries to consider for our research question:\nProportion of the sample that went into a directly related field\n\nStatistic - describes the sample\nCan be known, but it changes depending on the sample\nSymbol - \\(\\hat{p}\\)\n\nProportion of the population that went into a directly related field*\n\nParameter - describes the population\nUsually unknown - but we wish we knew it!\nSymbol - \\(\\pi\\)\n\n\n\n\nOur goal is to use the information we’ve gathered from the sample to infer, or predict, something about the population.\nFor our example, we want to predict the population proportion, using our knowledge of the sample.\nThe accuracy of our sample proportion relies heavily upon how well our sample represents the population at large.\nIf our sample does a poor job at representing the population, then any inferences that we make about the population are also going to be poor."
  },
  {
    "objectID": "4-sampling.html#sampling-procedures",
    "href": "4-sampling.html#sampling-procedures",
    "title": "Sampling Principles and Strategies",
    "section": "Sampling Procedures",
    "text": "Sampling Procedures\nAlmost all statistical methods are based on the notion of implied randomness. If data are not collected in a random framework from a population, these statistical methods – the estimates and errors associated with the estimates – are not reliable.\nThere are four different methods of random sampling that we will introduce:\n\nSimple Random Sampling (SRS)\nSystematic Sampling\nStratified Sampling\nCluster Sampling"
  },
  {
    "objectID": "4-sampling.html#example-data-fakeschool",
    "href": "4-sampling.html#example-data-fakeschool",
    "title": "Sampling Principles and Strategies",
    "section": "Example Data: FakeSchool",
    "text": "Example Data: FakeSchool\n\nWe will use the FakeSchool data to compare the sampling methods.\nThe dataset contains information on 28 students from FakeSchool. We will assume that this is the population from which we will sample.\n\nHere is a snippet of the data:\n\n\n# A tibble: 4 × 5\n  Students Sex   class   GPA Honors\n  &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; \n1 Alice    F     Fr     3.8  Yes   \n2 Brad     M     Fr     2.6  Yes   \n3 Caleb    M     Fr     2.25 No    \n4 Daisy    F     Fr     2.1  No    \n\n\n\nLet’s say that we are interested in mean GPA\nWe can compute the true mean GPA\n\n\nFakeSchool %&gt;% \n  pull(GPA) %&gt;% \n  mean\n\n[1] 2.766429\n\n\n\nRemember this value is not typically known!"
  },
  {
    "objectID": "4-sampling.html#simple-random-sampling",
    "href": "4-sampling.html#simple-random-sampling",
    "title": "Sampling Principles and Strategies",
    "section": "Simple Random Sampling",
    "text": "Simple Random Sampling\nIn simple random sampling (SRS), for a given sample size, n, each member of the population has an equal chance of being selected.\nLet’s select a simple random sample of 7 elements without replacement. We can accomplish this easily with the dplyr function sample_n() in R. This function requires two pieces of information:\n\nthe size of the sample\nthe dataset from which to draw the sample\n\n\nSimple Random Sampling in R\n\n## create a simple random sample (n = 7)\nsrs &lt;- FakeSchool %&gt;% \n          sample_n(7)\nsrs\n\n# A tibble: 7 × 5\n  Students Sex   class   GPA Honors\n  &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; \n1 Daisy    F     Fr      2.1 No    \n2 Bob      M     Sr      3.8 Yes   \n3 Frank    M     Sr      2   No    \n4 Garth    M     Jr      1.1 No    \n5 Eliott   M     Jr      1.9 No    \n6 Angela   F     Sr      4   Yes   \n7 Chris    M     So      4   Yes   \n\n## calculate the mean of the srs\nsrs %&gt;% \n  pull(GPA) %&gt;% \n  mean\n\n[1] 2.7\n\n\n\n\nSRS Strength vs Weaknesses\nStrengths\n\nThe selection of one element does not affect the selection of others.\nEach possible sample, of a given size, has an equal chance of being selected.\nSimple random samples tend to be good representations of the population.\nRequires little knowledge of the population.\n\nWeaknesses\n\nIf there are small subgroups within the population, a SRS may not give an accurate representation of that subgroup. In fact, it may not include it at all! This is especially true if the sample size is small.\nIf the population is large and widely dispersed, it can be costly (both in time and money) to collect the data."
  },
  {
    "objectID": "4-sampling.html#systematic-sampling",
    "href": "4-sampling.html#systematic-sampling",
    "title": "Sampling Principles and Strategies",
    "section": "Systematic Sampling",
    "text": "Systematic Sampling\nIn a systematic sample, the sample members from a larger population are selected according to a random starting point but with a fixed, periodic interval, i.\nThese are the two main steps required to implement systematic sampling:\n\nDivide the size of the target population “N” by sample size “n” to calculate the sampling interval “i”. If this value is in decimals, it must be rounded to the nearest whole number/integer.\nThen, a random starting point, “r”, may be chosen from where the sampling interval “i” is used in order to choose respondents from the target population.\n\nBefore selecting the sample group, researchers must ensure that the list of the sample frame is not organized in a cyclical or periodic way in order to avoid selecting a biased sample group.\n\n\nTo illustrate the idea, let’s take a 1-in-4 systematic sample from our FakeSchool population.\n\nSystematic Sampling in R\n\n# randomly selecting our starting element.\nstart=sample(1:4,1)\nstart\n\n[1] 3\n\n# Now find every 4th row index starting with start\nsample_rows &lt;- seq(start, nrow(FakeSchool), by = 4)\nsample_rows\n\n[1]  3  7 11 15 19 23 27\n\n# Now choose the data corresponding to the row indexes\nsys_samp &lt;- FakeSchool %&gt;% \n        filter(row_number() %in% sample_rows)\n\n\n# print the systematic sample\nsys_samp\n\n# A tibble: 7 × 5\n  Students Sex   class   GPA Honors\n  &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; \n1 Caleb    M     Fr     2.25 No    \n2 Georg    M     Fr     1.4  No    \n3 Dylan    M     So     3.5  Yes   \n4 Adam     M     Jr     3.98 Yes   \n5 Faith    F     Jr     2.5  Yes   \n6 Bob      M     Sr     3.8  Yes   \n7 Ed       M     Sr     1.5  No    \n\n# find the mean GPA from the sys_samp\nsys_samp %&gt;% \n  pull(GPA) %&gt;% \n  mean\n\n[1] 2.704286\n\n\n\n\nStrength vs Weaknesses\nStrengths\n\nAssures an even, random sampling of the population.\nIt is especially useful when the population that you are studying is arranged in time. For example, suppose you are interested in the average amount of money that people spend at the grocery store on a Wednesday evening. A systematic sample could be used by selecting every 10th person that walks into the store.\n\nWeaknesses\n\nNot every combination has an equal chance of being selected. Many combinations will never be selected using a systematic sample!\nBeware of periodicity in the population! If, the selections match some pattern then the sample may not be representative of the population.\n\n\n\nNoticing patterns in data\n\nThe FakeSchool data is ordered according to the student’s year in school (freshmen, sophomore, junior, senior) and then by GPA (highest - lowest)\nTaking a systematic sample ensures that we have a person from each class represented in our sample.\nBut, what would happen if we took a systematic sample where k = 7 and the sample started at 1?"
  },
  {
    "objectID": "4-sampling.html#stratified-sampling",
    "href": "4-sampling.html#stratified-sampling",
    "title": "Sampling Principles and Strategies",
    "section": "Stratified Sampling",
    "text": "Stratified Sampling\nIn a stratified sample, the population must first be separated into homogeneous groups, or strata. Each element only belongs to one stratum and the stratum consist of elements that are alike in some way. A simple random sample is then drawn from each stratum, which is combined to make the stratified sample.\nLet’s take a stratified sample of 7 elements from FakeSchool using the following strata: Honors, Not Honors.\n\nStratified Sampling in R\n\n# determine how many elements belong to each strata\nFakeSchool %&gt;% \n  count(Honors) \n\n# A tibble: 2 × 2\n  Honors     n\n  &lt;fct&gt;  &lt;int&gt;\n1 No        16\n2 Yes       12\n\n# get the data for each strata\n# honors\nhon_strata &lt;- FakeSchool %&gt;% \n                filter(Honors == \"Yes\")\n\n# not honors\nnonhon_strata &lt;- FakeSchool %&gt;% \n                filter(Honors == \"No\")\n\nTry to divide the sampling evenly, the sample size is odd so use the bigger number for the larger strata\n\nhon_samp &lt;- hon_strata %&gt;% \n              sample_n(3)\n\nnonhon_samp &lt;- nonhon_strata %&gt;% \n                sample_n(4)\n\nstrat_samp &lt;- full_join(hon_samp, nonhon_samp)\n\nJoining with `by = join_by(Students, Sex, class, GPA, Honors)`\n\n\n\n# print the stratified sample\nstrat_samp\n\n# A tibble: 7 × 5\n  Students Sex   class   GPA Honors\n  &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; \n1 Angela   F     Sr     4    Yes   \n2 Adam     M     Jr     3.98 Yes   \n3 Cassie   F     Jr     3.75 Yes   \n4 Garth    M     Jr     1.1  No    \n5 Brittany F     Jr     3.9  No    \n6 Frank    M     Sr     2    No    \n7 Eva      F     Fr     1.8  No    \n\n# get the mean GPA from the stratified sample\nstrat_samp %&gt;% \n  pull(GPA) %&gt;% \n  mean\n\n[1] 2.932857\n\n\n\n\nStrength vs Weaknesses\nStrengths\n\nRepresentative of the population, because elements from all strata are included in the sample.\nEnsures that specific groups are represented, sometimes even proportionally, in the sample.\nAllows comparisons to be made between strata, if necessary. For example, a stratified sample allows you to easily compare the mean GPA of Honors students to the mean GPA of non-Honors students.\n\nWeaknesses\n\nRequires prior knowledge of the population. You have to know something about the population to be able to split into strata!"
  },
  {
    "objectID": "4-sampling.html#cluster-sampling",
    "href": "4-sampling.html#cluster-sampling",
    "title": "Sampling Principles and Strategies",
    "section": "Cluster Sampling",
    "text": "Cluster Sampling\nCluster sampling is a sampling method used when natural groups are evident in the population. The clusters should all be similar each other: each cluster should be a small scale representation of the population. To take a cluster sample, a random sample of the clusters is chosen. The elements of the randomly chosen clusters make up the sample.\nLet’s take a cluster sample using the grade level (freshmen, sophomore, junior, senior) of FakeSchool as the clusters. Let’s take a random sample of 2 of them.\n\nCluster Sampling in R\n\ncluster_samp &lt;- FakeSchool %&gt;% \n                  group_nest(class) %&gt;% \n                  sample_n(size = 2) %&gt;% \n                  unnest(data)\n\n## what class groups were used \ncluster_samp$class %&gt;% unique()\n\n[1] Jr So\nLevels: Fr Jr So Sr\n\n# calculate the mean GPA from the cluster sample\ncluster_samp %&gt;% \n  pull(GPA) %&gt;% \n  mean\n\n[1] 3.057857\n\n\n\n\nStrength vs Weaknesses\nStrengths\nMakes it possible to sample if there is no list of the entire population, but there is a list of subpopulations. For example, there is not a list of all school members in the United States. However, there is a list of schools that you could sample and then acquire the members list from each of the selected schools.\nWeaknesses\nNot always representative of the population. Elements within clusters tend to be similar to one another based on some characteristic(s). This can lead to over-representation or under-representation of those characteristics in the sample."
  }
]