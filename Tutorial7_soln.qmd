---
title: "DS152 Tutorial Sheet 6"
format: 
  html:
    embed-resources: true
    self-contained-math: true
editor: visual
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Exercise 1

We will work with the Swiss banknotes dataset and perform logistic regression to determine whether the status of a bank note is genuine or counterfeit, based on six predictors related to measurements made on the bills.

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(mclust)
glimpse(banknote)
```

**(a)** Run the code below to add a binary (0,1) variable to the dataset called `status_binary` where the value is 0 when the Status is counterfeit and 1 when the Status is genuine. 

```{r}
banknote <- banknote %>% 
              mutate(status_binary = as.numeric(Status == "genuine")) 
glimpse(banknote) 
```


**(b)** Replace [A], [B] and [C] in the code below to create a scatter point plot showing `status_binary` as the response variable and  `Top` as a predictor. Do you think `Top` predictors for separating the counterfeit from the genuine notes? 

```{r, eval = TRUE}
ggplot(banknote, aes(x = Top, y = status_binary)) +
  geom_point()
```

"Genuine" is the reference group here. From the graph, it appears that as Top increases the chance of being a genuine note decreases. 

**(c)** Split the data into 85% training and 15% test data by replacing [A], [B], [C], [D] and [E] in the following code. Set the seed to 10465. 

```{r, eval = TRUE}
library(rsample)
set.seed(10465)
bank_split <- initial_split(banknote, prop = 0.85)

train_bank_data <- training(bank_split)
test_bank_data  <- testing(bank_split)
```

**(d)** Replace [A], [B], [C] and [D] in the code below to fit a simple logistic regression model to the training data, using `status_binary` as the response variable and `Top` as a predictor. 

```{r, eval = TRUE}
logistic_model <- glm(status_binary ~ Top,
                    family = binomial,
                    data = train_bank_data)
logistic_model
```


__Note for tutors:__ Remind students that we use `glm` for logistic regression rather than `lm`. Highlight the use of the training data for fitting the model rather than the full dataset. 


**(e)** Set up a threshold of 0.50 for classifying banknotes as genuine based on the test data set by replacing [A] and [B] in the following code. 

```{r, eval = TRUE}
bank_res <- test_bank_data %>% 
            select(Status, status_binary) %>% 
            mutate(pred_p = predict(logistic_model, 
                                    type = "response", newdata = test_bank_data)) %>% 
            mutate(pred_status = ifelse(pred_p >= 0.5,"genuine","counterfeit")) 
bank_res
```

__Note for tutors:__ Highlight the use of the test data for getting predictions from the model rather than the full dataset. 

**(f)** Create a confusion table by running the code below. What is the accuracy? 

```{r, eval = TRUE}
N <- nrow(bank_res)

confusion_mat <- table(bank_res %>% select(Status, pred_status))
confusion_mat

accuracy <- (confusion_mat %>% diag %>% sum)/N
accuracy
```

**(g)** Now rerun all of the above but instead use a multiple logistic regression model with `Top` and `Diagonal` as predictors. Which model indicates the best performance when it comes to distinguishing counterfeit notes from genuine ones? 

```{r}
logistic_model2 <- glm(status_binary ~ Top + Diagonal,
                    family = binomial,
                    data = train_bank_data)
logistic_model2

#### Update bank_res from earlier with prediction from model 2
bank_res <- bank_res %>% 
            mutate(pred_p2 = predict(logistic_model2, 
                                    type = "response", newdata = test_bank_data)) %>% 
            mutate(pred_status2 = ifelse(pred_p2 >= 0.5,"genuine","counterfeit")) 
bank_res


confusion_mat2 <- table(bank_res %>% select(Status, pred_status2))
confusion_mat2

accuracy2 <- (confusion_mat2 %>% diag %>% sum)/N
accuracy2
```

## Exercise 2

We will consider data on crime rates by US state. This data set contains arrests per 100,000 residents for assault and murder in each of the 50 US states in 1973. A glimpse of the dataset, called `arrests_dat`, is displayed below:

```{r, message = FALSE}
library(tidyverse)
arrests_dat <- read_csv("https://www.dropbox.com/s/8dc41ynd12fwvhr/arrests_dat.csv?raw=1")
glimpse(arrests_dat)
```

**(a)** To visualise the data, what would you replace [A], [B], [C] and [D] in the code below with to create this plot? 

```{r}
ggplot(arrests_dat, aes(x = murder, y = assault)) +
  geom_point() +
  xlab("Murder arrests (per 100,000)") +
  ylab("Assault arrests (per 100,000)")
```

**(b)** Now, let's perform k-means clustering.  Firstly, we will create a new dataset that contains only the `murder` and `assault` variables from the `arrests_dat` dataset. Then we will run the k-means algorithm with 2 clusters on this data. Replace [E],[F] and [G] in the code such that this objective can be achieved. 

```{r}
arrests_dat2 <- arrests_dat %>% 
                  dplyr::select(murder, assault)

kmean_res <- kmeans(arrests_dat2, centers = 2)
```

**(c)** Next we want to visualise the clustering results. What would you replace [H], [I], [J] in the code below to create this plot?

```{r}
library(factoextra)
fviz_cluster(kmean_res , data = arrests_dat2, geom = "point")
```

