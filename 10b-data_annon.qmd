---
title: "Introduction to Data Anonymisation"
format: html
editor: visual
---

```{r setup, include=FALSE, eval=FALSE}
# Randomised response simulation
n <- 1000
p <- 0.2
truth <- rbinom(n, 1, p)

coin <- function() rbinom(1, 1, 0.5)

resp <- numeric(n)
for (i in 1:n) {
  toss <- coin()
  resp[i] <- ifelse(toss == 1, 1, truth[i])
}

# Estimated and true proportions
estimated <- sum(resp) - (n / 2)
estimated / n  # estimate
sum(truth) / n # actual
```

## ðŸš¨ Case Study: Location Tracking at Scale

- Companies track smartphone users constantly, storing **billions** of data points.
- The NY Times [Privacy Project](https://www.nytimes.com/interactive/2019/12/19/opinion/location-tracking-cell-phone.html) obtained one such file:
  - 50 **billion** location pings
  - From over **12 million Americans** across 2016â€“2017

> *â€œYou may think your location data is anonymousâ€¦ itâ€™s not.â€*

### Why It's Hard to Hide

- Imagine your **daily commute**: does anyone elseâ€™s phone follow your same path?
- High-resolution geolocation data is *virtually impossible* to anonymise.
- Paul Ohm:  
  > *â€œDNA is probably the only thing that's harder to anonymise than geolocation data.â€*

ðŸ“– [How iPhone apps track you](https://www.nytimes.com/wirecutter/blog/how-iphone-apps-track-you/)

---

## ðŸ•µï¸â€â™€ï¸ Data Anonymisation: What and Why

### ðŸ”‘ Key Privacy Goals

**Disclosure Control**  
- Protect **individual identity**, allow **group-level insights**

**Privacyâ€“Utility Trade-off**  
- Balance between **data usefulness** and **privacy risk**

---

## ðŸ”Œ Privacy in the Digital Age

![](images/alexa.jpg)

> â€œYou have zero privacy anyway â€” get over it.â€  
> â€” *Scott McNealy, Sun Microsystems CEO, 1999*

> â€œIf you have something to hide, maybe you shouldnâ€™t be doing it.â€  
> â€” *Eric Schmidt, former Google CEO, 2009*

---

## ðŸ“ Defining Privacy

Dalenius (1977) on statistical data:

> *â€œNothing can be learned about an individual from a released dataset that couldnâ€™t be learned without it.â€*

Butâ€¦

- If Alice knows Bob is **5cm shorter** than average...
- And the **average** is published...
- She now knows Bob's **exact height**

âž¡ï¸ Highlights the power of **side information**

---

## ðŸ§© What Makes Data Personal?

### Attribute Types

| Type         | Examples                         |
|--------------|----------------------------------|
| Identifiers  | Name, ID number, Email           |
| Sensitive    | Health, Salary, Religion         |
| Non-sensitive| Age, ZIP code, Education         |

ðŸš¨ Quasi-identifiers: seemingly benign attributes that, in combination, reveal identity.

---

## âŒ Naive Anonymisation

- Just removing identifiers **isnâ€™t enough**
- Combinations like **ZIP + Gender + DOB** identify **87%** of the U.S. population (Sweeney, 2000)

![](images/qid.png)

> ðŸ¥ MA released "anonymous" health data.  
> ðŸ“‹ L. Sweeney used a copy of voter records containing names as well as ZIP, gender and DOB and by cross-referencing the two datasets, she identified the state governor's records in the medical data!

---

## ðŸ”¢ K-Anonymity

> Based on quasi-identifiers, make each person **indistinguishable** from at least $k-1$ others in a dataset.

![](images/kanon.png)

### Techniques to Achieve K-Anonymity:

1. **Suppression** â€“ remove or mask values  

![](images/kanon2.png)

2. **Generalisation** â€“ make values less specific

![](images/kanon3.png)

### Limitations

<!-- ![](images/kanon4.png) -->

- **Homogeneity attack**: if all $k$ rows have same sensitive value, identity could be inferred.

    ![](images/kanon5.png)
    
- If we know John, who's 19 years old and from Meath, is on the database, we can correctly infer he has a heart-related condition

---

## ðŸ§± Beyond k-Anonymity

### ðŸ”€ l-Diversity

- Requires **diverse sensitive values** among $k$-group

### ðŸŒ«ï¸ Differential Privacy

- Add **random noise** to numerical responses, but in such a way that aggregate results remain the same. 
- Ensures **individual contribution has minimal impact**
- Widely used by **Apple**, **Google**, **US Census**

---

## ðŸ”§ Other Techniques

### Microaggregation

- Group records, replace sensitive value with **group average**
- Preserves structure while hiding individuals

### Shuffling (Reverse Mapping)

1. Model sensitive variable using public attributes  
2. Predict & rank values  
3. Shuffle original data based on these ranks  
4. Result: **original values**, but **reordered**

### Randomised Response (Warner, 1965)

- Coin flip decides if respondent answers truthfully
- Provides **individual deniability**, preserves **population estimates**

![An embarrassing survey](https://www.youtube.com/watch?v=nwJ0qY_rP0A)
---

## ðŸ¤” Final Thoughts

- **Perfect anonymisation is extremely difficult**
- Privacy-preserving methods must consider:
  - Side info
  - Utility of data
  - Statistical inference risks

> Remember:  
> Data can be anonymous in theory, but not always in practice.

---

## ðŸ“š Further Reading

- [NYT Privacy Project](https://www.nytimes.com/series/new-york-times-privacy-project)
- Sweeney, L. (2000). Simple Demographics Often Identify People Uniquely.
- Dwork, C. (2006). Differential Privacy.

