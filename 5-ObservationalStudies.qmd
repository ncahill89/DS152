---
title: "Observational Studies"
format: html
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

<!-- ## Scientific Studies -->

<!-- **Observational** -->

<!-- -   Collect data in a way that does not directly interfere with how the data arise, i.e. merely "observe"; -->

<!-- -   Based on an observational study, we can only establish an association, in other words correlation, between the explanatory and response variables; -->

<!-- -   With an observational study you are just observing the data and collecting it after or as it occurs. -->

<!-- **Experimental** -->

<!-- -   Randomly assign subjects to treatments -->

<!-- -   Establish causal connections -->

<!-- ## Studies and Conclusions -->

<!-- ![](images/studies_conclusions.png) -->

<!-- ## Why use observational studies? -->

<!-- Reasons why we must sometimes use an observational study instead of an experiment ... -->

<!-- 1.  It is unethical or impossible to assign people to receive a specific treatment. -->

<!--     -   For example, if you want to know the impact of smoking on cancer you cannot design a study and assign some people to be smokers and other to be non-smokers. That is not ethical. -->

<!-- 2.  Certain exposure variables are inherent traits and cannot be randomly assigned. -->

<!-- ## Observational studies -->

<!-- -   Observational studies are very useful when it is not possible to design a study -->

<!-- -   Obervational studies often have large sample sizes -->

<!-- -   They are cheaper than designing a study (for example, clinical trials in medicine are very expensive) -->

<!-- -   You have to get to know the data a lot better with observational studies. You haven't set up to study or controlled for influences other than the thing you are studying. -->

<!-- ##  -->

<!-- \centering -->

<!-- ![](images/popsamp.png) -->

<!-- ##  -->

<!-- \centering -->

<!-- ![](images/precbias.png) -->

<!-- ## Retrospective Vs Prospective Studies -->

<!-- If an observational study uses data from the past, it is called retrospective study, whereas if data are collected throughout the study, it is called prospective; -->

<!-- ![](images/retroprosp.png) -->

## About Observational Studies

-   **Exposure**: A variable that represents a factor of interest that may influence an outcome, such as a risk factor, treatment, behavior, or environmental condition (e.g., smoking, diet, air pollution, or a medical intervention).

-   **Outcome**: The event or condition that is being studied as the possible effect of the exposure, such as disease occurrence, recovery, mortality, or any measurable health-related change (e.g., lung cancer, weight loss, or blood pressure levels).

-   In **retrospective** observational studies, investigators “look backwards in time” and use data that have already been collected. Retrospective studies are often criticized for having more confounding and bias compared to prospective studies.

-   In **prospective** observational studies, investigators choose a sample and collect new data generated from that sample. That is, the investigators “look forward in time.”

![](images/Screenshot%202025-03-03%20at%2010.12.32.png)

In **observational studies**, researchers examine the relationship between exposure and outcome without intervening, aiming to identify associations while controlling for confounders (more on this later).

-   An observational study on individuals from a random sample allows one to generalize conclusions about the sample to the population.

-   An observational study cannot show cause-and-effect relationships because there is the possibility that the response is affected by some variable(s) other than the ones being measured. That is, confounding variables may be present. 

## Example 1: Student Happiness

The `gcfeeling` data contains results of a survey conducted by Georgetown College students on 47 Georgetown College students in relation to feelings about Georgetown College.

```{r, message=FALSE, warning=FALSE}
# Load necessary packages
library(tidyverse)
library(tigerstats)

# Load the dataset
data("gcfeeling")

# View basic structure
glimpse(gcfeeling)
```

### **1. Exploring Categorical Variables**

`gcfeeling` contains categorical variables. We can visualize their distributions:

```{r}
# Count and visualize categorical variables
gcfeeling %>%
  select_if(is.factor) %>% 
  pivot_longer(everything(),names_to = "Variable", values_to = "Value") %>%
  ggplot(aes(x = Value)) +
  geom_bar() +
  facet_wrap(~ Variable, scales = "free") +
  theme_minimal() +
  coord_flip()
```

### **2. Exploring Numeric Variables**

`gcfeeling` contains numeric variables. We can visualize their distributions:

```{r}
# Check numeric variable distributions
gcfeeling %>%
  select_if(is.numeric) %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
  ggplot(aes(x = Variable,y = Value)) +
  geom_boxplot() +
  theme_minimal()
```

### **3. Relationships Between Variables**

#### **Contingency Table (For Categorical Variables)**

A contingency table shows the relationship between two categorical variables by displaying their frequency distribution.

```{r}
gcfeeling %>%
  select(athlete, happier) %>% 
  table()
```

#### **Barplots (For Categorical Variables)**

We can visualise conditional distributions, for example, given the student is an athlete what is the chance they were happier?

```{r}
gcfeeling %>%
  select(athlete, happier) %>% 
  count(athlete, happier) %>% 
  ggplot(aes(x = athlete, fill = happier, weight = n)) +
  geom_bar(position = "fill") 
```

#### **Pairwise Correlation (For Numeric Variables)**

As we've seen before, correlation measures the strength and direction of the relationship between two variables, indicating how changes in one variable are associated with changes in another.

```{r}
# Compute correlation matrix
cor(gcfeeling$rating.fresh, gcfeeling$rating.diff)
```

#### **Scatterplots (For Numeric Variables)**

We can visualise the relationship between two numerical variables, for example, current happiness level vs past happiness level.

```{r}
ggplot(gcfeeling, aes(x = rating.fresh, y = rating.diff)) +
  geom_point()
```

#### **Boxplots of Numerical Variables by Categories**

We can visualise the relationship between a categorical and a numerical variable, for example, current happiness level broken down by athlete categories.

```{r}
# Boxplots grouped by a categorical variable
gcfeeling %>%
  ggplot(aes(x = athlete, y = rating.diff)) +
  geom_boxplot() +
  theme_minimal()
```

### **4. Modelling**

#### **Numerical outcome**

We can use multiple linear regression to model student happiness.

```{r}
lm(rating.diff ~ rating.fresh + athlete + greek, data = gcfeeling)
```

#### **Binary outcome**

We can use multiple logisitc regression to model student happiness.

```{r}
glm(happier ~ rating.fresh + athlete + greek, data = gcfeeling, family = "binomial")
```

## Example 2: Does working out increase energy levels?

We want to evaluate if regularly working out has any impact on energy levels.

-   In an observational study, we sample two types of people from the population, those who choose to work out regularly and those who don't.

-   We ask the people in each group to rate their energy levels from 1-10.

-   Then, we find the average "energy level" for the two groups of people and compare.

![](images/EnergyLevels.jpg)

Can we conclude from this that working out is the cause of increased energy levels?

-   There may be other variables that we didn't control for in this study that contribute to the observed difference.

-   For example, people who have young children might have less time to work out and also have lower energy levels.

-   This is known as **confounding**.

-   This study allows us to make correlation statements. But, we cannot make a causal statement attributing increased energy levels to working out!

### Confounding variables

**Confounding variables:** Extraneous variables that affect both the exposure (e.g., working out) and the outcome variables (e.g., increased energy), and that make it seem like there is a relationship between them are called confounding variables.

![](images/Confounding.jpg)

## Example 3: Pancreatic Cancer Study

Many years ago, investigators reported an association between coffee drinking and pancreatic cancer in an observational study (*MacMahon B, Yen S, Trichopoulos D, Warren K, Nardi G. Coffee and cancer of the pancreas. N Eng J Med 1981; 304: 630-3*).

![](images/Confounding3.png)

If we take coffee as our exposure of interest and correlate it with the development of pancreatic cancer there is the potential, as was the case with these investigators, to be misled. There is a third causal factor, cigarette smoking, that was more common among those who reported drinking coffee and those with pancreatic cancer.

![](images/Confounding2.png)

Once the confounding variable, smoking is taken into account the correlation between coffee and pancreatic cancer disappears.

### Reducing confounding: Matching

Matching is a technique used in observational studies to reduce confounding by ensuring that compared groups have similar distributions of key variables. Since observational studies lack randomization of the exposure, confounders—variables that are related to both the exposure and the outcome—can introduce bias. Matching helps control for these confounders by pairing or grouping subjects with similar characteristics across different exposure levels.

### Reducing confounding: Multiple Regression

Multiple regression models specify the way in which different characteristics/variables (exposure and confounders) affects the outcome, thereby isolating the effect of each variable.

chance of cancer = a x (coffee) + b x (smoking) + c x (gender) + d x (age)

-   this allows us to make a statement about what would happen if one variable (i.e., the exposure) were to change while all the others (i.e., the confounders) remained the same.

-   Obtaining isolated exposure effects conditional on the other variables remaining constant is said to adjust for (or control for) the effect of these confounders

## Example 4: Restaurant Reviews

You and your friend are trying to find the perfect restaurant for dinner. You can't decide so you want to instead rely on some observational data (i.e., restaurant reviews).

You find two worthy restaurants, Carla's and Sophia's each with 400 reviews and an indicator of whether the restaurant is recommended or not recommended.

You find that

-   recommended for Sophia's = 250/400

-   recommended for Carla's = 216/400

So what we have is a conditional probability:

-   p(recommended\|Sophia's) = 62.5%

-   p(recommended\|Carla's) = 54%

------------------------------------------------------------------------

What if we consider age as a factor here?

-   recommended for 18-35 yr old diners at Sophia's = 50/150

-   recommended for 35+ diners at Sophia's = 200/250

-   recommended for 18-35 yr old diners at Carla's = 180/360

-   recommended for 35+ diners at Carla's = 36/40

So what we have is:

-   p(recommended\|Sophia's, younger) = 30%

-   p(recommended\|Sophia's, older) = 80%

-   p(recommended\|Carla's, younger) = 50%

-   p(recommended\|Carla's, older) = 90%

## What's going on?

You have unknowingly entered the world of Simpson's Paradox, where a restaurant can be both better and worse than its competitor, exercise can lower and increase the risk of disease, and the same dataset can be used to prove two opposing arguments. Instead of going out to dinner, perhaps you and your friend should spend the evening discussing this fascinating statistical phenomenon.

**Simpson's Paradox occurs when trends that appear when a dataset is separated into groups reverse when the data are aggregated.**

## Example 5: Exercise and Disease

Say we have (made up) data on the number of hours of exercise per week versus the risk of developing a disease for two sets of patients, those below the age of 50 and those over the age of 50. Here are individual plots showing the relationship between exercise and risk of disease.

```{r,echo = FALSE, message = FALSE, fig.width=6, fig.height=3.5}

# Load necessary libraries
library(ggplot2)
library(dplyr)

# Set seed for reproducibility
set.seed(123)

# Generate data for younger group (under 50)
n_young <- 100
ages_young <- round(runif(n_young, 20, 49))
hours_young <- round(runif(n_young, 1, 5)) + rnorm(n_young, 0, 0.5)
risk_young <- 50 - 2 * hours_young + rnorm(n_young, 0, 3)  # Negative correlation

# Generate data for older group (over 50)
n_old <- 100
ages_old <- round(runif(n_old, 50, 80))
hours_old <- round(runif(n_old, 3, 8)) + rnorm(n_old, 0, 0.5)
risk_old <- 80 - 2 * hours_old + rnorm(n_old, 0, 3)  # Negative correlation

# Combine data into a dataframe
exercise_dat <- tibble(
  age = c(ages_young, ages_old),
  hours_exercised = c(hours_young, hours_old),
  risk = c(risk_young, risk_old),
  age_group = rep(c("Under 50", "Over 50"), each = n_young)
)

# Scatter plot with separate age groups
ggplot(exercise_dat, aes(x = hours_exercised, y = risk, color = age_group)) +
  geom_point(alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Exercise vs. Health Risk",
       x = "Hours Exercised",
       y = "Health Risk",
       color = "Age Group") +
  theme_minimal()


```

We clearly see a negative correlation, indicating that increased levels of exercise per week are correlated with a lower risk of developing the disease for both groups. Now, let's combine the data together on a single plot:

```{r,echo = FALSE, message = FALSE, fig.width=6, fig.height=4}
ggplot(exercise_dat, aes(x = hours_exercised, y = risk))+
  geom_point()+
  geom_smooth(method = "lm", se = FALSE) +
    labs(title = "Exercise vs. Health Risk",
       x = "Hours Exercised",
       y = "Health Risk") +
  theme_minimal()

```

❌ Surprise! The overall correlation is positive: More exercise → Higher risk.

### Explanation of the Paradox:

Within each age group: More exercise is associated with lower risk (negative correlation).

Across both groups: Older individuals exercise more...

```{r,echo = FALSE, message = FALSE, fig.width=4, fig.height=3}
ggplot(exercise_dat, aes(x = age_group, y = hours_exercised))+
  geom_boxplot()
```

...but have higher baseline risk due to age.

```{r,echo = FALSE, message = FALSE, fig.width=4, fig.height=3}
ggplot(exercise_dat, aes(x = age_group, y = risk))+
  geom_boxplot()
```

When aggregated, the higher risk of the older group skews the data, making it seem like more exercise increases risk. This is Simpson’s Paradox — where a trend appears in subgroups but reverses when the data is combined!
